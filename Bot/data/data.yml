categories:
  - SO
  - C++
conversations:
- - Why is it faster to process a sorted array than an unsorted array?
  - "\nYou are a victim of branch prediction fail.\n\nWhat is Branch Prediction?\n\
    Consider a railroad junction:\n\nImage by Mecanismo, via Wikimedia Commons. Used\
    \ under the CC-By-SA 3.0 license.\nNow for the sake of argument, suppose this\
    \ is back in the 1800s - before long distance or radio communication.\nYou are\
    \ the operator of a junction and you hear a train coming. You have no idea which\
    \ way it is supposed to go. You stop the train to ask the driver which direction\
    \ they want. And then you set the switch appropriately.\nTrains are heavy and\
    \ have a lot of inertia. So they take forever to start up and slow down.\nIs there\
    \ a better way? You guess which direction the train will go!\n\nIf you guessed\
    \ right, it continues on.\nIf you guessed wrong, the captain will stop, back up,\
    \ and yell at you to flip the switch. Then it can restart down the other path.\n\
    \nIf you guess right every time, the train will never have to stop.\nIf you guess\
    \ wrong too often, the train will spend a lot of time stopping, backing up, and\
    \ restarting.\n\nConsider an if-statement: At the processor level, it is a branch\
    \ instruction:\n\nYou are a processor and you see a branch. You have no idea which\
    \ way it will go. What do you do? You halt execution and wait until the previous\
    \ instructions are complete. Then you continue down the correct path.\nModern\
    \ processors are complicated and have long pipelines. So they take forever to\
    \ \"warm up\" and \"slow down\".\nIs there a better way? You guess which direction\
    \ the branch will go!\n\nIf you guessed right, you continue executing.\nIf you\
    \ guessed wrong, you need to flush the pipeline and roll back to the branch. Then\
    \ you can restart down the other path.\n\nIf you guess right every time, the execution\
    \ will never have to stop.\nIf you guess wrong too often, you spend a lot of time\
    \ stalling, rolling back, and restarting.\n\nThis is branch prediction. I admit\
    \ it's not the best analogy since the train could just signal the direction with\
    \ a flag. But in computers, the processor doesn't know which direction a branch\
    \ will go until the last moment.\nSo how would you strategically guess to minimize\
    \ the number of times that the train must back up and go down the other path?\
    \ You look at the past history! If the train goes left 99% of the time, then you\
    \ guess left. If it alternates, then you alternate your guesses. If it goes one\
    \ way every 3 times, you guess the same...\nIn other words, you try to identify\
    \ a pattern and follow it. This is more or less how branch predictors work.\n\
    Most applications have well-behaved branches. So modern branch predictors will\
    \ typically achieve >90% hit rates. But when faced with unpredictable branches\
    \ with no recognizable patterns, branch predictors are virtually useless.\nFurther\
    \ reading: \"Branch predictor\" article on Wikipedia.\n\nAs hinted from above,\
    \ the culprit is this if-statement:\nif (data[c] >= 128)\n    sum += data[c];\n\
    \nNotice that the data is evenly distributed between 0 and 255. \nWhen the data\
    \ is sorted, roughly the first half of the iterations will not enter the if-statement.\
    \ After that, they will all enter the if-statement.\nThis is very friendly to\
    \ the branch predictor since the branch consecutively goes the same direction\
    \ many times.\nEven a simple saturating counter will correctly predict the branch\
    \ except for the few iterations after it switches direction.\nQuick visualization:\n\
    T = branch taken\nN = branch not taken\n\ndata[] = 0, 1, 2, 3, 4, ... 126, 127,\
    \ 128, 129, 130, ... 250, 251, 252, ...\nbranch = N  N  N  N  N  ...   N    N\
    \    T    T    T  ...   T    T    T  ...\n\n       = NNNNNNNNNNNN ... NNNNNNNTTTTTTTTT\
    \ ... TTTTTTTTTT  (easy to predict)\n\nHowever, when the data is completely random,\
    \ the branch predictor is rendered useless because it can't predict random data.\n\
    Thus there will probably be around 50% misprediction. (no better than random guessing)\n\
    data[] = 226, 185, 125, 158, 198, 144, 217, 79, 202, 118,  14, 150, 177, 182,\
    \ 133, ...\nbranch =   T,   T,   N,   T,   T,   T,   T,  N,   T,   N,   N,   T,\
    \   T,   T,   N  ...\n\n       = TTNTTTTNTNNTTTN ...   (completely random - hard\
    \ to predict)\n\n\nSo what can be done?\nIf the compiler isn't able to optimize\
    \ the branch into a conditional move, you can try some hacks if you are willing\
    \ to sacrifice readability for performance.\nReplace:\nif (data[c] >= 128)\n \
    \   sum += data[c];\n\nwith:\nint t = (data[c] - 128) >> 31;\nsum += ~t & data[c];\n\
    \nThis eliminates the branch and replaces it with some bitwise operations.\n(Note\
    \ that this hack is not strictly equivalent to the original if-statement. But\
    \ in this case, it's valid for all the input values of data[].)\nBenchmarks: Core\
    \ i7 920 @ 3.5 GHz\nC++ - Visual Studio 2010 - x64 Release\n//  Branch - Random\n\
    seconds = 11.777\n\n//  Branch - Sorted\nseconds = 2.352\n\n//  Branchless - Random\n\
    seconds = 2.564\n\n//  Branchless - Sorted\nseconds = 2.587\n\nJava - Netbeans\
    \ 7.1.1 JDK 7 - x64\n//  Branch - Random\nseconds = 10.93293813\n\n//  Branch\
    \ - Sorted\nseconds = 5.643797077\n\n//  Branchless - Random\nseconds = 3.113581453\n\
    \n//  Branchless - Sorted\nseconds = 3.186068823\n\nObservations:\n\nWith the\
    \ Branch: There is a huge difference between the sorted and unsorted data.\nWith\
    \ the Hack: There is no difference between sorted and unsorted data.\nIn the C++\
    \ case, the hack is actually a tad slower than with the branch when the data is\
    \ sorted.\n\nA general rule of thumb is to avoid data-dependent branching in critical\
    \ loops. (such as in this example)\n\nUpdate:\n\nGCC 4.6.1 with -O3 or -ftree-vectorize\
    \ on x64 is able to generate a conditional move. So there is no difference between\
    \ the sorted and unsorted data - both are fast.\nVC++ 2010 is unable to generate\
    \ conditional moves for this branch even under /Ox.\nIntel Compiler 11 does something\
    \ miraculous. It interchanges the two loops, thereby hoisting the unpredictable\
    \ branch to the outer loop. So not only is it immune the mispredictions, it is\
    \ also twice as fast as whatever VC++ and GCC can generate! In other words, ICC\
    \ took advantage of the test-loop to defeat the benchmark...\nIf you give the\
    \ Intel Compiler the branchless code, it just out-right vectorizes it... and is\
    \ just as fast as with the branch (with the loop interchange).\n\nThis goes to\
    \ show that even mature modern compilers can vary wildly in their ability to optimize\
    \ code...\n"
- - Why is it faster to process a sorted array than an unsorted array?
  - "\nBranch prediction.\nWith a sorted array, the condition data[c] >= 128 is first\
    \ false for a streak of values, then becomes true for all later values. That's\
    \ easy to predict. With an unsorted array, you pay for the branching cost.\n"
- - Why is it faster to process a sorted array than an unsorted array?
  - "\nThe reason why performance improves drastically when the data is sorted is\
    \ that the branch prediction penalty is removed, as explained beautifully in Mysticial's\
    \ answer.\nNow, if we look at the code\nif (data[c] >= 128)\n    sum += data[c];\n\
    \nwe can find that the meaning of this particular if... else... branch is to add\
    \ something when a condition is satisfied. This type of branch can be easily transformed\
    \ into a conditional move statement, which would be compiled into a conditional\
    \ move instruction: cmovl, in an x86 system. The branch and thus the potential\
    \ branch prediction penalty is removed.\nIn C, thus C++, the statement, which\
    \ would compile directly (without any optimization) into the conditional move\
    \ instruction in x86, is the ternary operator ... ? ... : .... So we rewrite the\
    \ above statement into an equivalent one:\nsum += data[c] >=128 ? data[c] : 0;\n\
    \nWhile maintaining readability, we can check the speedup factor.\nOn an Intel\
    \ Core i7-2600K @ 3.4Â GHz and Visual Studio 2010 Release Mode, the benchmark is\
    \ (format copied from Mysticial):\nx86\n//  Branch - Random\nseconds = 8.885\n\
    \n//  Branch - Sorted\nseconds = 1.528\n\n//  Branchless - Random\nseconds = 3.716\n\
    \n//  Branchless - Sorted\nseconds = 3.71\n\nx64\n//  Branch - Random\nseconds\
    \ = 11.302\n\n//  Branch - Sorted\n seconds = 1.830\n\n//  Branchless - Random\n\
    seconds = 2.736\n\n//  Branchless - Sorted\nseconds = 2.737\n\nThe result is robust\
    \ in multiple tests. We get a great speedup when the branch result is unpredictable,\
    \ but we suffer a little bit when it is predictable. In fact, when using a conditional\
    \ move, the performance is the same regardless of the data pattern.\nNow let's\
    \ look more closely by investigating the x86 assembly they generate. For simplicity,\
    \ we use two functions max1 and max2.\nmax1 uses the conditional branch if...\
    \ else ...:\nint max1(int a, int b) {\n    if (a > b)\n        return a;\n   \
    \ else\n        return b;\n}\n\nmax2 uses the ternary operator ... ? ... : ...:\n\
    int max2(int a, int b) {\n    return a > b ? a : b;\n}\n\nOn a x86-64 machine,\
    \ GCC -S generates the assembly below.\n:max1\n    movl    %edi, -4(%rbp)\n  \
    \  movl    %esi, -8(%rbp)\n    movl    -4(%rbp), %eax\n    cmpl    -8(%rbp), %eax\n\
    \    jle     .L2\n    movl    -4(%rbp), %eax\n    movl    %eax, -12(%rbp)\n  \
    \  jmp     .L4\n.L2:\n    movl    -8(%rbp), %eax\n    movl    %eax, -12(%rbp)\n\
    .L4:\n    movl    -12(%rbp), %eax\n    leave\n    ret\n\n:max2\n    movl    %edi,\
    \ -4(%rbp)\n    movl    %esi, -8(%rbp)\n    movl    -4(%rbp), %eax\n    cmpl \
    \   %eax, -8(%rbp)\n    cmovge  -8(%rbp), %eax\n    leave\n    ret\n\nmax2 uses\
    \ much less code due to the usage of instruction cmovge. But the real gain is\
    \ that max2 does not involve branch jumps, jmp, which would have a significant\
    \ performance penalty if the predicted result is not right.\nSo why does a conditional\
    \ move perform better?\nIn a typical x86 processor, the execution of an instruction\
    \ is divided into several stages. Roughly, we have different hardware to deal\
    \ with different stages. So we do not have to wait for one instruction to finish\
    \ to start a new one. This is called pipelining.\nIn a branch case, the following\
    \ instruction is determined by the preceding one, so we cannot do pipelining.\
    \ We have to either wait or predict.\nIn a conditional move case, the execution\
    \ conditional move instruction is divided into several stages, but the earlier\
    \ stages like Fetch and Decode does not depend on the result of the previous instruction;\
    \ only latter stages need the result. Thus, we wait a fraction of one instruction's\
    \ execution time. This is why the conditional move version is slower than the\
    \ branch when prediction is easy.\nThe book Computer Systems: A Programmer's Perspective,\
    \ second edition explains this in detail. You can check Section 3.6.6 for Conditional\
    \ Move Instructions, entire Chapter 4 for Processor Architecture, and Section\
    \ 5.11.2 for a special treatment for Branch Prediction and Misprediction Penalties.\n\
    Sometimes, some modern compilers can optimize our code to assembly with better\
    \ performance, sometimes some compilers can't (the code in question is using Visual\
    \ Studio's native compiler). Knowing the performance difference between branch\
    \ and conditional move when unpredictable can help us write code with better performance\
    \ when the scenario gets so complex that the compiler can not optimize them automatically.\n"
- - What is the â-->â operator in C++?
  - "\n--> is not an operator. It is in fact two separate operators, -- and >.\nThe\
    \ conditional's code decrements x, while returning x's original (not decremented)\
    \ value, and then compares the original value with 0 using the > operator.\nTo\
    \ better understand, the statement could be written as follows:\nwhile( (x--)\
    \ > 0 )\n\n"
- - What is the â-->â operator in C++?
  - "\nOr for something completely different... x slides to 0\nwhile (x --\\\n   \
    \         \\\n             \\\n              \\\n               > 0)\n     printf(\"\
    %d \", x);\n\nNot so mathematical, but... every picture paints a thousand words...\n"
- - What is the â-->â operator in C++?
  - "\nThat's a very complicated operator, so even ISO/IEC JTC1 (Joint Technical Committee\
    \ 1) placed its description in two different parts of the C++ Standard.\nJoking\
    \ aside, they are two different operators: -- and > described respectively in\
    \ Â§5.2.6/2 and Â§5.9 of the C++03 Standard.\n"
- - The Definitive C++ Book Guide and List
  - "\nBeginner\nIntroductory, no previous programming experience\n\nC++ Primer *\
    \ (Stanley Lippman, JosÃ©e Lajoie, and Barbara E. Moo)  (updated for C++11) Coming\
    \ at 1k pages, this is a very thorough introduction into C++ that covers just\
    \ about everything in the language in a very accessible format and in great detail.\
    \ The fifth edition (released August 16, 2012) covers C++11. [Review] \nProgramming:\
    \ Principles and Practice Using C++ (Bjarne Stroustrup, 2nd Edition - May 25,\
    \ 2014) (updated for C++11/C++14) An introduction to programming using C++ by\
    \ the creator of the language. A good read, that assumes no previous programming\
    \ experience, but is not only for beginners. \n\n\n* Not to be confused with C++\
    \ Primer Plus (Stephen Prata), with a significantly less favorable review.\n\n\
    Introductory, with previous programming experience\n\nA Tour of C++ (Bjarne Stroustrup)\
    \ (2nd edition  for C++17) The âtourâ is a quick (about 180 pages and 14 chapters)\
    \ tutorial overview of all of standard C++ (language and standard library, and\
    \ using C++11) at a moderately high level for people who already know C++ or at\
    \ least are experienced programmers. This book is an extended version of the material\
    \ that constitutes Chapters 2-5 of The C++ Programming Language, 4th edition.\n\
    Accelerated C++ (Andrew Koenig and Barbara Moo, 1st Edition - August 24, 2000)\
    \  This basically covers the same ground as the C++ Primer, but does so on a fourth\
    \ of its space. This is largely because it does not attempt to be an introduction\
    \ to programming, but an introduction to C++ for people who've previously programmed\
    \ in some other language. It has a steeper learning curve, but, for those who\
    \ can cope with this, it is a very compact introduction to the language. (Historically,\
    \ it broke new ground by being the first beginner's book to use a modern approach\
    \ to teaching the language.) Despite this, the C++\nit teaches is purely C++98.\
    \ [Review]\n\nBest practices\n\nEffective C++ (Scott Meyers, 3rd Edition - May\
    \ 22, 2005)  This was written with the aim of being the best second book C++ programmers\
    \ should read, and it succeeded. Earlier editions were aimed at programmers coming\
    \ from C, the third edition changes this and targets programmers coming from languages\
    \ like Java. It presents ~50 easy-to-remember rules of thumb along with their\
    \ rationale in a very accessible (and enjoyable) style. For C++11 and C++14 the\
    \ examples and a few issues are outdated and Effective Modern C++ should be preferred.\
    \ [Review]\nEffective Modern C++ (Scott Meyers) This is basically the new version\
    \ of Effective C++, aimed at C++ programmers making the transition from C++03\
    \ to C++11 and C++14. \nEffective STL (Scott Meyers)  This aims to do the same\
    \ to the part of the standard library coming from the STL what Effective C++ did\
    \ to the language as a whole: It presents rules of thumb along with their rationale.\
    \ [Review]\n\n\nIntermediate\n\nMore Effective C++ (Scott Meyers) Even more rules\
    \ of thumb than Effective C++. Not as important as the ones in the first book,\
    \ but still good to know.\nExceptional C++ (Herb Sutter)  Presented as a set of\
    \ puzzles, this has one of the best and thorough discussions of the proper resource\
    \ management and exception safety in C++ through Resource Acquisition is Initialization\
    \ (RAII) in addition to in-depth coverage of a variety of other topics including\
    \ the pimpl idiom, name lookup, good class design, and the C++ memory model. [Review]\n\
    More Exceptional C++ (Herb Sutter)  Covers additional exception safety topics\
    \ not covered in Exceptional C++, in addition to discussion of effective object-oriented\
    \ programming in C++ and correct use of the STL. [Review]\nExceptional C++ Style\
    \ (Herb Sutter)  Discusses generic programming, optimization, and resource management;\
    \ this book also has an excellent exposition of how to write modular code in C++\
    \ by using non-member functions and the single responsibility principle. [Review]\n\
    C++ Coding Standards (Herb Sutter and Andrei Alexandrescu) âCoding standardsâ\
    \ here doesn't mean âhow many spaces should I indent my code?â  This book contains\
    \ 101 best practices, idioms, and common pitfalls that can help you to write correct,\
    \ understandable, and efficient C++ code. [Review]\nC++ Templates: The Complete\
    \ Guide (David Vandevoorde and Nicolai M. Josuttis) This is the book about templates\
    \ as they existed before C++11.  It covers everything from the very basics to\
    \ some of the most advanced template metaprogramming and explains every detail\
    \ of how templates work (both conceptually and at how they are implemented) and\
    \ discusses many common pitfalls.  Has excellent summaries of the One Definition\
    \ Rule (ODR) and overload resolution in the appendices. A second edition covering\
    \ C++11, C++14 and C++17 has been already published . [Review]\nC++ 17 - The Complete\
    \ Guide (Nicolai M. Josuttis) This book describes all the new features introduced\
    \ in the C++17 Standard covering everything from the simple ones like 'Inline\
    \ Variables', 'constexpr if' all the way up to 'Polymorphic Memory Resources'\
    \ and 'New and Delete with overaligned Data'.\n\n\nAdvanced\n\nModern C++ Design\
    \ (Andrei Alexandrescu)  A groundbreaking book on advanced generic programming\
    \ techniques.  Introduces policy-based design, type lists, and fundamental generic\
    \ programming idioms then explains how many useful design patterns (including\
    \ small object allocators, functors, factories, visitors, and multi-methods) can\
    \ be implemented efficiently, modularly, and cleanly using generic programming.\
    \ [Review]\nC++ Template Metaprogramming (David Abrahams and Aleksey Gurtovoy)\n\
    C++ Concurrency In Action (Anthony Williams) A book covering C++11 concurrency\
    \ support including the thread library, the atomics library, the C++ memory model,\
    \ locks and mutexes, as well as issues of designing and debugging multithreaded\
    \ applications.\nAdvanced C++ Metaprogramming (Davide Di Gennaro) A pre-C++11\
    \ manual of TMP techniques, focused more on practice than theory.  There are a\
    \ ton of snippets in this book, some of which are made obsolete by type traits,\
    \ but the techniques, are nonetheless useful to know.  If you can put up with\
    \ the quirky formatting/editing, it is easier to read than Alexandrescu, and arguably,\
    \ more rewarding.  For more experienced developers, there is a good chance that\
    \ you may pick up something about a dark corner of C++ (a quirk) that usually\
    \ only comes about through extensive experience.\n\n\nReference Style - All Levels\n\
    \nThe C++ Programming Language (Bjarne Stroustrup) (updated for C++11) The classic\
    \ introduction to C++ by its creator. Written to parallel the classic K&R, this\
    \ indeed reads very much like it and covers just about everything from the core\
    \ language to the standard library, to programming paradigms to the language's\
    \ philosophy. [Review] Note: All releases of the C++ standard are tracked in this\
    \ question: Where do I find the current C++ standard.   \nC++ Standard Library\
    \ Tutorial and Reference (Nicolai Josuttis) (updated for C++11) The introduction\
    \ and reference for the C++ Standard Library. The second edition (released on\
    \ April 9, 2012) covers C++11. [Review]\nThe C++ IO Streams and Locales (Angelika\
    \ Langer and Klaus Kreft)  There's very little to say about this book except that,\
    \ if you want to know anything about streams and locales, then this is the one\
    \ place to find definitive answers. [Review]\n\nC++11/14/17/â¦ References:\n\n\
    The C++11/14/17 Standard (INCITS/ISO/IEC 14882:2011/2014/2017) This, of course,\
    \ is the final arbiter of all that is or isn't C++. Be aware, however, that it\
    \ is intended purely as a reference for experienced users willing to devote considerable\
    \ time and effort to its understanding. The C++17 standard is released in electronic\
    \ form for 198 Swiss Francs.\nThe C++17 standard is available, but seemingly not\
    \ in an economical form â directly from the ISO it costs 198 Swiss Francs (about\
    \ $200 US). For most people, the final draft before standardization is more than\
    \ adequate (and free). Many will prefer an even newer draft, documenting new features\
    \ that are likely to be included in C++20.\nOverview of the New C++ (C++11/14)\
    \ (PDF only) (Scott Meyers) (updated for C++14) These are the presentation materials\
    \ (slides and some lecture notes) of a three-day training course offered by Scott\
    \ Meyers, who's a highly respected author on C++. Even though the list of items\
    \ is short, the quality is high.\nThe C++ Core Guidelines (C++11/14/17/â¦) (edited\
    \ by Bjarne Stroustrup and Herb Sutter) is an evolving online document consisting\
    \ of a set of guidelines for using modern C++ well. The guidelines are focused\
    \ on relatively higher-level issues, such as interfaces, resource management,\
    \ memory management and concurrency affecting application architecture and library\
    \ design. The project was announced at CppCon'15 by Bjarne Stroustrup and others\
    \ and welcomes contributions from the community. Most guidelines are supplemented\
    \ with a rationale and examples as well as discussions of possible tool support.\
    \ Many rules are designed specifically to be automatically checkable by static\
    \ analysis tools.\nThe C++ Super-FAQ (Marshall Cline, Bjarne Stroustrup and others)\
    \ is an effort by the Standard C++ Foundation to unify the C++ FAQs previously\
    \ maintained individually by Marshall Cline and Bjarne Stroustrup and also incorporating\
    \ new contributions. The items mostly address issues at an intermediate level\
    \ and are often written with a humorous tone. Not all items might be fully up\
    \ to date with the latest edition of the C++ standard yet.\ncppreference.com (C++03/11/14/17/â¦)\
    \ (initiated by Nate Kohl) is a wiki that summarizes the basic core-language features\
    \ and has extensive documentation of the C++ standard library. The documentation\
    \ is very precise but is easier to read than the official standard document and\
    \ provides better navigation due to its wiki nature. The project documents all\
    \ versions of the C++ standard and the site allows filtering the display for a\
    \ specific version. The project was presented by Nate Kohl at CppCon'14.\n\n\n\
    Classics / Older\nNote: Some information contained within these books may not\
    \ be up-to-date or no longer considered best practice.\n\nThe Design and Evolution\
    \ of C++ (Bjarne Stroustrup)  If you want to know why the language is the way\
    \ it is, this book is where you find answers. This covers everything before the\
    \ standardization of C++.\nRuminations on C++ - (Andrew Koenig and Barbara Moo)\
    \ [Review]\nAdvanced C++ Programming Styles and Idioms (James Coplien)  A predecessor\
    \ of the pattern movement, it describes many C++-specific âidiomsâ. It's certainly\
    \ a very good book and might still be worth a read if you can spare the time,\
    \ but quite old and not up-to-date with current C++. \nLarge Scale C++ Software\
    \ Design (John Lakos)  Lakos explains techniques to manage very big C++ software\
    \ projects. Certainly, a good read, if it only was up to date. It was written\
    \ long before C++ 98 and misses on many features (e.g. namespaces) important for\
    \ large-scale projects. If you need to work in a big C++ software project, you\
    \ might want to read it, although you need to take more than a grain of salt with\
    \ it. The first volume of a new edition is expected in 2018.\nInside the C++ Object\
    \ Model (Stanley Lippman)  If you want to know how virtual member functions are\
    \ commonly implemented and how base objects are commonly laid out in memory in\
    \ a multi-inheritance scenario, and how all this affects performance, this is\
    \ where you will find thorough discussions of such topics.\nThe Annotated C++\
    \ Reference Manual (Bjarne Stroustrup, Margaret A. Ellis) This book is quite outdated\
    \ in the fact that it explores the 1989 C++ 2.0 version - Templates, exceptions,\
    \ namespaces and new casts were not yet introduced. Saying that however, this\
    \ book goes through the entire C++ standard of the time explaining the rationale,\
    \ the possible implementations, and features of the language. This is not a book\
    \ to learn programming principles and patterns on C++, but to understand every\
    \ aspect of the C++ language.\nThinking in C++ (Bruce Eckel, 2nd Edition, 2000).\
    \  Two volumes; is a tutorial style free set of intro level books. Downloads:\
    \ vol 1, vol 2. Unfortunately theyâre marred by a number of trivial errors (e.g.\
    \ maintaining that temporaries are automatically const), with no official errata\
    \ list. A partial 3rd party errata list is available at (http://www.computersciencelab.com/Eckel.htm),\
    \ but itâs apparently not maintained.\nScientific and Engineering C++: An Introduction\
    \ to Advanced Techniques and Examples (John Barton and Lee Nackman) \nIt is a\
    \ comprehensive and very detailed book that tried to explain and make use of all\
    \ the features available in C++, in the context of numerical methods. It introduced\
    \ at the time several new techniques, such as the Curiously Recurring Template\
    \ Pattern (CRTP, also called Barton-Nackman trick).\nIt pioneered several techniques\
    \ such as dimensional analysis and automatic differentiation. \nIt came with a\
    \ lot of compilable and useful code, ranging from an expression parser to a Lapack\
    \ wrapper. \nThe code is still available here: http://www.informit.com/store/scientific-and-engineering-c-plus-plus-an-introduction-9780201533934.\n\
    Unfortunately, the books have become somewhat outdated in the style and C++ features,\
    \ however, it was an incredible tour-de-force at the time (1994, pre-STL).\nThe\
    \ chapters on dynamics inheritance are a bit complicated to understand and not\
    \ very useful.\nAn updated version of this classic book that includes move semantics\
    \ and the lessons learned from the STL would be very nice.\n\n"
- - What are the differences between a pointer variable and a reference variable in
    C++?
  - "\n\nA pointer can be re-assigned: \nint x = 5;\nint y = 6;\nint *p;\np =  &x;\n\
    p = &y;\n*p = 10;\nassert(x == 5);\nassert(y == 10);\n\nA reference cannot, and\
    \ must be assigned at initialization:\nint x = 5;\nint y = 6;\nint &r = x;\n\n\
    A pointer has its own memory address and size on the stack (4 bytes on x86), whereas\
    \ a reference shares the same memory address (with the original variable) but\
    \ also takes up some space on the stack.  Since a reference has the same address\
    \ as the original variable itself, it is safe to think of a reference as another\
    \ name for the same variable.  Note: What a pointer points to can be on the stack\
    \ or heap.  Ditto a reference. My claim in this statement is not that a pointer\
    \ must point to the stack.  A pointer is just a variable that holds a memory address.\
    \  This variable is on the stack.  Since a reference has its own space on the\
    \ stack, and since the address is the same as the variable it references.  More\
    \ on stack vs heap.  This implies that there is a real address of a reference\
    \ that the compiler will not tell you. \nint x = 0;\nint &r = x;\nint *p = &x;\n\
    int *p2 = &r;\nassert(p == p2);\n\nYou can have pointers to pointers to pointers\
    \ offering extra levels of indirection.  Whereas references only offer one level\
    \ of indirection. \nint x = 0;\nint y = 0;\nint *p = &x;\nint *q = &y;\nint **pp\
    \ = &p;\npp = &q;//*pp = q\n**pp = 4;\nassert(y == 4);\nassert(x == 0);\n\nPointer\
    \ can be assigned nullptr directly, whereas reference cannot. If you try hard\
    \ enough, and you know how, you can make the address of a reference nullptr. \
    \ Likewise, if you try hard enough you can have a reference to a pointer, and\
    \ then that reference can contain nullptr.\nint *p = nullptr;\nint &r = nullptr;\
    \ <--- compiling error\nint &r = *p;  <--- likely no compiling error, especially\
    \ if the nullptr is hidden behind a function call, yet it refers to a non-existent\
    \ int at address 0\n\nPointers can iterate over an array, you can use ++ to go\
    \ to the next item that a pointer is pointing to, and + 4 to go to the 5th element.\
    \  This is no matter what size the object is that the pointer points to.\nA pointer\
    \ needs to be dereferenced with * to access the memory location it points to,\
    \ whereas a reference can be used directly.  A pointer to a class/struct uses\
    \ -> to access it's members whereas a reference uses a ..\nA pointer is a variable\
    \ that holds a memory address.  Regardless of how a reference is implemented,\
    \ a reference has the same memory address as the item it references.\nReferences\
    \ cannot be stuffed into an array, whereas pointers can be (Mentioned by user\
    \ @litb)\nConst references can be bound to temporaries. Pointers cannot (not without\
    \ some indirection):\nconst int &x = int(12); //legal C++\nint *y = &int(12);\
    \ //illegal to dereference a temporary.\n\nThis makes const& safer for use in\
    \ argument lists and so forth.\n\n"
- - What are the differences between a pointer variable and a reference variable in
    C++?
  - "\nWhat's a C++ reference (for C programmers)\nA reference can be thought of as\
    \ a constant pointer (not to be confused with a pointer to a constant value!)\
    \ with automatic indirection, ie the compiler will apply the * operator for you.\n\
    All references must be initialized with a non-null value or compilation will fail.\
    \ It's neither possible to get the address of a reference - the address operator\
    \ will return the address of the referenced value instead - nor is it possible\
    \ to do arithmetics on references.\nC programmers might dislike C++ references\
    \ as it will no longer be obvious when indirection happens or if an argument gets\
    \ passed by value or by pointer without looking at function signatures.\nC++ programmers\
    \ might dislike using pointers as they are considered unsafe - although references\
    \ aren't really any safer than constant pointers except in the most trivial cases\
    \ - lack the convenience of automatic indirection and carry a different semantic\
    \ connotation.\nConsider the following statement from the C++ FAQ:\n\nEven though\
    \ a reference is often implemented using an address in the\n  underlying assembly\
    \ language, please do not think of a reference as a\n  funny looking pointer to\
    \ an object. A reference is the object. It is\n  not a pointer to the object,\
    \ nor a copy of the object. It is the\n  object.\n\nBut if a reference really\
    \ were the object, how could there be dangling references? In unmanaged languages,\
    \ it's impossible for references to be any 'safer' than pointers - there generally\
    \ just isn't a way to reliably alias values across scope boundaries!\nWhy I consider\
    \ C++ references useful\nComing from a C background, C++ references may look like\
    \ a somewhat silly concept, but one should still use them instead of pointers\
    \ where possible: Automatic indirection is convenient, and references become especially\
    \ useful when dealing with RAII - but not because of any perceived safety advantage,\
    \ but rather because they make writing idiomatic code less awkward.\nRAII is one\
    \ of the central concepts of C++, but it interacts non-trivially with copying\
    \ semantics. Passing objects by reference avoids these issues as no copying is\
    \ involved. If references were not present in the language, you'd have to use\
    \ pointers instead, which are more cumbersome to use, thus violating the language\
    \ design principle that the best-practice solution should be easier than the alternatives.\n"
- - What are the differences between a pointer variable and a reference variable in
    C++?
  - "\nIf you want to be really pedantic, there is one thing you can do with a reference\
    \ that you can't do with a pointer: extend the lifetime of a temporary object.\
    \ In C++ if you bind a const reference to a temporary object, the lifetime of\
    \ that object becomes the lifetime of the reference.\nstd::string s1 = \"123\"\
    ;\nstd::string s2 = \"456\";\n\nstd::string s3_copy = s1 + s2;\nconst std::string&\
    \ s3_reference = s1 + s2;\n\nIn this example s3_copy copies the temporary object\
    \ that is a result of the concatenation. Whereas s3_reference in essence becomes\
    \ the temporary object. It's really a reference to a temporary object that now\
    \ has the same lifetime as the reference. \nIf you try this without the const\
    \ it should fail to compile. You cannot bind a non-const reference to a temporary\
    \ object, nor can you take its address for that matter.\n"
- - How do I iterate over the words of a string?
  - "\nFor what it's worth, here's another way to extract tokens from an input string,\
    \ relying only on standard library facilities. It's an example of the power and\
    \ elegance behind the design of the STL.\n#include <iostream>\n#include <string>\n\
    #include <sstream>\n#include <algorithm>\n#include <iterator>\n\nint main() {\n\
    \    using namespace std;\n    string sentence = \"And I feel fine...\";\n   \
    \ istringstream iss(sentence);\n    copy(istream_iterator<string>(iss),\n    \
    \     istream_iterator<string>(),\n         ostream_iterator<string>(cout, \"\\\
    n\"));\n}\n\nInstead of copying the extracted tokens to an output stream, one\
    \ could insert them into a container, using the same generic copy algorithm.\n\
    vector<string> tokens;\ncopy(istream_iterator<string>(iss),\n     istream_iterator<string>(),\n\
    \     back_inserter(tokens));\n\n... or create the vector directly:\nvector<string>\
    \ tokens{istream_iterator<string>{iss},\n                      istream_iterator<string>{}};\n\
    \n"
- - How do I iterate over the words of a string?
  - "\nI use this to split string by a delimiter. The first puts the results in a\
    \ pre-constructed vector, the second returns a new vector.\n#include <string>\n\
    #include <sstream>\n#include <vector>\n#include <iterator>\n\ntemplate<typename\
    \ Out>\nvoid split(const std::string &s, char delim, Out result) {\n    std::stringstream\
    \ ss(s);\n    std::string item;\n    while (std::getline(ss, item, delim)) {\n\
    \        *(result++) = item;\n    }\n}\n\nstd::vector<std::string> split(const\
    \ std::string &s, char delim) {\n    std::vector<std::string> elems;\n    split(s,\
    \ delim, std::back_inserter(elems));\n    return elems;\n}\n\n\nNote that this\
    \ solution does not skip empty tokens, so the following will find 4 items, one\
    \ of which is empty:\nstd::vector<std::string> x = split(\"one:two::three\", ':');\n\
    \n"
- - How do I iterate over the words of a string?
  - "\nA possible solution using Boost might be:\n#include <boost/algorithm/string.hpp>\n\
    std::vector<std::string> strs;\nboost::split(strs, \"string to split\", boost::is_any_of(\"\
    \\t \"));\n\nThis approach might be even faster than the stringstream approach.\
    \ And since this is a generic template function it can be used to split other\
    \ types of strings (wchar, etc. or UTF-8) using all kinds of delimiters.\nSee\
    \ the documentation for details.\n"
- - What does the explicit keyword mean?
  - "\nThe compiler is allowed to make one implicit conversion to resolve the parameters\
    \ to a function. What this means is that the compiler can use constructors callable\
    \ with a single parameter to convert from one type to another in order to get\
    \ the right type for a parameter. \nHere's an example class with a constructor\
    \ that can be used for implicit conversions:\nclass Foo\n{\npublic:\n  // single\
    \ parameter constructor, can be used as an implicit conversion\n  Foo (int foo)\
    \ : m_foo (foo) \n  {\n  }\n\n  int GetFoo () { return m_foo; }\n\nprivate:\n\
    \  int m_foo;\n};\n\nHere's a simple function that takes a Foo object:\nvoid DoBar\
    \ (Foo foo)\n{\n  int i = foo.GetFoo ();\n}\n\nand here's where the DoBar function\
    \ is called.\nint main ()\n{\n  DoBar (42);\n}\n\nThe argument is not a Foo object,\
    \ but an int. However, there exists a constructor for Foo that takes an int so\
    \ this constructor can be used to convert the parameter to the correct type.\n\
    The compiler is allowed to do this once for each parameter.\nPrefixing the explicit\
    \ keyword to the constructor prevents the compiler from using that constructor\
    \ for implicit conversions. Adding it to the above class will create a compiler\
    \ error at the function call DoBar (42).  It is now necessary to call for conversion\
    \ explicitly with  DoBar (Foo (42))\nThe reason you might want to do this is to\
    \ avoid accidental construction that can hide bugs.  Contrived example:\n\nYou\
    \ have a MyString(int size) class with a constructor that constructs a string\
    \ of the given size.  You have a function print(const MyString&), and you call\
    \ print(3) (when you actually intended to call print(\"3\")).  You expect it to\
    \ print \"3\", but it prints an empty string of length 3 instead.\n\n"
- - What does the explicit keyword mean?
  - "\nSuppose, you have a class String:\nclass String {\npublic:\n    String(int\
    \ n); // allocate n bytes to the String object\n    String(const char *p); //\
    \ initializes object with char *p\n};\n\nNow, if you try:\nString mystring = 'x';\n\
    \nThe character 'x' will be implicitly converted to int and then the String(int)\
    \ constructor will be called. But, this is not what the user might have intended.\
    \ So, to prevent such conditions, we shall define the constructor as explicit:\n\
    class String {\npublic:\n    explicit String (int n); //allocate n bytes\n   \
    \ String(const char *p); // initialize sobject with string p\n};\n\n"
- - What does the explicit keyword mean?
  - "\nIn C++, a constructor with only one required parameter is considered an implicit\
    \ conversion function.  It converts the parameter type to the class type.  Whether\
    \ this is a good thing or not depends on the semantics of the constructor.\nFor\
    \ example, if you have a string class with constructor String(const char* s),\
    \ that's probably exactly what you want.  You can pass a const char* to a function\
    \ expecting a String, and the compiler will automatically construct a temporary\
    \ String object for you.\nOn the other hand, if you have a buffer class whose\
    \ constructor Buffer(int size) takes the size of the buffer in bytes, you probably\
    \ don't want the compiler to quietly turn ints into Buffers.  To prevent that,\
    \ you declare the constructor with the explicit keyword:\nclass Buffer { explicit\
    \ Buffer(int size); ... }\n\nThat way,\nvoid useBuffer(Buffer& buf);\nuseBuffer(4);\n\
    \nbecomes a compile-time error.  If you want to pass a temporary Buffer object,\
    \ you have to do so explicitly:\nuseBuffer(Buffer(4));\n\nIn summary, if your\
    \ single-parameter constructor converts the parameter into an object of your class,\
    \ you probably don't want to use the explicit keyword.  But if you have a constructor\
    \ that simply happens to take a single parameter, you should declare it as explicit\
    \ to prevent the compiler from surprising you with unexpected conversions.\n"
- - Why is âusing namespace stdâ considered bad practice?
  - "\nThis is not related to performance at all. But consider this: you are using\
    \ two libraries called Foo and Bar:\nusing namespace foo;\nusing namespace bar;\n\
    \nEverything works fine, you can call Blah() from Foo and Quux() from Bar without\
    \ problems. But one day you upgrade to a new version of Foo 2.0, which now offers\
    \ a function called Quux(). Now you've got a conflict: Both Foo 2.0 and Bar import\
    \ Quux() into your global namespace. This is going to take some effort to fix,\
    \ especially if the function parameters happen to match.\nIf you had used foo::Blah()\
    \ and bar::Quux(), then the introduction of foo::Quux() would have been a non-event.\n"
- - Why is âusing namespace stdâ considered bad practice?
  - "\nI agree with everything Greg wrote, but I'd like to add: It can even get worse\
    \ than Greg said!\nLibrary Foo 2.0 could introduce a function, Quux(), that is\
    \ an unambiguously better match for some of your calls to Quux() than the bar::Quux()\
    \ your code called for years. Then your code still compiles, but it silently calls\
    \ the wrong function and does god-knows-what. That's about as bad as things can\
    \ get.\nKeep in mind that the std namespace has tons of identifiers, many of which\
    \ are very common ones (think list, sort, string, iterator, etc.) which are very\
    \ likely to appear in other code, too.\nIf you consider this unlikely: There was\
    \ a question asked here on Stack Overflow where pretty much exactly this happened\
    \ (wrong function called due to omitted std:: prefix) about half a year after\
    \ I gave this answer. Here is another, more recent example of such a question.\n\
    So this is a real problem.\n\nHere's one more data point: Many, many years ago,\
    \ I also used to find it annoying having to prefix everything from the standard\
    \ library with std::. Then I worked in a project where it was decided at the start\
    \ that both using directives and declarations are banned except for function scopes.\
    \ Guess what? It took most of us very few weeks to get used to writing the prefix,\
    \ and after a few more weeks most of us even agreed that it actually made the\
    \ code more readable. There's a reason for that: Whether you like shorter or longer\
    \ prose is subjective, but the prefixes objectively add clarity to the code. Not\
    \ only the compiler, but you, too, find it easier to see which identifier is referred\
    \ to.\nIn a decade, that project grew to have several million lines of code. Since\
    \ these discussions come up again and again, I once was curious how often the\
    \ (allowed) function-scope using actually was used in the project. I grep'd the\
    \ sources for it and only found one or two dozen places where it was used. To\
    \ me this indicates that, once tried, developers don't find std:: painful enough\
    \ to employ using directives even once every 100 kLoC even where it was allowed\
    \ to be used.\n\nBottom line: Explicitly prefixing everything doesn't do any harm,\
    \ takes very little getting used to, and has objective advantages. In particular,\
    \ it makes the code easier to interpret by the compiler and by human readers â\
    \ and that should probably be the main goal when writing code.\n"
- - Why is âusing namespace stdâ considered bad practice?
  - "\nThe problem with putting using namespace in the header files of your classes\
    \ is that it forces anyone who wants to use your classes (by including your header\
    \ files) to also be 'using' (i.e. seeing everything in) those other namespaces.\n\
    However, you may feel free to put a using statement in your (private) *.cpp files.\n\
    \nBeware that some people disagree with my saying \"feel free\" like this -- because\
    \ although a using statement in a cpp file is better than in a header (because\
    \ it doesn't affect people who include your header file), they think it's still\
    \ not good (because depending on the code it could make the implementation of\
    \ the class more difficult to maintain). This FAQ topic says,\n\nThe using-directive\
    \ exists for legacy C++ code and to ease the transition to namespaces, but you\
    \ probably shouldnât use it on a regular basis, at least not in your new C++ code.\n\
    \nThe FAQ suggests two alternatives:\n\nA using-declaration:\nusing std::cout;\
    \ // a using-declaration lets you use cout without qualification\ncout << \"Values:\"\
    ;\n\nJust typing std::\nstd::cout << \"Values:\";\n\n\n"
- - How do you set, clear, and toggle a single bit?
  - "\nSetting a bit\nUse the bitwise OR operator (|) to set a bit.\nnumber |= 1UL\
    \ << n;\n\nThat will set the nth bit of number. n should be zero, if you want\
    \ to set the 1st bit and so on upto n-1, if you want to set the nth bit.\nUse\
    \ 1ULL if number is wider than unsigned long; promotion of 1UL << n doesn't happen\
    \ until after evaluating 1UL << n where it's undefined behaviour to shift by more\
    \ than the width of a long.  The same applies to all the rest of the examples.\n\
    Clearing a bit\nUse the bitwise AND operator (&) to clear a bit.\nnumber &= ~(1UL\
    \ << n);\n\nThat will clear the nth bit of number. You must invert the bit string\
    \ with the bitwise NOT operator (~), then AND it.\nToggling a bit\nThe XOR operator\
    \ (^) can be used to toggle a bit.\nnumber ^= 1UL << n;\n\nThat will toggle the\
    \ nth bit of number.\nChecking a bit\nYou didn't ask for this, but I might as\
    \ well add it.\nTo check a bit, shift the number n to the right, then bitwise\
    \ AND it:\nbit = (number >> n) & 1U;\n\nThat will put the value of the nth bit\
    \ of number into the variable bit.\nChanging the nth bit to x\nSetting the nth\
    \ bit to either 1 or 0 can be achieved with the following on a 2's complement\
    \ C++ implementation:\nnumber ^= (-x ^ number) & (1UL << n);\n\nBit n will be\
    \ set if x is 1, and cleared if x is 0.  If x has some other value, you get garbage.\
    \  x = !!x will booleanize it to 0 or 1.\nTo make this independent of 2's complement\
    \ negation behaviour (where -1 has all bits set, unlike on a 1's complement or\
    \ sign/magnitude C++ implementation), use unsigned negation.\nnumber ^= (-(unsigned\
    \ long)x ^ number) & (1UL << n);\n\nor\nunsigned long newbit = !!x;    // Also\
    \ booleanize to force 0 or 1\nnumber ^= (-newbit ^ number) & (1UL << n);\n\nIt's\
    \ generally a good idea to use unsigned types for portable bit manipulation.\n\
    It's also generally a good idea to not to copy/paste code in general and so many\
    \ people use preprocessor macros (like the community wiki answer further down)\
    \ or some sort of encapsulation.\n"
- - How do you set, clear, and toggle a single bit?
  - "\nUsing the Standard C++ Library: std::bitset<N>.\nOr the Boost version: boost::dynamic_bitset.\n\
    There is no need to roll your own:\n#include <bitset>\n#include <iostream>\n\n\
    int main()\n{\n    std::bitset<5> x;\n\n    x[1] = 1;\n    x[2] = 0;\n    // Note\
    \ x[0-4]  valid\n\n    std::cout << x << std::endl;\n}\n\n\n[Alpha:] > ./a.out\n\
    00010\n\nThe Boost version allows a runtime sized bitset compared with a standard\
    \ library compile-time sized bitset.\n"
- - How do you set, clear, and toggle a single bit?
  - "\nThe other option is to use bit fields:\nstruct bits {\n    unsigned int a:1;\n\
    \    unsigned int b:1;\n    unsigned int c:1;\n};\n\nstruct bits mybits;\n\ndefines\
    \ a 3-bit field (actually, it's three 1-bit felds). Bit operations now become\
    \ a bit (haha) simpler:\nTo set or clear a bit:\nmybits.b = 1;\nmybits.c = 0;\n\
    \nTo toggle a bit:\nmybits.a = !mybits.a;\nmybits.b = ~mybits.b;\nmybits.c ^=\
    \ 1;  /* all work */\n\nChecking a bit:\nif (mybits.c)  //if mybits.c is non zero\
    \ the next line below will execute\n\nThis only works with fixed-size bit fields.\
    \ Otherwise you have to resort to the bit-twiddling techniques described in previous\
    \ posts.\n"
- - When should static_cast, dynamic_cast, const_cast and reinterpret_cast be used?
  - "\nstatic_cast is the first cast you should attempt to use. It does things like\
    \ implicit conversions between types (such as int to float, or pointer to void*),\
    \ and it can also call explicit conversion functions (or implicit ones). In many\
    \ cases, explicitly stating static_cast isn't necessary, but it's important to\
    \ note that the T(something) syntax is equivalent to (T)something and should be\
    \ avoided (more on that later). A T(something, something_else) is safe, however,\
    \ and guaranteed to call the constructor.\nstatic_cast can also cast through inheritance\
    \ hierarchies. It is unnecessary when casting upwards (towards a base class),\
    \ but when casting downwards it can be used as long as it doesn't cast through\
    \ virtual inheritance. It does not do checking, however, and it is undefined behavior\
    \ to static_cast down a hierarchy to a type that isn't actually the type of the\
    \ object.\n\nconst_cast can be used to remove or add const to a variable; no other\
    \ C++ cast is capable of removing it (not even reinterpret_cast). It is important\
    \ to note that modifying a formerly const value is only undefined if the original\
    \ variable is const; if you use it to take the const off a reference to something\
    \ that wasn't declared with const, it is safe. This can be useful when overloading\
    \ member functions based on const, for instance. It can also be used to add const\
    \ to an object, such as to call a member function overload.\nconst_cast also works\
    \ similarly on volatile, though that's less common.\n\ndynamic_cast is almost\
    \ exclusively used for handling polymorphism. You can cast a pointer or reference\
    \ to any polymorphic type to any other class type (a polymorphic type has at least\
    \ one virtual function, declared or inherited). You can use it for more than just\
    \ casting downwards -- you can cast sideways or even up another chain. The dynamic_cast\
    \ will seek out the desired object and return it if possible. If it can't, it\
    \ will return nullptr in the case of a pointer, or throw std::bad_cast in the\
    \ case of a reference.\ndynamic_cast has some limitations, though. It doesn't\
    \ work if there are multiple objects of the same type in the inheritance hierarchy\
    \ (the so-called 'dreaded diamond') and you aren't using virtual inheritance.\
    \ It also can only go through public inheritance - it will always fail to travel\
    \ through protected or private inheritance. This is rarely an issue, however,\
    \ as such forms of inheritance are rare.\n\nreinterpret_cast is the most dangerous\
    \ cast, and should be used very sparingly. It turns one type directly into another\
    \ - such as casting the value from one pointer to another, or storing a pointer\
    \ in an int, or all sorts of other nasty things. Largely, the only guarantee you\
    \ get with reinterpret_cast is that normally if you cast the result back to the\
    \ original type, you will get the exact same value (but not if the intermediate\
    \ type is smaller than the original type). There are a number of conversions that\
    \ reinterpret_cast cannot do, too. It's used primarily for particularly weird\
    \ conversions and bit manipulations, like turning a raw data stream into actual\
    \ data, or storing data in the low bits of an aligned pointer.\n\nC-style cast\
    \ and function-style cast are casts using (type)object or type(object), respectively.\
    \ A C-style cast is defined as the first of the following which succeeds:\n\n\
    const_cast\nstatic_cast (though ignoring access restrictions)\nstatic_cast (see\
    \ above), then const_cast\nreinterpret_cast\nreinterpret_cast, then const_cast\n\
    \nIt can therefore be used as a replacement for other casts in some instances,\
    \ but can be extremely dangerous because of the ability to devolve into a reinterpret_cast,\
    \ and the latter should be preferred when explicit casting is needed, unless you\
    \ are sure static_cast will succeed or reinterpret_cast will fail. Even then,\
    \ consider the longer, more explicit option.\nC-style casts also ignore access\
    \ control when performing a static_cast, which means that they have the ability\
    \ to perform an operation that no other cast can. This is mostly a kludge, though,\
    \ and in my mind is just another reason to avoid C-style casts.\n"
- - When should static_cast, dynamic_cast, const_cast and reinterpret_cast be used?
  - "\nUse dynamic_cast for converting pointers/references within an inheritance hierarchy.\n\
    Use static_cast for ordinary type conversions.\nUse reinterpret_cast for low-level\
    \ reinterpreting of bit patterns.  Use with extreme caution.\nUse const_cast for\
    \ casting away const/volatile.  Avoid this unless you are stuck using a const-incorrect\
    \ API.\n"
- - When should static_cast, dynamic_cast, const_cast and reinterpret_cast be used?
  - "\n(A lot of theoretical and conceptual explanation has been given above) \nBelow\
    \ are some of the practical examples when I used static_cast, dynamic_cast, const_cast,\
    \ reinterpret_cast.\n(Also referes this to understand the explaination : http://www.cplusplus.com/doc/tutorial/typecasting/)\n\
    static_cast :\nOnEventData(void* pData)\n\n{\n  ......\n\n  //  pData is a void*\
    \ pData, \n\n  //  EventData is a structure e.g. \n  //  typedef struct _EventData\
    \ {\n  //  std::string id;\n  //  std:: string remote_id;\n  //  } EventData;\n\
    \n  // On Some Situation a void pointer *pData\n  // has been static_casted as\
    \ \n  // EventData* pointer \n\n  EventData *evtdata = static_cast<EventData*>(pData);\n\
    \  .....\n}\n\ndynamic_cast :\nvoid DebugLog::OnMessage(Message *msg)\n{\n   \
    \ static DebugMsgData *debug;\n    static XYZMsgData *xyz;\n\n    if(debug = dynamic_cast<DebugMsgData*>(msg->pdata)){\n\
    \        // debug message\n    }\n    else if(xyz = dynamic_cast<XYZMsgData*>(msg->pdata)){\n\
    \        // xyz message\n    }\n    else/* if( ... )*/{\n        // ...\n    }\n\
    }\n\nconst_cast :\n// *Passwd declared as a const\n\nconst unsigned char *Passwd\n\
    \n\n// on some situation it require to remove its constness\n\nconst_cast<unsigned\
    \ char*>(Passwd)\n\nreinterpret_cast :\ntypedef unsigned short uint16;\n\n// Read\
    \ Bytes returns that 2 bytes got read. \n\nbool ByteBuffer::ReadUInt16(uint16&\
    \ val) {\n  return ReadBytes(reinterpret_cast<char*>(&val), 2);\n}\n\n"
- - Why are elementwise additions much faster in separate loops than in a combined
    loop?
  - "\nUpon further analysis of this, I believe this is (at least partially) caused\
    \ by data alignment of the four pointers. This will cause some level of cache\
    \ bank/way conflicts.\nIf I've guessed correctly on how you are allocating your\
    \ arrays, they are likely to be aligned to the page line.\nThis means that all\
    \ your accesses in each loop will fall on the same cache way. However, Intel processors\
    \ have had 8-way L1 cache associativity for a while. But in reality, the performance\
    \ isn't completely uniform. Accessing 4-ways is still slower than say 2-ways.\n\
    EDIT : It does in fact look like you are allocating all the arrays separately.\n\
    Usually when such large allocations are requested, the allocator will request\
    \ fresh pages from the OS. Therefore, there is a high chance that large allocations\
    \ will appear at the same offset from a page-boundary.\nHere's the test code:\n\
    int main(){\n    const int n = 100000;\n\n#ifdef ALLOCATE_SEPERATE\n    double\
    \ *a1 = (double*)malloc(n * sizeof(double));\n    double *b1 = (double*)malloc(n\
    \ * sizeof(double));\n    double *c1 = (double*)malloc(n * sizeof(double));\n\
    \    double *d1 = (double*)malloc(n * sizeof(double));\n#else\n    double *a1\
    \ = (double*)malloc(n * sizeof(double) * 4);\n    double *b1 = a1 + n;\n    double\
    \ *c1 = b1 + n;\n    double *d1 = c1 + n;\n#endif\n\n    //  Zero the data to\
    \ prevent any chance of denormals.\n    memset(a1,0,n * sizeof(double));\n   \
    \ memset(b1,0,n * sizeof(double));\n    memset(c1,0,n * sizeof(double));\n   \
    \ memset(d1,0,n * sizeof(double));\n\n    //  Print the addresses\n    cout <<\
    \ a1 << endl;\n    cout << b1 << endl;\n    cout << c1 << endl;\n    cout << d1\
    \ << endl;\n\n    clock_t start = clock();\n\n    int c = 0;\n    while (c++ <\
    \ 10000){\n\n#if ONE_LOOP\n        for(int j=0;j<n;j++){\n            a1[j] +=\
    \ b1[j];\n            c1[j] += d1[j];\n        }\n#else\n        for(int j=0;j<n;j++){\n\
    \            a1[j] += b1[j];\n        }\n        for(int j=0;j<n;j++){\n     \
    \       c1[j] += d1[j];\n        }\n#endif\n\n    }\n\n    clock_t end = clock();\n\
    \    cout << \"seconds = \" << (double)(end - start) / CLOCKS_PER_SEC << endl;\n\
    \n    system(\"pause\");\n    return 0;\n}\n\n\nBenchmark Results:\nEDIT: Results\
    \ on an actual Core 2 architecture machine:\n2 x Intel Xeon X5482 Harpertown @\
    \ 3.2 GHz:\n#define ALLOCATE_SEPERATE\n#define ONE_LOOP\n00600020\n006D0020\n\
    007A0020\n00870020\nseconds = 6.206\n\n#define ALLOCATE_SEPERATE\n//#define ONE_LOOP\n\
    005E0020\n006B0020\n00780020\n00850020\nseconds = 2.116\n\n//#define ALLOCATE_SEPERATE\n\
    #define ONE_LOOP\n00570020\n00633520\n006F6A20\n007B9F20\nseconds = 1.894\n\n\
    //#define ALLOCATE_SEPERATE\n//#define ONE_LOOP\n008C0020\n00983520\n00A46A20\n\
    00B09F20\nseconds = 1.993\n\nObservations:\n\n6.206 seconds with one loop and\
    \ 2.116 seconds with two loops. This reproduces the OP's results exactly.\nIn\
    \ the first two tests, the arrays are allocated separately. You'll notice that\
    \ they all have the same alignment relative to the page.\nIn the second two tests,\
    \ the arrays are packed together to break that alignment. Here you'll notice both\
    \ loops are faster. Furthermore, the second (double) loop is now the slower one\
    \ as you would normally expect.\n\nAs @Stephen Cannon points out in the comments,\
    \ there is very likely possibility that this alignment causes false aliasing in\
    \ the load/store units or the cache. I Googled around for this and found that\
    \ Intel actually has a hardware counter for partial address aliasing stalls:\n\
    http://software.intel.com/sites/products/documentation/doclib/stdxe/2013/~amplifierxe/pmw_dp/events/partial_address_alias.html\n\
    \n5 Regions - Explanations\nRegion 1:\nThis one is easy. The dataset is so small\
    \ that the performance is dominated by overhead like looping and branching.\n\
    Region 2:\nHere, as the data sizes increases, the amount of relative overhead\
    \ goes down and the performance \"saturates\". Here two loops is slower because\
    \ it has twice as much loop and branching overhead.\nI'm not sure exactly what's\
    \ going on here... Alignment could still play an effect as Agner Fog mentions\
    \ cache bank conflicts. (That link is about Sandy Bridge, but the idea should\
    \ still be applicable to Core 2.)\nRegion 3:\nAt this point, the data no longer\
    \ fits in L1 cache. So performance is capped by the L1 <-> L2 cache bandwidth.\n\
    Region 4:\nThe performance drop in the single-loop is what we are observing. And\
    \ as mentioned, this is due to the alignment which (most likely) causes false\
    \ aliasing stalls in the processor load/store units.\nHowever, in order for false\
    \ aliasing to occur, there must be a large enough stride between the datasets.\
    \ This is why you don't see this in region 3.\nRegion 5:\nAt this point, nothing\
    \ fits in cache. So you're bound by memory bandwidth.\n\n\n\n\n"
- - Why are elementwise additions much faster in separate loops than in a combined
    loop?
  - "\nOK, the right answer definitely has to do something with the CPU cache. But\
    \ to use the cache argument can be quite difficult, especially without data.\n\
    There are many answers, that led to a lot of discussion, but let's face it: Cache\
    \ issues can be very complex and are not one dimensional. They depend heavily\
    \ on the size of the data, so my question was unfair: It turned out to be at a\
    \ very interesting point in the cache graph.\n@Mysticial's answer convinced a\
    \ lot of people (including me), probably because it was the only one that seemed\
    \ to rely on facts, but it was only one \"data point\" of the truth.\nThat's why\
    \ I combined his test (using a continuous vs. separate allocation) and @James'\
    \ Answer's advice.\nThe graphs below shows, that most of the answers and especially\
    \ the majority of comments to the question and answers can be considered completely\
    \ wrong or true depending on the exact scenario and parameters used.\nNote that\
    \ my initial question was at n = 100.000. This point (by accident) exhibits special\
    \ behavior: \n\nIt possesses the greatest discrepancy between the one and two\
    \ loop'ed version (almost a factor of three)\nIt is the only point, where one-loop\
    \ (namely with continuous allocation) beats the two-loop version. (This made Mysticial's\
    \ answer possible, at all.)\n\nThe result using initialized data:\n\nThe result\
    \ using uninitialized data (this is what Mysticial tested):\n\nAnd this is a hard-to-explain\
    \ one: Initialized data, that is allocated once and reused for every following\
    \ test case of different vector size:\n\nProposal\nEvery low-level performance\
    \ related question on StackÂ Overflow should be required to provide MFLOPS information\
    \ for the whole range of cache relevant data sizes! It's a waste of everybody's\
    \ time to think of answers and especially discuss them with others without this\
    \ information.\n"
- - Why are elementwise additions much faster in separate loops than in a combined
    loop?
  - "\nThe second loop involves a lot less cache activity, so it's easier for the\
    \ processor to keep up with the memory demands.\n"
- - 'What is the difference between #include <filename> and #include âfilenameâ?'
  - "\nIn practice, the difference is in the location where the preprocessor searches\
    \ for the included file. \nFor #include <filename> the preprocessor searches in\
    \ an implementation dependent manner, normally in search directories pre-designated\
    \ by the compiler/IDE. This method is normally used to include standard library\
    \ header files.\nFor #include \"filename\" the preprocessor searches first in\
    \ the same directory as the file containing the directive, and then follows the\
    \ search path used for the #include <filename> form. This method is normally used\
    \ to include programmer-defined header files.\nA more complete description is\
    \ available in the GCC documentation on search paths.\n"
- - 'What is the difference between #include <filename> and #include âfilenameâ?'
  - "\nThe only way to know is to read your implementation's documentation.\nIn the\
    \ C standard, section 6.10.2, paragraphs 2 to 4 state:\n\n\nA preprocessing directive\
    \ of the form\n#include <h-char-sequence> new-line\n\nsearches a sequence of implementation-defined\
    \ places for a header identified uniquely by the specified sequence between the\
    \ < and > delimiters, and causes the replacement of that directive by the entire\
    \ contents of the header. How the places are specified or the header identified\
    \ is implementation-defined.\nA preprocessing directive of the form\n#include\
    \ \"q-char-sequence\" new-line\n\ncauses the replacement of that directive by\
    \ the entire contents of the source file identified by the specified sequence\
    \ between the \" delimiters. The named source file is searched for in an implementation-defined\
    \ manner. If this search is not supported, or if the search fails, the directive\
    \ is reprocessed as if it read\n#include <h-char-sequence> new-line\n\nwith the\
    \ identical contained sequence (including > characters, if any) from the original\n\
    \  directive.\nA preprocessing directive of the form\n#include pp-tokens new-line\n\
    \n(that does not match one of the two previous forms) is permitted. The preprocessing\
    \ tokens after include in the directive are processed just as in normal text.\
    \ (Each identifier currently defined as a macro name is replaced by its replacement\
    \ list of preprocessing tokens.) The directive resulting after all replacements\
    \ shall match one of the two previous forms. The method by which a sequence of\
    \ preprocessing tokens between a < and a > preprocessing token pair or a pair\
    \ of \" characters is combined into a single header name preprocessing token is\
    \ implementation-defined.\n\nDefinitions:\n\nh-char: any member of the source\
    \ character set except the new-line character and >\nq-char: any member of the\
    \ source character set except the new-line character and \"\n\n\n"
- - 'What is the difference between #include <filename> and #include âfilenameâ?'
  - "\nThe sequence of characters between < and > uniquely refer to a header, which\
    \ isn't necessarily a file. Implementations are pretty much free to use the character\
    \ sequence as they wish. (Mostly, however, just treat it as a file name and do\
    \ a search in the include path, as the other posts state.)\nIf the #include \"\
    file\" form is used, the implementation first looks for a file of the given name,\
    \ if supported. If not (supported), or if the search fails, the implementation\
    \ behaves as though the other (#include <file>) form was used.\nAlso, a third\
    \ form exists and is used when the #include directive doesn't match either of\
    \ the forms above. In this form, some basic preprocessing (such as macro expansion)\
    \ is done on the \"operands\" of the #include directive, and the result is expected\
    \ to match one of the two other forms.\n"
- - What are the basic rules and idioms for operator overloading?
  - "\nCommon operators to overload\nMost of the work in overloading operators is\
    \ boiler-plate code. That is little wonder, since operators are merely syntactic\
    \ sugar, their actual work could be done by (and often is forwarded to) plain\
    \ functions. But it is important that you get this boiler-plate code right. If\
    \ you fail, either your operatorâs code wonât compile or your usersâ code wonât\
    \ compile or your usersâ code will behave surprisingly.\nAssignment Operator\n\
    There's a lot to be said about assignment. However, most of it has already been\
    \ said in GMan's famous Copy-And-Swap FAQ, so I'll skip most of it here, only\
    \ listing the perfect assignment operator for reference:\nX& X::operator=(X rhs)\n\
    {\n  swap(rhs);\n  return *this;\n}\n\nBitshift Operators (used for Stream I/O)\n\
    The bitshift operators << and >>, although still used in hardware interfacing\
    \ for the bit-manipulation functions they inherit from C, have become more prevalent\
    \ as overloaded stream input and output operators in most applications.  For guidance\
    \ overloading as bit-manipulation operators, see the section below on Binary Arithmetic\
    \ Operators.  For implementing your own custom format and parsing logic when your\
    \ object is used with iostreams, continue.\nThe stream operators, among the most\
    \ commonly overloaded operators, are binary infix operators for which the syntax\
    \ specifies no restriction on whether they should be members or non-members.\n\
    Since they change their left argument (they alter the streamâs state), they should,\
    \ according to the rules of thumb, be implemented as members of their left operandâs\
    \ type. However, their left operands are streams from the standard library, and\
    \ while most of the stream output and input operators defined by the standard\
    \ library are indeed defined as members of the stream classes, when you implement\
    \ output and input operations for your own types, you cannot change the standard\
    \ libraryâs stream types. Thatâs why you need to implement these operators for\
    \ your own types as non-member functions.\nThe canonical forms of the two are\
    \ these:\nstd::ostream& operator<<(std::ostream& os, const T& obj)\n{\n  // write\
    \ obj to stream\n\n  return os;\n}\n\nstd::istream& operator>>(std::istream& is,\
    \ T& obj)\n{\n  // read obj from stream\n\n  if( /* no valid object of T found\
    \ in stream */ )\n    is.setstate(std::ios::failbit);\n\n  return is;\n}\n\nWhen\
    \ implementing operator>>, manually setting the streamâs state is only necessary\
    \ when the reading itself succeeded, but the result is not what would be expected.\n\
    Function call operator\nThe function call operator, used to create function objects,\
    \ also known as functors, must be defined as a member function, so it always has\
    \ the implicit this argument of member functions. Other than this it can be overloaded\
    \ to take any number of additional arguments, including zero.\nHere's an example\
    \ of the syntax:\nclass foo {\npublic:\n    // Overloaded call operator\n    int\
    \ operator()(const std::string& y) {\n        // ...\n    }\n};\n\nUsage:\nfoo\
    \ f;\nint a = f(\"hello\");\n\nThroughout the C++ standard library, function objects\
    \ are always copied. Your own function objects should therefore be cheap to copy.\
    \ If a function object absolutely needs to use data which is expensive to copy,\
    \ it is better to store that data elsewhere and have the function object refer\
    \ to it.\nComparison operators\nThe binary infix comparison operators should,\
    \ according to the rules of thumb, be implemented as non-member functions1. The\
    \ unary prefix negation ! should (according to the same rules) be implemented\
    \ as a member function. (but it is usually not a good idea to overload it.)\n\
    The standard libraryâs algorithms (e.g. std::sort()) and types (e.g. std::map)\
    \ will always only expect operator< to be present. However, the users of your\
    \ type will expect all the other operators to be present, too, so if you define\
    \ operator<, be sure to follow the third fundamental rule of operator overloading\
    \ and also define all the other boolean comparison operators. The canonical way\
    \ to implement them is this:\ninline bool operator==(const X& lhs, const X& rhs){\
    \ /* do actual comparison */ }\ninline bool operator!=(const X& lhs, const X&\
    \ rhs){return !operator==(lhs,rhs);}\ninline bool operator< (const X& lhs, const\
    \ X& rhs){ /* do actual comparison */ }\ninline bool operator> (const X& lhs,\
    \ const X& rhs){return  operator< (rhs,lhs);}\ninline bool operator<=(const X&\
    \ lhs, const X& rhs){return !operator> (lhs,rhs);}\ninline bool operator>=(const\
    \ X& lhs, const X& rhs){return !operator< (lhs,rhs);}\n\nThe important thing to\
    \ note here is that only two of these operators actually do anything, the others\
    \ are just forwarding their arguments to either of these two to do the actual\
    \ work.\nThe syntax for overloading the remaining binary boolean operators (||,\
    \ &&) follows the rules of the comparison operators. However, it is very unlikely\
    \ that you would find a reasonable use case for these2.\n1 As with all rules of\
    \ thumb, sometimes there might be reasons to break this one, too. If so, do not\
    \ forget that the left-hand operand of the binary comparison operators, which\
    \ for member functions will be *this, needs to be const, too. So a comparison\
    \ operator implemented as a member function would have to have this signature:\n\
    bool operator<(const X& rhs) const { /* do actual comparison with *this */ }\n\
    \n(Note the const at the end.)\n2 It should be noted that the built-in version\
    \ of || and && use shortcut semantics. While the user defined ones (because they\
    \ are syntactic sugar for method calls) do not use shortcut semantics. User will\
    \ expect these operators to have shortcut semantics, and their code may depend\
    \ on it, Therefore it is highly advised NEVER to define them.\nArithmetic Operators\n\
    Unary arithmetic operators\nThe unary increment and decrement operators come in\
    \ both prefix and postfix flavor. To tell one from the other, the postfix variants\
    \ take an additional dummy int argument. If you overload increment or decrement,\
    \ be sure to always implement both prefix and postfix versions.\nHere is the canonical\
    \ implementation of increment, decrement follows the same rules:\nclass X {\n\
    \  X& operator++()\n  {\n    // do actual increment\n    return *this;\n  }\n\
    \  X operator++(int)\n  {\n    X tmp(*this);\n    operator++();\n    return tmp;\n\
    \  }\n};\n\nNote that the postfix variant is implemented in terms of prefix. Also\
    \ note that postfix does an extra copy.2\nOverloading unary minus and plus is\
    \ not very common and probably best avoided. If needed, they should probably be\
    \ overloaded as member functions. \n2 Also note that the postfix variant does\
    \ more work and is therefore less efficient to use than the prefix variant. This\
    \ is a good reason to generally prefer prefix increment over postfix increment.\
    \ While compilers can usually optimize away the additional work of postfix increment\
    \ for built-in types, they might not be able to do the same for user-defined types\
    \ (which could be something as innocently looking as a list iterator). Once you\
    \ got used to do i++, it becomes very hard to remember to do ++i instead when\
    \ i is not of a built-in type (plus you'd have to change code when changing a\
    \ type), so it is better to make a habit of always using prefix increment, unless\
    \ postfix is explicitly needed.\nBinary arithmetic operators\nFor the binary arithmetic\
    \ operators, do not forget to obey the third basic rule operator overloading:\
    \ If you provide +, also provide +=, if you provide -, do not omit -=, etc. Andrew\
    \ Koenig is said to have been the first to observe that the compound assignment\
    \ operators can be used as a base for their non-compound counterparts. That is,\
    \ operator + is implemented in terms of +=, - is implemented in terms of -= etc.\n\
    According to our rules of thumb, + and its companions should be non-members, while\
    \ their compound assignment counterparts (+= etc.), changing their left argument,\
    \ should be a member. Here is the exemplary code for += and +, the other binary\
    \ arithmetic operators should be implemented in the same way:\nclass X {\n  X&\
    \ operator+=(const X& rhs)\n  {\n    // actual addition of rhs to *this\n    return\
    \ *this;\n  }\n};\ninline X operator+(X lhs, const X& rhs)\n{\n  lhs += rhs;\n\
    \  return lhs;\n}\n\noperator+= returns its result per reference, while operator+\
    \ returns a copy of its result. Of course, returning a reference is usually more\
    \ efficient than returning a copy, but in the case of operator+, there is no way\
    \ around the copying. When you write a + b, you expect the result to be a new\
    \ value, which is why operator+ has to return a new value.3\nAlso note that operator+\
    \ takes its left operand by copy rather than by const reference. The reason for\
    \ this is the same as the reason giving for operator= taking its argument per\
    \ copy.\nThe bit manipulation operators ~ & | ^ << >> should be implemented in\
    \ the same way as the arithmetic operators. However, (except for overloading <<\
    \ and >> for output and input) there are very few reasonable use cases for overloading\
    \ these.\n3 Again, the lesson to be taken from this is that a += b is, in general,\
    \ more efficient than a + b and should be preferred if possible.\nArray Subscripting\n\
    The array subscript operator is a binary operator which must be implemented as\
    \ a class member. It is used for container-like types that allow access to their\
    \ data elements by a key.\nThe canonical form of providing these is this:\nclass\
    \ X {\n        value_type& operator[](index_type idx);\n  const value_type& operator[](index_type\
    \ idx) const;\n  // ...\n};\n\nUnless you do not want users of your class to be\
    \ able to change data elements returned by operator[] (in which case you can omit\
    \ the non-const variant), you should always provide both variants of the operator.\n\
    If value_type is known to refer to a built-in type, the const variant of the operator\
    \ should return a copy instead of a const reference.\nOperators for Pointer-like\
    \ Types\nFor defining your own iterators or smart pointers, you have to overload\
    \ the unary prefix dereference operator * and the binary infix pointer member\
    \ access operator ->:\nclass my_ptr {\n        value_type& operator*();\n  const\
    \ value_type& operator*() const;\n        value_type* operator->();\n  const value_type*\
    \ operator->() const;\n};\n\nNote that these, too, will almost always need both\
    \ a const and a non-const version.\nFor the -> operator, if value_type is of class\
    \ (or struct or union) type, another operator->() is called recursively, until\
    \ an operator->() returns a value of non-class type.\nThe unary address-of operator\
    \ should never be overloaded.\nFor operator->*() see this question. It's rarely\
    \ used and thus rarely ever overloaded. In fact, even iterators do not overload\
    \ it.\n\nContinue to Conversion Operators\n"
- - What are the basic rules and idioms for operator overloading?
  - "\nThe Three Basic Rules of Operator Overloading in C++\nWhen it comes to operator\
    \ overloading in C++, there are three basic rules you should follow. As with all\
    \ such rules, there are indeed exceptions. Sometimes people have deviated from\
    \ them and the outcome was not bad code, but such positive deviations are few\
    \ and far between. At the very least, 99 out of 100 such deviations I have seen\
    \ were unjustified. However, it might just as well have been 999 out of 1000.\
    \ So youâd better stick to the following rules. \n\nWhenever the meaning of an\
    \ operator is not obviously clear and undisputed, it should not be overloaded.\
    \ Instead, provide a function with a well-chosen name.\nBasically, the first and\
    \ foremost rule for overloading operators, at its very heart, says: Donât do it.\
    \ That might seem strange, because there is a lot to be known about operator overloading\
    \ and so a lot of articles, book chapters, and other texts deal with all this.\
    \ But despite this seemingly obvious evidence, there are only a surprisingly few\
    \ cases where operator overloading is appropriate. The reason is that actually\
    \ it is hard to understand the semantics behind the application of an operator\
    \ unless the use of the operator in the application domain is well known and undisputed.\
    \ Contrary to popular belief, this is hardly ever the case.  \nAlways stick to\
    \ the operatorâs well-known semantics.\nC++ poses no limitations on the semantics\
    \ of overloaded operators. Your compiler will happily accept code that implements\
    \ the binary + operator to subtract from its right operand. However, the users\
    \ of such an operator would never suspect the expression a + b to subtract a from\
    \ b. Of course, this supposes that the semantics of the operator in the application\
    \ domain is undisputed. \nAlways provide all out of a set of related operations.\n\
    Operators are related to each other and to other operations. If your type supports\
    \ a + b, users will expect to be able to call a += b, too. If it supports prefix\
    \ increment ++a, they will expect a++ to work as well. If they can check whether\
    \ a < b, they will most certainly expect to also to be able to check whether a\
    \ > b. If they can copy-construct your type, they expect assignment to work as\
    \ well. \n\n\nContinue to The Decision between Member and Non-member.\n"
- - What are the basic rules and idioms for operator overloading?
  - "\nThe General Syntax of operator overloading in C++\nYou cannot change the meaning\
    \ of operators for built-in types in C++, operators can only be overloaded for\
    \ user-defined types1. That is, at least one of the operands has to be of a user-defined\
    \ type. As with other overloaded functions, operators can be overloaded for a\
    \ certain set of parameters only once. \nNot all operators can be overloaded in\
    \ C++. Among the operators that cannot be overloaded are: . :: sizeof typeid .*\
    \ and the only ternary operator in C++, ?: \nAmong the operators that can be overloaded\
    \ in C++ are these: \n\narithmetic operators: + - * / % and += -= *= /= %= (all\
    \ binary infix); + - (unary prefix); ++ -- (unary prefix and postfix) \nbit manipulation:\
    \ & | ^ << >> and &= |= ^= <<= >>= (all binary infix); ~ (unary prefix) \nboolean\
    \ algebra: == != < > <= >= || && (all binary infix); ! (unary prefix)\nmemory\
    \ management: new new[] delete delete[]\nimplicit conversion operators\nmiscellany:\
    \ = [] -> ->* ,  (all binary infix); * & (all unary prefix) () (function call,\
    \ n-ary infix) \n\nHowever, the fact that you can overload all of these does not\
    \ mean you should do so. See the basic rules of operator overloading.\nIn C++,\
    \ operators are overloaded in the form of functions with special names. As with\
    \ other functions, overloaded operators can generally be implemented either as\
    \ a member function of their left operand's type or as non-member functions. Whether\
    \ you are free to choose or bound to use either one depends on several criteria.2\
    \ A unary operator @3, applied to an object x, is invoked either as operator@(x)\
    \ or as x.operator@(). A binary infix operator @, applied to the objects x and\
    \ y, is called either as operator@(x,y) or as x.operator@(y).4 \nOperators that\
    \ are implemented as non-member functions are sometimes friend of their operandâs\
    \ type. \n1 The term âuser-definedâ might be slightly misleading. C++ makes the\
    \ distinction between built-in types and user-defined types. To the former belong\
    \ for example int, char, and double; to the latter belong all struct, class, union,\
    \ and enum types, including those from the standard library, even though they\
    \ are not, as such, defined by users.\n2 This is covered in a later part of this\
    \ FAQ.\n3 The @ is not a valid operator in C++ which is why I use it as a placeholder.\n\
    4 The only ternary operator in C++ cannot be overloaded and the only n-ary operator\
    \ must always be implemented as a member function.\n\nContinue to The Three Basic\
    \ Rules of Operator Overloading in C++. \n"
- - What is The Rule of Three?
  - "\nIntroduction\nC++ treats variables of user-defined types with value semantics.\n\
    This means that objects are implicitly copied in various contexts,\nand we should\
    \ understand what \"copying an object\" actually means.\nLet us consider a simple\
    \ example:\nclass person\n{\n    std::string name;\n    int age;\n\npublic:\n\n\
    \    person(const std::string& name, int age) : name(name), age(age)\n    {\n\
    \    }\n};\n\nint main()\n{\n    person a(\"Bjarne Stroustrup\", 60);\n    person\
    \ b(a);   // What happens here?\n    b = a;         // And here?\n}\n\n(If you\
    \ are puzzled by the name(name), age(age) part,\nthis is called a member initializer\
    \ list.)\nSpecial member functions\nWhat does it mean to copy a person object?\n\
    The main function shows two distinct copying scenarios.\nThe initialization person\
    \ b(a); is performed by the copy constructor.\nIts job is to construct a fresh\
    \ object based on the state of an existing object.\nThe assignment b = a is performed\
    \ by the copy assignment operator.\nIts job is generally a little more complicated,\n\
    because the target object is already in some valid state that needs to be dealt\
    \ with.\nSince we declared neither the copy constructor nor the assignment operator\
    \ (nor the destructor) ourselves,\nthese are implicitly defined for us. Quote\
    \ from the standard:\n\nThe [...] copy constructor and copy assignment operator,\
    \ [...] and destructor are special member functions.\n  [ Note: The implementation\
    \ will implicitly declare these member functions\n  for some class types when\
    \ the program does not explicitly declare them.\n  The implementation will implicitly\
    \ define them if they are used. [...] end note ]\n  [n3126.pdf section 12 Â§1]\n\
    \nBy default, copying an object means copying its members:\n\nThe implicitly-defined\
    \ copy constructor for a non-union class X performs a memberwise copy of its subobjects.\n\
    \  [n3126.pdf section 12.8 Â§16]\nThe implicitly-defined copy assignment operator\
    \ for a non-union class X performs memberwise copy assignment\n  of its subobjects.\n\
    \  [n3126.pdf section 12.8 Â§30]\n\nImplicit definitions\nThe implicitly-defined\
    \ special member functions for person look like this:\n// 1. copy constructor\n\
    person(const person& that) : name(that.name), age(that.age)\n{\n}\n\n// 2. copy\
    \ assignment operator\nperson& operator=(const person& that)\n{\n    name = that.name;\n\
    \    age = that.age;\n    return *this;\n}\n\n// 3. destructor\n~person()\n{\n\
    }\n\nMemberwise copying is exactly what we want in this case:\nname and age are\
    \ copied, so we get a self-contained, independent person object.\nThe implicitly-defined\
    \ destructor is always empty.\nThis is also fine in this case since we did not\
    \ acquire any resources in the constructor.\nThe members' destructors are implicitly\
    \ called after the person destructor is finished:\n\nAfter executing the body\
    \ of the destructor and destroying any automatic objects allocated within the\
    \ body,\n  a destructor for class X calls the destructors for X's direct [...]\
    \ members\n  [n3126.pdf 12.4 Â§6]\n\nManaging resources\nSo when should we declare\
    \ those special member functions explicitly?\nWhen our class manages a resource,\
    \ that is,\nwhen an object of the class is responsible for that resource.\nThat\
    \ usually means the resource is acquired in the constructor\n(or passed into the\
    \ constructor) and released in the destructor.\nLet us go back in time to pre-standard\
    \ C++.\nThere was no such thing as std::string, and programmers were in love with\
    \ pointers.\nThe person class might have looked like this:\nclass person\n{\n\
    \    char* name;\n    int age;\n\npublic:\n\n    // the constructor acquires a\
    \ resource:\n    // in this case, dynamic memory obtained via new[]\n    person(const\
    \ char* the_name, int the_age)\n    {\n        name = new char[strlen(the_name)\
    \ + 1];\n        strcpy(name, the_name);\n        age = the_age;\n    }\n\n  \
    \  // the destructor must release this resource via delete[]\n    ~person()\n\
    \    {\n        delete[] name;\n    }\n};\n\nEven today, people still write classes\
    \ in this style and get into trouble:\n\"I pushed a person into a vector and now\
    \ I get crazy memory errors!\"\nRemember that by default, copying an object means\
    \ copying its members,\nbut copying the name member merely copies a pointer, not\
    \ the character array it points to!\nThis has several unpleasant effects:\n\n\
    Changes via a can be observed via b.\nOnce b is destroyed, a.name is a dangling\
    \ pointer.\nIf a is destroyed, deleting the dangling pointer yields undefined\
    \ behavior.\nSince the assignment does not take into account what name pointed\
    \ to before the assignment,\nsooner or later you will get memory leaks all over\
    \ the place.\n\nExplicit definitions\nSince memberwise copying does not have the\
    \ desired effect, we must define the copy constructor and the copy assignment\
    \ operator explicitly to make deep copies of the character array:\n// 1. copy\
    \ constructor\nperson(const person& that)\n{\n    name = new char[strlen(that.name)\
    \ + 1];\n    strcpy(name, that.name);\n    age = that.age;\n}\n\n// 2. copy assignment\
    \ operator\nperson& operator=(const person& that)\n{\n    if (this != &that)\n\
    \    {\n        delete[] name;\n        // This is a dangerous point in the flow\
    \ of execution!\n        // We have temporarily invalidated the class invariants,\n\
    \        // and the next statement might throw an exception,\n        // leaving\
    \ the object in an invalid state :(\n        name = new char[strlen(that.name)\
    \ + 1];\n        strcpy(name, that.name);\n        age = that.age;\n    }\n  \
    \  return *this;\n}\n\nNote the difference between initialization and assignment:\n\
    we must tear down the old state before assigning to name to prevent memory leaks.\n\
    Also, we have to protect against self-assignment of the form x = x.\nWithout that\
    \ check, delete[] name would delete the array containing the source string,\n\
    because when you write x = x, both this->name and that.name contain the same pointer.\n\
    Exception safety\nUnfortunately, this solution will fail if new char[...] throws\
    \ an exception due to memory exhaustion.\nOne possible solution is to introduce\
    \ a local variable and reorder the statements:\n// 2. copy assignment operator\n\
    person& operator=(const person& that)\n{\n    char* local_name = new char[strlen(that.name)\
    \ + 1];\n    // If the above statement throws,\n    // the object is still in\
    \ the same state as before.\n    // None of the following statements will throw\
    \ an exception :)\n    strcpy(local_name, that.name);\n    delete[] name;\n  \
    \  name = local_name;\n    age = that.age;\n    return *this;\n}\n\nThis also\
    \ takes care of self-assignment without an explicit check.\nAn even more robust\
    \ solution to this problem is the copy-and-swap idiom,\nbut I will not go into\
    \ the details of exception safety here.\nI only mentioned exceptions to make the\
    \ following point: Writing classes that manage resources is hard.\nNoncopyable\
    \ resources\nSome resources cannot or should not be copied, such as file handles\
    \ or mutexes.\nIn that case, simply declare the copy constructor and copy assignment\
    \ operator as private without giving a definition:\nprivate:\n\n    person(const\
    \ person& that);\n    person& operator=(const person& that);\n\nAlternatively,\
    \ you can inherit from boost::noncopyable or declare them as deleted (in C++11\
    \ and above):\nperson(const person& that) = delete;\nperson& operator=(const person&\
    \ that) = delete;\n\nThe rule of three\nSometimes you need to implement a class\
    \ that manages a resource.\n(Never manage multiple resources in a single class,\n\
    this will only lead to pain.)\nIn that case, remember the rule of three:\n\nIf\
    \ you need to explicitly declare either the destructor,\n  copy constructor or\
    \ copy assignment operator yourself,\n  you probably need to explicitly declare\
    \ all three of them.\n\n(Unfortunately, this \"rule\" is not enforced by the C++\
    \ standard or any compiler I am aware of.)\nThe rule of five\nFrom C++11 on, an\
    \ object has 2 extra special member functions: the move constructor and move assignment.\
    \ The rule of five states to implement these functions as well.\nAn example with\
    \ the signatures:\nclass person\n{\n    std::string name;\n    int age;\n\npublic:\n\
    \    person(const std::string& name, int age);        // Ctor\n    person(const\
    \ person &) = default;                // Copy Ctor\n    person(person &&) noexcept\
    \ = default;            // Move Ctor\n    person& operator=(const person &) =\
    \ default;     // Copy Assignment\n    person& operator=(person &&) noexcept =\
    \ default; // Move Assignment\n    ~person() noexcept = default;             \
    \       // Dtor\n};\n\nThe rule of zero\nThe rule of 3/5 is also referred to as\
    \ the rule of 0/3/5. The zero part of the rule states that you are allowed to\
    \ not write any of the special member functions when creating your class.\nAdvice\n\
    Most of the time, you do not need to manage a resource yourself,\nbecause an existing\
    \ class such as std::string already does it for you.\nJust compare the simple\
    \ code using a std::string member\nto the convoluted and error-prone alternative\
    \ using a char* and you should be convinced.\nAs long as you stay away from raw\
    \ pointer members, the rule of three is unlikely to concern your own code.\n"
- - What is The Rule of Three?
  - "\nThe Rule of Three is a rule of thumb for C++, basically saying \n\nIf your\
    \ class needs any of\n\na copy constructor,\nan assignment operator, \nor a destructor,\
    \ \n\ndefined explictly, then it is likely to need all three of them.\n\nThe reasons\
    \ for this is that all three of them are usually used to manage a resource, and\
    \ if your class manages a resource, it usually needs to manage copying as well\
    \ as freeing. \nIf there is no good semantic for copying the resource your class\
    \ manages, then consider to forbid copying by declaring (not defining) the copy\
    \ constructor and assignment operator as private. \n(Note that the forthcoming\
    \ new version of the C++ standard (which is C++11) adds move semantics to C++,\
    \ which will likely change the Rule of Three. However, I know too little about\
    \ this to write a C++11 section about the Rule of Three.)\n"
- - What is The Rule of Three?
  - "\nThe law of the big three is as specified above.\nAn easy example, in plain\
    \ English, of the kind of problem it solves:\nNon default destructor\nYou allocated\
    \ memory in your constructor and so you need to write a destructor to delete it.\
    \  Otherwise you will cause a memory leak.\nYou might think that this is job done.\
    \ \nThe problem will be, if a copy is made of your object, then the copy will\
    \ point to the same memory as the original object.\nOnce, one of these deletes\
    \ the memory in its destructor, the other will have a pointer to invalid memory\
    \ (this is called a dangling pointer) when it tries to use it things are going\
    \ to get hairy.\nTherefore, you write a copy constructor so that it allocates\
    \ new objects their own pieces of memory to destroy.\nAssignment operator and\
    \ copy constructor\nYou allocated memory in your constructor to a member pointer\
    \ of your class.  When you copy an object of this class the default assignment\
    \ operator and copy constructor will copy the value of this member pointer to\
    \ the new object.\nThis means that the new object and the old object will be pointing\
    \ at the same piece of memory so when you change it in one object it will be changed\
    \ for the other objerct too.  If one object deletes this memory the other will\
    \ carry on trying to use it - eek.\nTo resolve this you write your own version\
    \ of the copy constructor and assignment operator.  Your versions allocate separate\
    \ memory to the new objects and copy across the values that the first pointer\
    \ is pointing to rather than its address.\n"
- - What is the copy-and-swap idiom?
  - "\nOverview\nWhy do we need the copy-and-swap idiom?\nAny class that manages a\
    \ resource (a wrapper, like a smart pointer) needs to implement The Big Three.\
    \ While the goals and implementation of the copy-constructor and destructor are\
    \ straightforward, the copy-assignment operator is arguably the most nuanced and\
    \ difficult. How should it be done? What pitfalls need to be avoided?\nThe copy-and-swap\
    \ idiom is the solution, and elegantly assists the assignment operator in achieving\
    \ two things: avoiding code duplication, and providing a strong exception guarantee.\n\
    How does it work?\nConceptually, it works by using the copy-constructor's functionality\
    \ to create a local copy of the data, then takes the copied data with a swap function,\
    \ swapping the old data with the new data. The temporary copy then destructs,\
    \ taking the old data with it. We are left with a copy of the new data.\nIn order\
    \ to use the copy-and-swap idiom, we need three things: a working copy-constructor,\
    \ a working destructor (both are the basis of any wrapper, so should be complete\
    \ anyway), and a swap function.\nA swap function is a non-throwing function that\
    \ swaps two objects of a class, member for member. We might be tempted to use\
    \ std::swap instead of providing our own, but this would be impossible; std::swap\
    \ uses the copy-constructor and copy-assignment operator within its implementation,\
    \ and we'd ultimately be trying to define the assignment operator in terms of\
    \ itself!\n(Not only that, but unqualified calls to swap will use our custom swap\
    \ operator, skipping over the unnecessary construction and destruction of our\
    \ class that std::swap would entail.)\n\nAn in-depth explanation\nThe goal\nLet's\
    \ consider a concrete case. We want to manage, in an otherwise useless class,\
    \ a dynamic array. We start with a working constructor, copy-constructor, and\
    \ destructor:\n#include <algorithm> // std::copy\n#include <cstddef> // std::size_t\n\
    \nclass dumb_array\n{\npublic:\n    // (default) constructor\n    dumb_array(std::size_t\
    \ size = 0)\n        : mSize(size),\n          mArray(mSize ? new int[mSize]()\
    \ : nullptr)\n    {\n    }\n\n    // copy-constructor\n    dumb_array(const dumb_array&\
    \ other)\n        : mSize(other.mSize),\n          mArray(mSize ? new int[mSize]\
    \ : nullptr),\n    {\n        // note that this is non-throwing, because of the\
    \ data\n        // types being used; more attention to detail with regards\n \
    \       // to exceptions must be given in a more general case, however\n     \
    \   std::copy(other.mArray, other.mArray + mSize, mArray);\n    }\n\n    // destructor\n\
    \    ~dumb_array()\n    {\n        delete [] mArray;\n    }\n\nprivate:\n    std::size_t\
    \ mSize;\n    int* mArray;\n};\n\nThis class almost manages the array successfully,\
    \ but it needs operator= to work correctly.\nA failed solution\nHere's how a naive\
    \ implementation might look:\n// the hard part\ndumb_array& operator=(const dumb_array&\
    \ other)\n{\n    if (this != &other) // (1)\n    {\n        // get rid of the\
    \ old data...\n        delete [] mArray; // (2)\n        mArray = nullptr; //\
    \ (2) *(see footnote for rationale)\n\n        // ...and put in the new\n    \
    \    mSize = other.mSize; // (3)\n        mArray = mSize ? new int[mSize] : nullptr;\
    \ // (3)\n        std::copy(other.mArray, other.mArray + mSize, mArray); // (3)\n\
    \    }\n\n    return *this;\n}\n\nAnd we say we're finished; this now manages\
    \ an array, without leaks. However, it suffers from three problems, marked sequentially\
    \ in the code as (n).\n\nThe first  is the self-assignment test. This check serves\
    \ two purposes: it's an easy way to prevent us from running needless code on self-assignment,\
    \ and it protects us from subtle bugs (such as deleting the array only to try\
    \ and copy it). But in all other cases it merely serves to slow the program down,\
    \ and act as noise in the code; self-assignment rarely occurs, so most of the\
    \ time this check is a waste. It would be better if the operator could work properly\
    \ without it.\nThe second is that it only provides a basic exception guarantee.\
    \ If new int[mSize] fails, *this will have been modified. (Namely, the size is\
    \ wrong and the data is gone!) For a strong exception guarantee, it would need\
    \ to be something akin to:\ndumb_array& operator=(const dumb_array& other)\n{\n\
    \    if (this != &other) // (1)\n    {\n        // get the new data ready before\
    \ we replace the old\n        std::size_t newSize = other.mSize;\n        int*\
    \ newArray = newSize ? new int[newSize]() : nullptr; // (3)\n        std::copy(other.mArray,\
    \ other.mArray + newSize, newArray); // (3)\n\n        // replace the old data\
    \ (all are non-throwing)\n        delete [] mArray;\n        mSize = newSize;\n\
    \        mArray = newArray;\n    }\n\n    return *this;\n}\n\nThe code has expanded!\
    \ Which leads us to the third problem: code duplication. Our assignment operator\
    \ effectively duplicates all the code we've already written elsewhere, and that's\
    \ a terrible thing.\n\nIn our case, the core of it is only two lines (the allocation\
    \ and the copy), but with more complex resources this code bloat can be quite\
    \ a hassle. We should strive to never repeat ourselves.\n(One might wonder: if\
    \ this much code is needed to manage one resource correctly, what if my class\
    \ manages more than one? While this may seem to be a valid concern, and indeed\
    \ it requires non-trivial try/catch clauses, this is a non-issue. That's because\
    \ a class should manage one resource only!)\nA successful solution\nAs mentioned,\
    \ the copy-and-swap idiom will fix all these issues. But right now, we have all\
    \ the requirements except one: a swap function. While The Rule of Three successfully\
    \ entails the existence of our copy-constructor, assignment operator, and destructor,\
    \ it should really be called \"The Big Three and A Half\": any time your class\
    \ manages a resource it also makes sense to provide a swap function.\nWe need\
    \ to add swap functionality to our class, and we do that as followsâ :\nclass dumb_array\n\
    {\npublic:\n    // ...\n\n    friend void swap(dumb_array& first, dumb_array&\
    \ second) // nothrow\n    {\n        // enable ADL (not necessary in our case,\
    \ but good practice)\n        using std::swap;\n\n        // by swapping the members\
    \ of two objects,\n        // the two objects are effectively swapped\n      \
    \  swap(first.mSize, second.mSize);\n        swap(first.mArray, second.mArray);\n\
    \    }\n\n    // ...\n};\n\n(Here is the explanation why public friend swap.)\
    \ Now not only can we swap our dumb_array's, but swaps in general can be more\
    \ efficient; it merely swaps pointers and sizes, rather than allocating and copying\
    \ entire arrays. Aside from this bonus in functionality and efficiency, we are\
    \ now ready to implement the copy-and-swap idiom.\nWithout further ado, our assignment\
    \ operator is:\ndumb_array& operator=(dumb_array other) // (1)\n{\n    swap(*this,\
    \ other); // (2)\n\n    return *this;\n}\n\nAnd that's it! With one fell swoop,\
    \ all three problems are elegantly tackled at once.\nWhy does it work?\nWe first\
    \ notice an important choice: the parameter argument is taken by-value. While\
    \ one could just as easily do the following (and indeed, many naive implementations\
    \ of the idiom do):\ndumb_array& operator=(const dumb_array& other)\n{\n    dumb_array\
    \ temp(other);\n    swap(*this, temp);\n\n    return *this;\n}\n\nWe lose an important\
    \ optimization opportunity. Not only that, but this choice is critical in C++11,\
    \ which is discussed later. (On a general note, a remarkably useful guideline\
    \ is as follows: if you're going to make a copy of something in a function, let\
    \ the compiler do it in the parameter list.â¡)\nEither way, this method of obtaining\
    \ our resource is the key to eliminating code duplication: we get to use the code\
    \ from the copy-constructor to make the copy, and never need to repeat any bit\
    \ of it. Now that the copy is made, we are ready to swap.\nObserve that upon entering\
    \ the function that all the new data is already allocated, copied, and ready to\
    \ be used. This is what gives us a strong exception guarantee for free: we won't\
    \ even enter the function if construction of the copy fails, and it's therefore\
    \ not possible to alter the state of *this. (What we did manually before for a\
    \ strong exception guarantee, the compiler is doing for us now; how kind.)\nAt\
    \ this point we are home-free, because swap is non-throwing. We swap our current\
    \ data with the copied data, safely altering our state, and the old data gets\
    \ put into the temporary. The old data is then released when the function returns.\
    \ (Where upon the parameter's scope ends and its destructor is called.)\nBecause\
    \ the idiom repeats no code, we cannot introduce bugs within the operator. Note\
    \ that this means we are rid of the need for a self-assignment check, allowing\
    \ a single uniform implementation of operator=. (Additionally, we no longer have\
    \ a performance penalty on non-self-assignments.)\nAnd that is the copy-and-swap\
    \ idiom.\nWhat about C++11?\nThe next version of C++, C++11, makes one very important\
    \ change to how we manage resources: the Rule of Three is now The Rule of Four\
    \ (and a half). Why? Because not only do we need to be able to copy-construct\
    \ our resource, we need to move-construct it as well.\nLuckily for us, this is\
    \ easy:\nclass dumb_array\n{\npublic:\n    // ...\n\n    // move constructor\n\
    \    dumb_array(dumb_array&& other)\n        : dumb_array() // initialize via\
    \ default constructor, C++11 only\n    {\n        swap(*this, other);\n    }\n\
    \n    // ...\n};\n\nWhat's going on here? Recall the goal of move-construction:\
    \ to take the resources from another instance of the class, leaving it in a state\
    \ guaranteed to be assignable and destructible.\nSo what we've done is simple:\
    \ initialize via the default constructor (a C++11 feature), then swap with other;\
    \ we know a default constructed instance of our class can safely be assigned and\
    \ destructed, so we know other will be able to do the same, after swapping.\n\
    (Note that some compilers do not support constructor delegation; in this case,\
    \ we have to manually default construct the class. This is an unfortunate but\
    \ luckily trivial task.)\nWhy does that work?\nThat is the only change we need\
    \ to make to our class, so why does it work? Remember the ever-important decision\
    \ we made to make the parameter a value and not a reference:\ndumb_array& operator=(dumb_array\
    \ other); // (1)\n\nNow, if other is being initialized with an rvalue, it will\
    \ be move-constructed. Perfect. In the same way C++03 let us re-use our copy-constructor\
    \ functionality by taking the argument by-value, C++11 will automatically pick\
    \ the move-constructor when appropriate as well. (And, of course, as mentioned\
    \ in previously linked article, the copying/moving of the value may simply be\
    \ elided altogether.)\nAnd so concludes the copy-and-swap idiom.\n\nFootnotes\n\
    *Why do we set mArray to null? Because if any further code in the operator throws,\
    \ the destructor of dumb_array might be called; and if that happens without setting\
    \ it to null, we attempt to delete memory that's already been deleted! We avoid\
    \ this by setting it to null, as deleting null is a no-operation.\nâ There are\
    \ other claims that we should specialize std::swap for our type, provide an in-class\
    \ swap along-side a free-function swap, etc. But this is all unnecessary: any\
    \ proper use of swap will be through an unqualified call, and our function will\
    \ be found through ADL. One function will do.\nâ¡The reason is simple: once you\
    \ have the resource to yourself, you may swap and/or move it (C++11) anywhere\
    \ it needs to be. And by making the copy in the parameter list, you maximize optimization.\n"
- - What is the copy-and-swap idiom?
  - "\nAssignment, at its heart, is two steps: tearing down the object's old state\
    \ and building its new state as a copy of some other object's state. \nBasically,\
    \ that's what the destructor and the copy constructor do, so the first idea would\
    \ be to delegate the work to them. However, since destruction mustn't fail, while\
    \ construction might, we actually want to do it the other way around: first perform\
    \ the constructive part and if that succeeded, then do the destructive part. The\
    \ copy-and-swap idiom is a way to do just that: It first calls a class' copy constructor\
    \ to create a temporary, then swaps its data with the temporary's, and then lets\
    \ the temporary's destructor destroy the old state.\nSince swap() is supposed\
    \ to never fail, the only part which might fail is the copy-construction. That\
    \ is performed first, and if it fails, nothing will be changed in the targeted\
    \ object. \nIn its refined form, copy-and-swap is implemented by having the copy\
    \ performed by initializing the (non-reference) parameter of the assignment operator:\
    \ \nT& operator=(T tmp)\n{\n    this->swap(tmp);\n    return *this;\n}\n\n"
- - What is the copy-and-swap idiom?
  - "\nThere are some good answers already.  I'll focus mainly on what I think they\
    \ lack - an explanation of the \"cons\" with the copy-and-swap idiom....\n\nWhat\
    \ is the copy-and-swap idiom?\n\nA way of implementing the assignment operator\
    \ in terms of a swap function:\nX& operator=(X rhs)\n{\n    swap(rhs);\n    return\
    \ *this;\n}\n\nThe fundamental idea is that:\n\nthe most error-prone part of assigning\
    \ to an object is ensuring any resources the new state needs are acquired (e.g.\
    \ memory, descriptors)\nthat acquisition can be attempted before modifying the\
    \ current state of the object (i.e. *this) if a copy of the new value is made,\
    \ which is why rhs is accepted by value (i.e. copied) rather than by reference\n\
    swapping the state of the local copy rhs and *this is usually relatively easy\
    \ to do without potential failure/exceptions, given the local copy doesn't need\
    \ any particular state afterwards (just needs state fit for the destructor to\
    \ run, much as for an object being moved from in >= C++11)\n\n\nWhen should it\
    \ be used?  (Which problems does it solve [/create]?)\n\n\nWhen you want the assigned-to\
    \ objected unaffected by an assignment that throws an exception, assuming you\
    \ have or can write a swap with strong exception guarantee, and ideally one that\
    \ can't fail/throw..â \nWhen you want a clean, easy to understand, robust way to\
    \ define the assignment operator in terms of (simpler) copy constructor, swap\
    \ and destructor functions.\n\nSelf-assignment done as a copy-and-swap avoids\
    \ oft-overlooked edge cases.â¡\n\n\nWhen any performance penalty or momentarily\
    \ higher resource usage created by having an extra temporary object during the\
    \ assignment is not important to your application. â\n\n\nâ  swap throwing: it's\
    \ generally possible to reliably swap data members that the objects track by pointer,\
    \ but non-pointer data members that don't have a throw-free swap, or for which\
    \ swapping has to be implemented as X tmp = lhs; lhs = rhs; rhs = tmp; and copy-construction\
    \ or assignment may throw, still have the potential to fail leaving some data\
    \ members swapped and others not.  This potential applies even to C++03 std::string's\
    \ as James comments on another answer:\n\n@wilhelmtell: In C++03, there is no\
    \ mention of exceptions potentially thrown by std::string::swap (which is called\
    \ by std::swap). In C++0x, std::string::swap is noexcept and must not throw exceptions.\
    \ â James McNellis Dec 22 '10 at 15:24 \n\n\nâ¡ assignment operator implementation\
    \ that seems sane when assigning from a distinct object can easily fail for self-assignment.\
    \  While it might seem unimaginable that client code would even attempt self-assignment,\
    \ it can happen relatively easily during algo operations on containers, with x\
    \ = f(x); code where f is (perhaps only for some #ifdef branches) a macro ala\
    \ #define f(x) x or a function returning a reference to x, or even (likely inefficient\
    \ but concise) code like x = c1 ? x * 2 : c2 ? x / 2 : x;).  For example:\nstruct\
    \ X\n{\n    T* p_;\n    size_t size_;\n    X& operator=(const X& rhs)\n    {\n\
    \        delete[] p_;  // OUCH!\n        p_ = new T[size_ = rhs.size_];\n    \
    \    std::copy(p_, rhs.p_, rhs.p_ + rhs.size_);\n    }\n    ...\n};\n\nOn self-assignment,\
    \ the above code delete's x.p_;, points p_ at a newly allocated heap region, then\
    \ attempts to read the uninitialised data therein (Undefined Behaviour), if that\
    \ doesn't do anything too weird, copy attempts a self-assignment to every just-destructed\
    \ 'T'!\n\nâ The copy-and-swap idiom can introduce inefficiencies or limitations\
    \ due to the use of an extra temporary (when the operator's parameter is copy-constructed):\n\
    struct Client\n{\n    IP_Address ip_address_;\n    int socket_;\n    X(const X&\
    \ rhs)\n      : ip_address_(rhs.ip_address_), socket_(connect(rhs.ip_address_))\n\
    \    { }\n};\n\nHere, a hand-written Client::operator= might check if *this is\
    \ already connected to the same server as rhs (perhaps sending a \"reset\" code\
    \ if useful), whereas the copy-and-swap approach would invoke the copy-constructor\
    \ which would likely be written to open a distinct socket connection then close\
    \ the original one.  Not only could that mean a remote network interaction instead\
    \ of a simple in-process variable copy, it could run afoul of client or server\
    \ limits on socket resources or connections.  (Of course this class has a pretty\
    \ horrid interface, but that's another matter ;-P).\n"
- - C++11 introduced a standardized memory model. What does it mean? And how is it
    going to affect C++ programming?
  - "\nFirst, you have to learn to think like a Language Lawyer.\nThe C++ specification\
    \ does not make reference to any particular compiler, operating system, or CPU.\
    \  It makes reference to an abstract machine that is a generalization of actual\
    \ systems.  In the Language Lawyer world, the job of the programmer is to write\
    \ code for the abstract machine; the job of the compiler is to actualize that\
    \ code on a concrete machine.  By coding rigidly to the spec, you can be certain\
    \ that your code will compile and run without modification on any system with\
    \ a compliant C++ compiler, whether today or 50 years from now.\nThe abstract\
    \ machine in the C++98/C++03 specification is fundamentally single-threaded. \
    \ So it is not possible to write multi-threaded C++ code that is \"fully portable\"\
    \ with respect to the spec.  The spec does not even say anything about the atomicity\
    \ of memory loads and stores or the order in which loads and stores might happen,\
    \ never mind things like mutexes.\nOf course, you can write multi-threaded code\
    \ in practice for particular concrete systems â like pthreads or Windows.  But\
    \ there is no standard way to write multi-threaded code for C++98/C++03.\nThe\
    \ abstract machine in C++11 is multi-threaded by design.  It also has a well-defined\
    \ memory model; that is, it says what the compiler may and may not do when it\
    \ comes to accessing memory.\nConsider the following example, where a pair of\
    \ global variables are accessed concurrently by two threads:\n           Global\n\
    \           int x, y;\n\nThread 1            Thread 2\nx = 17;             cout\
    \ << y << \" \";\ny = 37;             cout << x << endl;\n\nWhat might Thread\
    \ 2 output?\nUnder C++98/C++03, this is not even Undefined Behavior; the question\
    \ itself is meaningless because the standard does not contemplate anything called\
    \ a \"thread\".\nUnder C++11, the result is Undefined Behavior, because loads\
    \ and stores need not be atomic in general.  Which may not seem like much of an\
    \ improvement...  And by itself, it's not.\nBut with C++11, you can write this:\n\
    \           Global\n           atomic<int> x, y;\n\nThread 1                 Thread\
    \ 2\nx.store(17);             cout << y.load() << \" \";\ny.store(37);       \
    \      cout << x.load() << endl;\n\nNow things get much more interesting.  First\
    \ of all, the behavior here is defined.  Thread 2 could now print 0 0 (if it runs\
    \ before Thread 1), 37 17 (if it runs after Thread 1), or 0 17 (if it runs after\
    \ Thread 1 assigns to x but before it assigns to y).\nWhat it cannot print is\
    \ 37 0, because the default mode for atomic loads/stores in C++11 is to enforce\
    \ sequential consistency.  This just means all loads and stores must be \"as if\"\
    \ they happened in the order you wrote them within each thread, while operations\
    \ among threads can be interleaved however the system likes.  So the default behavior\
    \ of atomics provides both atomicity and ordering for loads and stores.\nNow,\
    \ on a modern CPU, ensuring sequential consistency can be expensive.  In particular,\
    \ the compiler is likely to emit full-blown memory barriers between every access\
    \ here.  But if your algorithm can tolerate out-of-order loads and stores; i.e.,\
    \ if it requires atomicity but not ordering; i.e., if it can tolerate 37 0 as\
    \ output from this program, then you can write this:\n           Global\n    \
    \       atomic<int> x, y;\n\nThread 1                            Thread 2\nx.store(17,memory_order_relaxed);\
    \   cout << y.load(memory_order_relaxed) << \" \";\ny.store(37,memory_order_relaxed);\
    \   cout << x.load(memory_order_relaxed) << endl;\n\nThe more modern the CPU,\
    \ the more likely this is to be faster than the previous example.\nFinally, if\
    \ you just need to keep particular loads and stores in order, you can write:\n\
    \           Global\n           atomic<int> x, y;\n\nThread 1                 \
    \           Thread 2\nx.store(17,memory_order_release);   cout << y.load(memory_order_acquire)\
    \ << \" \";\ny.store(37,memory_order_release);   cout << x.load(memory_order_acquire)\
    \ << endl;\n\nThis takes us back to the ordered loads and stores â so 37 0 is\
    \ no longer a possible output â but it does so with minimal overhead.  (In this\
    \ trivial example, the result is the same as full-blown sequential consistency;\
    \ in a larger program, it would not be.)\nOf course, if the only outputs you want\
    \ to see are 0 0 or 37 17, you can just wrap a mutex around the original code.\
    \  But if you have read this far, I bet you already know how that works, and this\
    \ answer is already longer than I intended :-).\nSo, bottom line. Mutexes are\
    \ great, and C++11 standardizes them. But sometimes for performance reasons you\
    \ want lower-level primitives (e.g., the classic double-checked locking pattern).\
    \  The new standard provides high-level gadgets like mutexes and condition variables,\
    \ and it also provides low-level gadgets like atomic types and the various flavors\
    \ of memory barrier.  So now you can write sophisticated, high-performance concurrent\
    \ routines entirely within the language specified by the standard, and you can\
    \ be certain your code will compile and run unchanged on both today's systems\
    \ and tomorrow's.\nAlthough to be frank, unless you are an expert and working\
    \ on some serious low-level code, you should probably stick to mutexes and condition\
    \ variables.  That's what I intend to do.\nFor more on this stuff, see this blog\
    \ post.\n"
- - C++11 introduced a standardized memory model. What does it mean? And how is it
    going to affect C++ programming?
  - "\nI will just give the analogy with which I understand memory consistency models\
    \ (or memory models, for short). It is inspired by Leslie Lamport's seminal paper\
    \ \"Time, Clocks, and the Ordering of Events in a Distributed System\".\nThe analogy\
    \ is apt and has fundamental significance, but may be overkill for many people.\
    \ However, I hope it provides a mental image (a pictorial representation) that\
    \ facilitates reasoning about memory consistency models.\nLetâs view the histories\
    \ of all memory locations in a space-time diagram in which the horizontal axis\
    \ represents the address space (i.e., each memory location is represented by a\
    \ point on that axis) and the vertical axis represents time (we will see that,\
    \ in general, there is not a universal notion of time). The history of values\
    \ held by each memory location is, therefore, represented by a vertical column\
    \ at that memory address. Each value change is due to one of the threads writing\
    \ a new value to that location. By a memory image, we will mean the aggregate/combination\
    \ of values of all memory locations observable at a particular time by a particular\
    \ thread.\nQuoting from \"A Primer on Memory Consistency and Cache Coherence\"\
    \n\nThe intuitive (and most restrictive) memory model is sequential consistency\
    \ (SC) in which a multithreaded execution should look like an interleaving of\
    \ the sequential executions of each constituent thread, as if the threads were\
    \ time-multiplexed on a single-core processor.\n\nThat global memory order can\
    \ vary from one run of the program to another and may not be known beforehand.\
    \ The characteristic feature of SC is the set of horizontal slices in the address-space-time\
    \ diagram representing planes of simultaneity (i.e., memory images). On a given\
    \ plane, all of its events (or memory values) are simultaneous. There is a notion\
    \ of Absolute Time, in which all threads agree on which memory values are simultaneous.\
    \ In SC, at every time instant, there is only one memory image shared by all threads.\
    \ That's, at every instant of time, all processors agree on the memory image (i.e.,\
    \ the aggregate content of memory). Not only does this imply that all threads\
    \ view the same sequence of values for all memory locations, but also that all\
    \ processors observe the same combinations of values of all variables. This is\
    \ the same as saying all memory operations (on all memory locations) are observed\
    \ in the same total order by all threads.\nIn relaxed memory models, each thread\
    \ will slice up address-space-time in its own way, the only restriction being\
    \ that slices of each thread shall not cross each other because all threads must\
    \ agree on the history of every individual memory location (of course, slices\
    \ of different threads may, and will, cross each other). There is no universal\
    \ way to slice it up (no privileged foliation of address-space-time). Slices do\
    \ not have to be planar (or linear). They can be curved and this is what can make\
    \ a thread read values written by another thread out of the order they were written\
    \ in. Histories of different memory locations may slide (or get stretched) arbitrarily\
    \ relative to each other when viewed by any particular thread. Each thread will\
    \ have a different sense of which events (or, equivalently, memory values) are\
    \ simultaneous. The set of events (or memory values) that are simultaneous to\
    \ one thread are not simultaneous to another. Thus, in a relaxed memory model,\
    \ all threads still observe the same history (i.e., sequence of values) for each\
    \ memory location. But they may observe different memory images (i.e., combinations\
    \ of values of all memory locations). Even if two different memory locations are\
    \ written by the same thread in sequence, the two newly written values may be\
    \ observed in different order by other threads.\n[Picture from Wikipedia]\n\n\
    Readers familiar with Einsteinâs Special Theory of Relativity will notice what\
    \ I am alluding to. Translating Minkowskiâs words into the memory models realm:\
    \ address space and time are shadows of address-space-time. In this case, each\
    \ observer (i.e., thread) will project shadows of events (i.e., memory stores/loads)\
    \ onto his own world-line (i.e., his time axis) and his own plane of simultaneity\
    \ (his address-space axis). Threads in the C++11 memory model correspond to observers\
    \ that are moving relative to each other in special relativity. Sequential consistency\
    \ corresponds to the Galilean space-time (i.e., all observers agree on one absolute\
    \ order of events and a global sense of simultaneity).\nThe resemblance between\
    \ memory models and special relativity stems from the fact that both define a\
    \ partially-ordered set of events, often called a causal set. Some events (i.e.,\
    \ memory stores) can affect (but not be affected by) other events. A C++11 thread\
    \ (or observer in physics) is no more than a chain (i.e., a totally ordered set)\
    \ of events (e.g., memory loads and stores to possibly different addresses).\n\
    In relativity, some order is restored to the seemingly chaotic picture of partially\
    \ ordered events, since the only temporal ordering that all observers agree on\
    \ is the ordering among âtimelikeâ events (i.e., those events that are in principle\
    \ connectible by any particle going slower than the speed of light in a vacuum).\
    \ Only the timelike related events are invariantly ordered.\nTime in Physics,\
    \ Craig Callender.\nIn C++11 memory model, a similar mechanism (the acquire-release\
    \ consistency model) is used to establish these local causality relations.\nTo\
    \ provide a definition of memory consistency and a motivation for abandoning SC,\
    \ I will quote from \"A Primer on Memory Consistency and Cache Coherence\"\n\n\
    For a shared memory machine, the memory consistency model defines the architecturally\
    \ visible behavior of its memory system. The correctness criterion for a single\
    \ processor core partitions behavior between âone correct resultâ and âmany incorrect\
    \ alternativesâ. This is because the processorâs architecture mandates that the\
    \ execution of a thread transforms a given input state into a single well-defined\
    \ output state, even on an out-of-order core. Shared memory consistency models,\
    \ however, concern the loads and stores of multiple threads and usually allow\
    \ many correct executions while disallowing many (more) incorrect ones. The possibility\
    \ of multiple correct executions is due to the ISA allowing multiple threads to\
    \ execute concurrently, often with many possible legal interleavings of instructions\
    \ from different threads.\nRelaxed or weak memory consistency models are motivated\
    \ by the fact that most memory orderings in strong models are unnecessary. If\
    \ a thread updates ten data items and then a synchronization flag, programmers\
    \ usually do not care if the data items are updated in order with respect to each\
    \ other but only that all data items are updated before the flag is updated (usually\
    \ implemented using FENCE instructions). Relaxed models seek to capture this increased\
    \ ordering flexibility and preserve only the orders that programmers ârequireâ\
    \ to get both higher performance and correctness of SC. For example, in certain\
    \ architectures, FIFO write buffers are used by each core to hold the results\
    \ of committed (retired) stores before writing the results to the caches. This\
    \ optimization enhances performance but violates SC. The write buffer hides the\
    \ latency of servicing a store miss. Because stores are common, being able to\
    \ avoid stalling on most of them is an important benefit. For a single-core processor,\
    \ a write buffer can be made architecturally invisible by ensuring that a load\
    \ to address A returns the value of the most recent store to A even if one or\
    \ more stores to A are in the write buffer. This is typically done by either bypassing\
    \ the value of the most recent store to A to the load from A, where âmost recentâ\
    \ is determined by program order, or by stalling a load of A if a store to A is\
    \ in the write buffer. When multiple cores are used, each will have its own bypassing\
    \ write buffer. Without write buffers, the hardware is SC, but with write buffers,\
    \ it is not, making write buffers architecturally visible in a multicore processor.\n\
    Store-store reordering may happen if a core has a non-FIFO write buffer that lets\
    \ stores depart in a different order than the order in which they entered. This\
    \ might occur if the first store misses in the cache while the second hits or\
    \ if the second store can coalesce with an earlier store (i.e., before the first\
    \ store). Load-load reordering may also happen on dynamically-scheduled cores\
    \ that execute instructions out of program order. That can behave the same as\
    \ reordering stores on another core (Can you come up with an example interleaving\
    \ between two threads?). Reordering an earlier load with a later store (a load-store\
    \ reordering) can cause many incorrect behaviors, such as loading a value after\
    \ releasing the lock that protects it (if the store is the unlock operation).\
    \ Note that store-load reorderings may also arise due to local bypassing in the\
    \ commonly implemented FIFO write buffer, even with a core that executes all instructions\
    \ in program order.\n\nBecause cache coherence and memory consistency are sometimes\
    \ confused, it is instructive to also have this quote:\n\nUnlike consistency,\
    \ cache coherence is neither visible to software nor required. Coherence seeks\
    \ to make the caches of a shared-memory system as functionally invisible as the\
    \ caches in a single-core system. Correct coherence ensures that a programmer\
    \ cannot determine whether and where a system has caches by analyzing the results\
    \ of loads and stores. This is because correct coherence ensures that the caches\
    \ never enable new or different functional behavior (programmers may still be\
    \ able to infer likely cache structure using timing information). The main purpose\
    \ of cache coherence protocols is maintaining the single-writer-multiple-readers\
    \ (SWMR) invariant for every memory location.\n  An important distinction between\
    \ coherence and consistency is that coherence is specified on a per-memory location\
    \ basis, whereas consistency is specified with respect to all memory locations.\n\
    \nContinuing with our mental picture, the SWMR invariant corresponds to the physical\
    \ requirement that there be at most one particle located at any one location but\
    \ there can be an unlimited number of observers of any location.\n"
- - C++11 introduced a standardized memory model. What does it mean? And how is it
    going to affect C++ programming?
  - "\nThis is now a multiple-year old question, but being very popular, it's worth\
    \ mentioning a fantastic resource for learning about the C++11 memory model. I\
    \ see no point in summing up his talk in order to make this yet another full answer,\
    \ but given this is the guy who actually wrote the standard, I think it's well\
    \ worth watching the talk.\nHerb Sutter has a three hour long talk about the C++11\
    \ memory model titled \"atomic<> Weapons\", available on the Channel9 site - part\
    \ 1 and part 2. The talk is pretty technical, and covers the following topics:\n\
    \nOptimizations, Races, and the Memory Model\nOrdering â What: Acquire and Release\n\
    Ordering â How: Mutexes, Atomics, and/or Fences\nOther Restrictions on Compilers\
    \ and Hardware\nCode Gen & Performance: x86/x64, IA64, POWER, ARM\nRelaxed Atomics\n\
    \nThe talk doesn't elaborate on the API, but rather on the reasoning, background,\
    \ under the hood and behind the scenes (did you know relaxed semantics were added\
    \ to the standard only because POWER and ARM do not support synchronized load\
    \ efficiently?).\n"
- - Why is reading lines from stdin much slower in C++ than Python?
  - "\nBy default, cin is synchronized with stdio, which causes it to avoid any input\
    \ buffering.  If you add this to the top of your main, you should see much better\
    \ performance:\nstd::ios_base::sync_with_stdio(false);\n\nNormally, when an input\
    \ stream is buffered, instead of reading one character at a time, the stream will\
    \ be read in larger chunks.  This reduces the number of system calls, which are\
    \ typically relatively expensive.  However, since the FILE* based stdio and iostreams\
    \ often have separate implementations and therefore separate buffers, this could\
    \ lead to a problem if both were used together.  For example:\nint myvalue1;\n\
    cin >> myvalue1;\nint myvalue2;\nscanf(\"%d\",&myvalue2);\n\nIf more input was\
    \ read by cin than it actually needed, then the second integer value wouldn't\
    \ be available for the scanf function, which has its own independent buffer. \
    \ This would lead to unexpected results.\nTo avoid this, by default, streams are\
    \ synchronized with stdio.  One common way to achieve this is to have cin read\
    \ each character one at a time as needed using stdio functions.  Unfortunately,\
    \ this introduces a lot of overhead.  For small amounts of input, this isn't a\
    \ big problem, but when you are reading millions of lines, the performance penalty\
    \ is significant.\nFortunately, the library designers decided that you should\
    \ also be able to disable this feature to get improved performance if you knew\
    \ what you were doing, so they provided the sync_with_stdio method.\n"
- - Why is reading lines from stdin much slower in C++ than Python?
  - "\nJust out of curiosity I've taken a look at what happens under the hood, and\
    \ I've used dtruss/strace on each test.\nC++\n./a.out < in\nSaw 6512403 lines\
    \ in 8 seconds.  Crunch speed: 814050\n\nsyscalls sudo dtruss -c ./a.out < in\n\
    CALL                                        COUNT\n__mac_syscall             \
    \                      1\n<snip>\nopen                                       \
    \     6\npread                                           8\nmprotect         \
    \                              17\nmmap                                      \
    \     22\nstat64                                         30\nread_nocancel   \
    \                            25958\n\nPython\n./a.py < in\nRead 6512402 lines\
    \ in 1 seconds. LPS: 6512402\n\nsyscalls sudo dtruss -c ./a.py < in\nCALL    \
    \                                    COUNT\n__mac_syscall                    \
    \               1\n<snip>\nopen                                            5\n\
    pread                                           8\nmprotect                  \
    \                     17\nmmap                                           21\n\
    stat64                                         29\n\n"
- - Why is reading lines from stdin much slower in C++ than Python?
  - "\nI'm a few years behind here, but:\nIn 'Edit 4/5/6' of the original post, you\
    \ are using the construction:\n$ /usr/bin/time cat big_file | program_to_benchmark\n\
    \nThis is wrong in a couple of different ways:\n\nYou're actually timing the execution\
    \ of `cat`, not your benchmark.  The 'user' and 'sys' CPU usage displayed by `time`\
    \ are those of `cat`, not your benchmarked program.  Even worse, the 'real' time\
    \ is also not necessarily accurate.  Depending on the implementation of `cat`\
    \ and of pipelines in your local OS, it is possible that `cat` writes a final\
    \ giant buffer and exits long before the reader process finishes its work.\nUse\
    \ of `cat` is unnecessary and in fact counterproductive; you're adding moving\
    \ parts.  If you were on a sufficiently old system (i.e.  with a single CPU and\
    \ -- in certain generations of computers -- I/O faster than CPU) -- the mere fact\
    \ that `cat` was running could substantially color the results.  You are also\
    \ subject to whatever input and output buffering and other processing `cat` may\
    \ do.  (This would likely earn you a 'Useless Use Of Cat' award if I were Randal\
    \ Schwartz.\n\nA better construction would be:\n$ /usr/bin/time program_to_benchmark\
    \ < big_file\n\nIn this statement it is the shell which opens big_file, passing\
    \ it to your program (well, actually to `time` which then executes your program\
    \ as a subprocess) as an already-open file descriptor.  100% of the file reading\
    \ is strictly the responsibility of the program you're trying to benchmark.  This\
    \ gets you a real reading of its performance without spurious complications.\n\
    I will mention two possible, but actually wrong, 'fixes' which could also be considered\
    \ (but I 'number' them differently as these are not things which were wrong in\
    \ the original post):\nA. You could 'fix' this by timing only your program:\n\
    $ cat big_file | /usr/bin/time program_to_benchmark\n\nB. or by timing the entire\
    \ pipeline:\n$ /usr/bin/time sh -c 'cat big_file | program_to_benchmark'\n\nThese\
    \ are wrong for the same reasons as #2: they're still using `cat` unnecessarily.\
    \  I mention them for a few reasons:\n\nthey're more 'natural' for people who\
    \ aren't entirely comfortable with the I/O redirection facilities of the POSIX\
    \ shell\nthere may be cases where `cat` is needed (e.g.: the file to be read requires\
    \ some sort of privilege to access, and you do not want to grant that privilege\
    \ to the program to be benchmarked: `sudo cat /dev/sda | /usr/bin/time my_compression_test\
    \ --no-output`)\nin practice, on modern machines, the added `cat` in the pipeline\
    \ is probably of no real consequence\n\nBut I say that last thing with some hesitation.\
    \  If we examine the last result in 'Edit 5' --\n$ /usr/bin/time cat temp_big_file\
    \ | wc -l\n0.01user 1.34system 0:01.83elapsed 74%CPU ...\n\n-- this claims that\
    \ `cat` consumed 74% of the CPU during the test; and indeed 1.34/1.83 is approximately\
    \ 74%.  Perhaps a run of:\n$ /usr/bin/time wc -l < temp_big_file\n\nwould have\
    \ taken only the remaining .49 seconds!  Probably not: `cat` here had to pay for\
    \ the read() system calls (or equivalent) which transferred the file from 'disk'\
    \ (actually buffer cache), as well as the pipe writes to deliver them to `wc`.\
    \  The correct test would still have had to do those read() calls; only the write-to-pipe\
    \ and read-from-pipe calls would have been saved, and those should be pretty cheap.\n\
    Still, I predict you would be able to measure the difference between `cat file\
    \ | wc -l` and `wc -l < file` and find a noticeable (2-digit percentage) difference.\
    \  Each of the slower tests will have paid a similar penalty in absolute time;\
    \ which would however amount to a smaller fraction of its larger total time.\n\
    In fact I did some quick tests with a 1.5 gigabyte file of garbage, on a Linux\
    \ 3.13 (Ubuntu 14.04) system, obtaining these results (these are actually 'best\
    \ of 3' results; after priming the cache, of course): \n$ time wc -l < /tmp/junk\n\
    real 0.280s user 0.156s sys 0.124s (total cpu 0.280s)\n$ time cat /tmp/junk |\
    \ wc -l\nreal 0.407s user 0.157s sys 0.618s (total cpu 0.775s)\n$ time sh -c 'cat\
    \ /tmp/junk | wc -l'\nreal 0.411s user 0.118s sys 0.660s (total cpu 0.778s)\n\n\
    Notice that the two pipeline results claim to have taken more CPU time (user+sys)\
    \ than realtime.  This is because I'm using the shell (Bash)'s built-in 'time'\
    \ command, which is cognizant of the pipeline; and I'm on a multi-core machine\
    \ where separate processes in a pipeline can use separate cores, accumulating\
    \ CPU time faster than realtime.  Using /usr/bin/time I see smaller CPU time than\
    \ realtime -- showing that it can only time the single pipeline element passed\
    \ to it on its command line.  Also, the shell's output gives milliseconds while\
    \ /usr/bin/time only gives hundreths of a second.\nSo at the efficiency level\
    \ of `wc -l`, the `cat` makes a huge difference: 409 / 283 = 1.453 or 45.3% more\
    \ realtime, and 775 / 280 = 2.768, or a whopping 177% more CPU used!  On my random\
    \ it-was-there-at-the-time test box.\nI should add that there is at least one\
    \ other significant difference between these styles of testing, and I can't say\
    \ whether it is a benefit or fault; you have to decide this yourself:\nWhen you\
    \ run `cat big_file | /usr/bin/time my_program`, your program is receiving input\
    \ from a pipe, at precisely the pace sent by `cat`, and in chunks no larger than\
    \ written by `cat`.\nWhen you run `/usr/bin/time my_program < big_file`, your\
    \ program receives an open file descriptor to the actual file.  Your program --\
    \ or in many cases the I/O libraries of the language in which it was written --\
    \ may take different actions when presented with a file descriptor referencing\
    \ a regular file.  It may use mmap(2) to map the input file into its address space,\
    \ instead of using explicit read(2) system calls.  These differences could have\
    \ a far larger effect on your benchmark results than the small cost of running\
    \ the `cat` binary.\nOf course it is an interesting benchmark result if the same\
    \ program performs significantly differently between the two cases.  It shows\
    \ that, indeed, the program or its I/O libraries are doing something interesting,\
    \ like using mmap().  So in practice it might be good to run the benchmarks both\
    \ ways; perhaps discounting the `cat` result by some small factor to \"forgive\"\
    \ the cost of running `cat` itself.\n"
- - Cycles in family tree software
  - "\nIt seems you (and/or your company) have a fundamental misunderstanding of what\
    \ a family tree is supposed to be. \nLet me clarify, I also work for a company\
    \ that has (as one of its products) a family tree in its portfolio, and we have\
    \ been struggling with similar problems.\nThe problem, in our case, and I assume\
    \ your case as well, comes from the GEDCOM format that is extremely opinionated\
    \ about what a family should be. However this format contains some severe misconceptions\
    \ about what a family tree really looks like.\nGEDCOM has many issues, such as\
    \ incompatibility with same sex relations, incest, etc... Which in real life happens\
    \ more often than you'd imagine (especially when going back in time to the 1700-1800).\n\
    We have modeled our family tree to what happens in the real world: Events (for\
    \ example, births, weddings, engagement, unions, deaths, adoptions, etc.). We\
    \ do not put any restrictions on these, except for logically impossible ones (for\
    \ example, one can't be one's own parent, relations need two individuals, etc...)\n\
    The lack of validations gives us a more \"real world\", simpler and more flexible\
    \ solution.\nAs for this specific case, I would suggest removing the assertions\
    \ as they do not hold universally.\nFor displaying issues (that will arise) I\
    \ would suggest drawing the same node as many times as needed, hinting at the\
    \ duplication by lighting up all the copies on selecting one of them.\n"
- - Cycles in family tree software
  - "\nRelax your assertions.\nNot by changing the rules, which are mostly likely\
    \ very helpful to 99.9% of your customers in catching mistakes in entering their\
    \ data.\nInstead, change it from an error \"can't add relationship\" to a warning\
    \ with an \"add anyway\".\n"
- - Cycles in family tree software
  - "\nHere's the problem with family trees: they are not trees. They are directed\
    \ acyclic graphs or DAGs. If I understand the principles of the biology of human\
    \ reproduction correctly, there will not be any cycles.\nAs far as I know, even\
    \ the Christians accept marriages (and thus children) between cousins, which will\
    \ turn the family tree into a family DAG.\nThe moral of the story is: choose the\
    \ right data structures.\n"
- - How can I profile C++ code running on Linux?
  - "\nIf your goal is to use a profiler, use one of the suggested ones.\nHowever,\
    \ if you're in a hurry and you can manually interrupt your program under the debugger\
    \ while it's being subjectively slow, there's a simple way to find performance\
    \ problems.\nJust halt it several times, and each time look at the call stack.\
    \ If there is some code that is wasting some percentage of the time, 20% or 50%\
    \ or whatever, that is the probability that you will catch it in the act on each\
    \ sample. So that is roughly the percentage of samples on which you will see it.\
    \ There is no educated guesswork required.\nIf you do have a guess as to what\
    \ the problem is, this will prove or disprove it.\nYou may have multiple performance\
    \ problems of different sizes. If you clean out any one of them, the remaining\
    \ ones will take a larger percentage, and be easier to spot, on subsequent passes.\n\
    This magnification effect, when compounded over multiple problems, can lead to\
    \ truly massive speedup factors.\nCaveat: Programmers tend to be skeptical of\
    \ this technique unless they've used it themselves. They will say that profilers\
    \ give you this information, but that is only true if they sample the entire call\
    \ stack, and then let you examine a random set of samples. (The summaries are\
    \ where the insight is lost.) Call graphs don't give you the same information,\
    \ because \n\nthey don't summarize at the instruction level, and\nthey give confusing\
    \ summaries in the presence of recursion.\n\nThey will also say it only works\
    \ on toy programs, when actually it works on any program, and it seems to work\
    \ better on bigger programs, because they tend to have more problems to find.\n\
    They will say it sometimes finds things that aren't problems, but that is only\
    \ true if you see something once. If you see a problem on more than one sample,\
    \ it is real.\nP.S. This can also be done on multi-thread programs if there is\
    \ a way to collect call-stack samples of the thread pool at a point in time, as\
    \ there is in Java.\nP.P.S As a rough generality, the more layers of abstraction\
    \ you have in your software, the more likely you are to find that that is the\
    \ cause of performance problems (and the opportunity to get speedup).\nAdded:\
    \ It might not be obvious, but the stack sampling technique works equally well\
    \ in the presence of recursion. The reason is that the time that would be saved\
    \ by removal of an instruction is approximated by the fraction of samples containing\
    \ it, regardless of the number of times it may occur within a sample.\nAnother\
    \ objection I often hear is: \"It will stop someplace random, and it will miss\
    \ the real problem\".\nThis comes from having a prior concept of what the real\
    \ problem is.\nA key property of performance problems is that they defy expectations.\n\
    Sampling tells you something is a problem, and your first reaction is disbelief.\n\
    That is natural, but you can be sure if it finds a problem it is real, and vice-versa.\n\
    ADDED: Let me make a Bayesian explanation of how it works.  Suppose there is some\
    \ instruction I (call or otherwise) which is on the call stack some fraction f\
    \ of the time (and thus costs that much). For simplicity, suppose we don't know\
    \ what f is, but assume it is either 0.1, 0.2, 0.3, ... 0.9, 1.0, and the prior\
    \ probability of each of these possibilities is 0.1, so all of these costs are\
    \ equally likely a-priori.\nThen suppose we take just 2 stack samples, and we\
    \ see instruction I on both samples, designated observation o=2/2. This gives\
    \ us new estimates of the frequency f of I, according to this:\nPrior        \
    \                            \nP(f=x) x  P(o=2/2|f=x) P(o=2/2&&f=x)  P(o=2/2&&f\
    \ >= x)  P(f >= x)\n\n0.1    1     1             0.1          0.1            0.25974026\n\
    0.1    0.9   0.81          0.081        0.181          0.47012987\n0.1    0.8\
    \   0.64          0.064        0.245          0.636363636\n0.1    0.7   0.49 \
    \         0.049        0.294          0.763636364\n0.1    0.6   0.36         \
    \ 0.036        0.33           0.857142857\n0.1    0.5   0.25          0.025  \
    \      0.355          0.922077922\n0.1    0.4   0.16          0.016        0.371\
    \          0.963636364\n0.1    0.3   0.09          0.009        0.38         \
    \  0.987012987\n0.1    0.2   0.04          0.004        0.384          0.997402597\n\
    0.1    0.1   0.01          0.001        0.385          1\n\n                 \
    \ P(o=2/2) 0.385                \n\nThe last column says that, for example, the\
    \ probability that f >= 0.5 is 92%, up from the prior assumption of 60%.\nSuppose\
    \ the prior assumptions are different. Suppose we assume P(f=0.1) is .991 (nearly\
    \ certain), and all the other possibilities are almost impossible (0.001). In\
    \ other words, our prior certainty is that I is cheap. Then we get:\nPrior   \
    \                                 \nP(f=x) x  P(o=2/2|f=x) P(o=2/2&& f=x)  P(o=2/2&&f\
    \ >= x)  P(f >= x)\n\n0.001  1    1              0.001        0.001          0.072727273\n\
    0.001  0.9  0.81           0.00081      0.00181        0.131636364\n0.001  0.8\
    \  0.64           0.00064      0.00245        0.178181818\n0.001  0.7  0.49  \
    \         0.00049      0.00294        0.213818182\n0.001  0.6  0.36          \
    \ 0.00036      0.0033         0.24\n0.001  0.5  0.25           0.00025      0.00355\
    \        0.258181818\n0.001  0.4  0.16           0.00016      0.00371        0.269818182\n\
    0.001  0.3  0.09           0.00009      0.0038         0.276363636\n0.001  0.2\
    \  0.04           0.00004      0.00384        0.279272727\n0.991  0.1  0.01  \
    \         0.00991      0.01375        1\n\n                  P(o=2/2) 0.01375\
    \                \n\nNow it says P(f >= 0.5) is 26%, up from the prior assumption\
    \ of 0.6%. So Bayes allows us to update our estimate of the probable cost of I.\
    \ If the amount of data is small, it doesn't tell us accurately what the cost\
    \ is, only that it is big enough to be worth fixing.\nYet another way to look\
    \ at it is called the Rule Of Succession.\nIf you flip a coin 2 times, and it\
    \ comes up heads both times, what does that tell you about the probable weighting\
    \ of the coin?\nThe respected way to answer is to say that it's a Beta distribution,\
    \ with average value (number of hits + 1) / (number of tries + 2) = (2+1)/(2+2)\
    \ = 75%.\n(The key is that we see I more than once. If we only see it once, that\
    \ doesn't tell us much except that f > 0.)\nSo, even a very small number of samples\
    \ can tell us a lot about the cost of instructions that it sees. (And it will\
    \ see them with a frequency, on average, proportional to their cost. If n samples\
    \ are taken, and f is the cost, then I will appear on nf+/-sqrt(nf(1-f)) samples.\
    \ Example, n=10, f=0.3, that is 3+/-1.4 samples.)\n\nADDED, to give an intuitive\
    \ feel for the difference between measuring and random stack sampling:\nThere\
    \ are profilers now that sample the stack, even on wall-clock time, but what comes\
    \ out is measurements (or hot path, or hot spot, from which a \"bottleneck\" can\
    \ easily hide). What they don't show you (and they easily could) is the actual\
    \ samples themselves. And if your goal is to find the bottleneck, the number of\
    \ them you need to see is, on average, 2 divided by the fraction of time it takes.\n\
    So if it takes 30% of time, 2/.3 = 6.7 samples, on average, will show it, and\
    \ the chance that 20 samples will show it is 99.2%.\nHere is an off-the-cuff illustration\
    \ of the difference between examining measurements and examining stack samples.\n\
    The bottleneck could be one big blob like this, or numerous small ones, it makes\
    \ no difference.\n\nMeasurement is horizontal; it tells you what fraction of time\
    \ specific routines take.\nSampling is vertical.\nIf there is any way to avoid\
    \ what the whole program is doing at that moment, and if you see it on a second\
    \ sample, you've found the bottleneck.\nThat's what makes the difference - seeing\
    \ the whole reason for the time being spent, not just how much.\n"
- - How can I profile C++ code running on Linux?
  - "\nYou can use Valgrind with the following options\nvalgrind --tool=callgrind\
    \ ./(Your binary)\n\nIt will generate a file called callgrind.out.x. You can then\
    \ use kcachegrind tool to read this file. It will give you a graphical analysis\
    \ of things with results like which lines cost how much. \n"
- - How can I profile C++ code running on Linux?
  - "\nI assume you're using GCC. The standard solution would be to profile with gprof.\n\
    Be sure to add -pg to compilation before profiling:\ncc -o myprog myprog.c utils.c\
    \ -g -pg\n\nI haven't tried it yet but I've heard good things about google-perftools.\
    \ It is definitely worth a try.\nRelated question here.\nA few other buzzwords\
    \ if gprof does not do the job for you: Valgrind, Intel VTune, Sun DTrace.\n"
- - Regular cast vs. static_cast vs. dynamic_cast [duplicate]
  - "\nstatic_cast\nstatic_cast is used for cases where you basically want to reverse\
    \ an implicit conversion, with a few restrictions and additions. static_cast performs\
    \ no runtime checks. This should be used if you know that you refer to an object\
    \ of a specific type, and thus a check would be unnecessary. Example:\nvoid func(void\
    \ *data) {\n  // Conversion from MyClass* -> void* is implicit\n  MyClass *c =\
    \ static_cast<MyClass*>(data);\n  ...\n}\n\nint main() {\n  MyClass c;\n  start_thread(&func,\
    \ &c)  // func(&c) will be called\n      .join();\n}\n\nIn this example, you know\
    \ that you passed a MyClass object, and thus there isn't any need for a runtime\
    \ check to ensure this.\ndynamic_cast\ndynamic_cast is useful when you don't know\
    \ what the dynamic type of the object is. It returns a null pointer if the object\
    \ referred to doesn't contain the type casted to as a base class (when you cast\
    \ to a reference, a bad_cast exception is thrown in that case).\nif (JumpStm *j\
    \ = dynamic_cast<JumpStm*>(&stm)) {\n  ...\n} else if (ExprStm *e = dynamic_cast<ExprStm*>(&stm))\
    \ {\n  ...\n}\n\nYou cannot use dynamic_cast if you downcast (cast to a derived\
    \ class) and the argument type is not polymorphic. For example, the following\
    \ code is not valid, because Base doesn't contain any virtual function:\nstruct\
    \ Base { };\nstruct Derived : Base { };\nint main() {\n  Derived d; Base *b =\
    \ &d;\n  dynamic_cast<Derived*>(b); // Invalid\n}\n\nAn \"up-cast\" (cast to the\
    \ base class) is always valid with both static_cast and dynamic_cast, and also\
    \ without any cast, as an \"up-cast\" is an implicit conversion.\nRegular Cast\n\
    These casts are also called C-style cast. A C-style cast is basically identical\
    \ to trying out a range of sequences of C++ casts, and taking the first C++ cast\
    \ that works, without ever considering dynamic_cast. Needless to say, this is\
    \ much more powerful as it combines all of const_cast, static_cast and reinterpret_cast,\
    \ but it's also unsafe, because it does not use dynamic_cast.\nIn addition, C-style\
    \ casts not only allow you to do this, but they also allow you to safely cast\
    \ to a private base-class, while the \"equivalent\" static_cast sequence would\
    \ give you a compile-time error for that.\nSome people prefer C-style casts because\
    \ of their brevity. I use them for numeric casts only, and use the appropriate\
    \ C++ casts when user defined types are involved, as they provide stricter checking.\n"
- - Regular cast vs. static_cast vs. dynamic_cast [duplicate]
  - "\nStatic cast\nThe static cast performs conversions between compatible types.\
    \ It is similar to the C-style cast, but is more restrictive. For example, the\
    \ C-style cast would allow an integer pointer to point to a char.\nchar c = 10;\
    \       // 1 byte\nint *p = (int*)&c; // 4 bytes\n\nSince this results in a 4-byte\
    \ pointer pointing to 1 byte of allocated memory, writing to this pointer will\
    \ either cause a run-time error or will overwrite some adjacent memory.\n*p =\
    \ 5; // run-time error: stack corruption\n\nIn contrast to the C-style cast, the\
    \ static cast will allow the compiler to check that the pointer and pointee data\
    \ types are compatible, which allows the programmer to catch this incorrect pointer\
    \ assignment during compilation.\nint *q = static_cast<int*>(&c); // compile-time\
    \ error\n\nReinterpret cast\nTo force the pointer conversion, in the same way\
    \ as the C-style cast does in the background, the reinterpret cast would be used\
    \ instead.\nint *r = reinterpret_cast<int*>(&c); // forced conversion\n\nThis\
    \ cast handles conversions between certain unrelated types, such as from one pointer\
    \ type to another incompatible pointer type. It will simply perform a binary copy\
    \ of the data without altering the underlying bit pattern. Note that the result\
    \ of such a low-level operation is system-specific and therefore not portable.\
    \ It should be used with caution if it cannot be avoided altogether.\nDynamic\
    \ cast\nThis one is only used to convert object pointers and object references\
    \ into other pointer or reference types in the inheritance hierarchy. It is the\
    \ only cast that makes sure that the object pointed to can be converted, by performing\
    \ a run-time check that the pointer refers to a complete object of the destination\
    \ type. For this run-time check to be possible the object must be polymorphic.\
    \ That is, the class must define or inherit at least one virtual function. This\
    \ is because the compiler will only generate the needed run-time type information\
    \ for such objects.\nDynamic cast examples\nIn the example below, a MyChild pointer\
    \ is converted into a MyBase pointer using a dynamic cast. This derived-to-base\
    \ conversion succeeds, because the Child object includes a complete Base object.\n\
    class MyBase \n{ \n  public:\n  virtual void test() {}\n};\nclass MyChild : public\
    \ MyBase {};\n\n\n\nint main()\n{\n  MyChild *child = new MyChild();\n  MyBase\
    \  *base = dynamic_cast<MyBase*>(child); // ok\n}\n\nThe next example attempts\
    \ to convert a MyBase pointer to a MyChild pointer. Since the Base object does\
    \ not contain a complete Child object this pointer conversion will fail. To indicate\
    \ this, the dynamic cast returns a null pointer. This gives a convenient way to\
    \ check whether or not a conversion has succeeded during run-time.\nMyBase  *base\
    \ = new MyBase();\nMyChild *child = dynamic_cast<MyChild*>(base);\n\n\nif (child\
    \ == 0) \nstd::cout << \"Null pointer returned\";\n\nIf a reference is converted\
    \ instead of a pointer, the dynamic cast will then fail by throwing a bad_cast\
    \ exception. This needs to be handled using a try-catch statement.\n#include <exception>\n\
    // â¦  \ntry\n{ \n  MyChild &child = dynamic_cast<MyChild&>(*base);\n}\ncatch(std::bad_cast\
    \ &e) \n{ \n  std::cout << e.what(); // bad dynamic_cast\n}\n\nDynamic or static\
    \ cast\nThe advantage of using a dynamic cast is that it allows the programmer\
    \ to check whether or not a conversion has succeeded during run-time. The disadvantage\
    \ is that there is a performance overhead associated with doing this check. For\
    \ this reason using a static cast would have been preferable in the first example,\
    \ because a derived-to-base conversion will never fail.\nMyBase *base = static_cast<MyBase*>(child);\
    \ // ok\n\nHowever, in the second example the conversion may either succeed or\
    \ fail. It will fail if the MyBase object contains a MyBase instance and it will\
    \ succeed if it contains a MyChild instance. In some situations this may not be\
    \ known until run-time. When this is the case dynamic cast is a better choice\
    \ than static cast.\n// Succeeds for a MyChild object\nMyChild *child = dynamic_cast<MyChild*>(base);\n\
    \nIf the base-to-derived conversion had been performed using a static cast instead\
    \ of a dynamic cast the conversion would not have failed. It would have returned\
    \ a pointer that referred to an incomplete object. Dereferencing such a pointer\
    \ can lead to run-time errors.\n// Allowed, but invalid\nMyChild *child = static_cast<MyChild*>(base);\n\
    \n// Incomplete MyChild object dereferenced\n(*child);\n\nConst cast\nThis one\
    \ is primarily used to add or remove the const modifier of a variable.\nconst\
    \ int myConst = 5;\nint *nonConst = const_cast<int*>(&myConst); // removes const\n\
    \nAlthough const cast allows the value of a constant to be changed, doing so is\
    \ still invalid code that may cause a run-time error. This could occur for example\
    \ if the constant was located in a section of read-only memory.\n*nonConst = 10;\
    \ // potential run-time error\n\nConst cast is instead used mainly when there\
    \ is a function that takes a non-constant pointer argument, even though it does\
    \ not modify the pointee.\nvoid print(int *p) \n{\n   std::cout << *p;\n}\n\n\
    The function can then be passed a constant variable by using a const cast.\nprint(&myConst);\
    \ // error: cannot convert \n                 // const int* to int*\n\nprint(nonConst);\
    \ // allowed\n\nSource and More Explanations\n"
- - Regular cast vs. static_cast vs. dynamic_cast [duplicate]
  - "\nYou should look at the article C++ Programming/Type Casting.\nIt contains a\
    \ good description of all of the different cast types. The following taken from\
    \ the above link:\n\nconst_cast\nconst_cast(expression) The const_cast<>() is\
    \ used to add/remove\n  const(ness) (or volatile-ness) of a variable.\nstatic_cast\n\
    static_cast(expression) The static_cast<>() is used to cast between\n  the integer\
    \ types. 'e.g.' char->long, int->short etc.\nStatic cast is also used to cast\
    \ pointers to related types, for\n  example casting void* to the appropriate type.\n\
    dynamic_cast\nDynamic cast is used to convert pointers and references at run-time,\n\
    \  generally for the purpose of casting a pointer or reference up or down\n  an\
    \ inheritance chain (inheritance hierarchy).\ndynamic_cast(expression)\nThe target\
    \ type must be a pointer or reference type, and the\n  expression must evaluate\
    \ to a pointer or reference. Dynamic cast works\n  only when the type of object\
    \ to which the expression refers is\n  compatible with the target type and the\
    \ base class has at least one\n  virtual member function. If not, and the type\
    \ of expression being cast\n  is a pointer, NULL is returned, if a dynamic cast\
    \ on a reference\n  fails, a bad_cast exception is thrown. When it doesn't fail,\
    \ dynamic\n  cast returns a pointer or reference of the target type to the object\n\
    \  to which expression referred.\nreinterpret_cast\nReinterpret cast simply casts\
    \ one type bitwise to another. Any pointer\n  or integral type can be casted to\
    \ any other with reinterpret cast,\n  easily allowing for misuse. For instance,\
    \ with reinterpret cast one\n  might, unsafely, cast an integer pointer to a string\
    \ pointer.\n\n"
- - What is a smart pointer and when should I use one?
  - "\nA smart pointer is a class that wraps a 'raw' (or 'bare') C++ pointer, to manage\
    \ the lifetime of the object being pointed to. There is no single smart pointer\
    \ type, but all of them try to abstract a raw pointer in a practical way.\nSmart\
    \ pointers should be preferred over raw pointers. If you feel you need to use\
    \ pointers (first consider if you really do), you would normally want to use a\
    \ smart pointer as this can alleviate many of the problems with raw pointers,\
    \ mainly forgetting to delete the object and leaking memory.\nWith raw pointers,\
    \ the programmer has to explicitly destroy the object when it is no longer useful.\n\
    // Need to create the object to achieve some goal\nMyObject* ptr = new MyObject();\
    \ \nptr->DoSomething(); // Use the object in some way\ndelete ptr; // Destroy\
    \ the object. Done with it.\n// Wait, what if DoSomething() raises an exception...?\n\
    \nA smart pointer by comparison defines a policy as to when the object is destroyed.\
    \ You still have to create the object, but you no longer have to worry about destroying\
    \ it.\nSomeSmartPtr<MyObject> ptr(new MyObject());\nptr->DoSomething(); // Use\
    \ the object in some way.\n\n// Destruction of the object happens, depending \n\
    // on the policy the smart pointer class uses.\n\n// Destruction would happen\
    \ even if DoSomething() \n// raises an exception\n\nThe simplest policy in use\
    \ involves the scope of the smart pointer wrapper object, such as implemented\
    \ by boost::scoped_ptr or std::unique_ptr. \nvoid f()\n{\n    {\n       boost::scoped_ptr<MyObject>\
    \ ptr(new MyObject());\n       ptr->DoSomethingUseful();\n    } // boost::scopted_ptr\
    \ goes out of scope -- \n      // the MyObject is automatically destroyed.\n\n\
    \    // ptr->Oops(); // Compile error: \"ptr\" not defined\n                 \
    \   // since it is no longer in scope.\n}\n\nNote that scoped_ptr instances cannot\
    \ be copied. This prevents the pointer from being deleted multiple times (incorrectly).\
    \ You can, however, pass references to it around to other functions you call.\n\
    Scoped pointers are useful when you want to tie the lifetime of the object to\
    \ a particular block of code, or if you embedded it as member data inside another\
    \ object, the lifetime of that other object. The object exists until the containing\
    \ block of code is exited, or until the containing object is itself destroyed.\n\
    A more complex smart pointer policy involves reference counting the pointer. This\
    \ does allow the pointer to be copied. When the last \"reference\" to the object\
    \ is destroyed, the object is deleted. This policy is implemented by boost::shared_ptr\
    \ and std::shared_ptr.\nvoid f()\n{\n    typedef std::shared_ptr<MyObject> MyObjectPtr;\
    \ // nice short alias\n    MyObjectPtr p1; // Empty\n\n    {\n        MyObjectPtr\
    \ p2(new MyObject());\n        // There is now one \"reference\" to the created\
    \ object\n        p1 = p2; // Copy the pointer.\n        // There are now two\
    \ references to the object.\n    } // p2 is destroyed, leaving one reference to\
    \ the object.\n} // p1 is destroyed, leaving a reference count of zero. \n  //\
    \ The object is deleted.\n\nReference counted pointers are very useful when the\
    \ lifetime of your object is much more complicated, and is not tied directly to\
    \ a particular section of code or to another object.\nThere is one drawback to\
    \ reference counted pointers â the possibility of creating a dangling reference:\n\
    // Create the smart pointer on the heap\nMyObjectPtr* pp = new MyObjectPtr(new\
    \ MyObject())\n// Hmm, we forgot to destroy the smart pointer,\n// because of\
    \ that, the object is never destroyed!\n\nAnother possibility is creating circular\
    \ references:\nstruct Owner {\n   boost::shared_ptr<Owner> other;\n};\n\nboost::shared_ptr<Owner>\
    \ p1 (new Owner());\nboost::shared_ptr<Owner> p2 (new Owner());\np1->other = p2;\
    \ // p1 references p2\np2->other = p1; // p2 references p1\n\n// Oops, the reference\
    \ count of of p1 and p2 never goes to zero!\n// The objects are never destroyed!\n\
    \nTo work around this problem, both Boost and C++11 have defined a weak_ptr to\
    \ define a weak (uncounted) reference to a shared_ptr.\n\nUPDATE\nThis answer\
    \ is rather old, and so describes what was 'good' at the time, which was smart\
    \ pointers provided by the Boost library. Since C++11, the standard library has\
    \ provided sufficient smart pointers types, and so you should favour the use of\
    \ std::unique_ptr, std::shared_ptr and std::weak_ptr. \nThere is also std::auto_ptr.\
    \ It is very much like a scoped pointer, except that it also has the \"special\"\
    \ dangerous ability to be copied â which also unexpectedly transfers ownership!\
    \ It is deprecated in the newest standards, so you shouldn't use it. Use the std::unique_ptr\
    \ instead.\nstd::auto_ptr<MyObject> p1 (new MyObject());\nstd::auto_ptr<MyObject>\
    \ p2 = p1; // Copy and transfer ownership. \n                                \
    \ // p1 gets set to empty!\np2->DoSomething(); // Works.\np1->DoSomething(); //\
    \ Oh oh. Hopefully raises some NULL pointer exception.\n\n"
- - What is a smart pointer and when should I use one?
  - "\nHere's a simple answer for these days of modern C++:\n\nWhat is a smart pointer?\
    \ \nIt's a type whose values can be used like pointers, but which provides the\
    \ additional feature of automatic memory management: When a smart pointer is no\
    \ longer in use, the memory it points to is deallocated (see also the more detailed\
    \ definition on Wikipedia).\nWhen should I use one? \nIn code which involves tracking\
    \ the ownership of a piece of memory, allocating or de-allocating; the smart pointer\
    \ often saves you the need to do these things explicitly.\nBut which smart pointer\
    \ should I use in which of those cases?\n\nUse std::unique_ptr when you don't\
    \ intend to hold multiple references to the same object. For example, use it for\
    \ a pointer to memory which gets allocated on entering some scope and de-allocated\
    \ on exiting the scope.\nUse std::shared_ptr when you do want to refer to your\
    \ object from multiple places - and do not want your object to be de-allocated\
    \ until all these references are themselves gone.\nUse std::weak_ptr when you\
    \ do want to refer to your object from multiple places - for those references\
    \ for which it's ok to ignore and deallocate (so they'll just note the object\
    \ is gone when you try to dereference).\nDon't use the boost:: smart pointers\
    \ or std::auto_ptr except in special cases which you can read up on if you must.\n\
    \nHey, I didn't ask which one to use! \nAh, but you really wanted to, admit it.\n\
    So when should I use regular pointers then? \nMostly in code that is oblivious\
    \ to memory ownership. This would typically be in functions which get a pointer\
    \ from someplace else and do not allocatemor de-allocate, and do not store a copy\
    \ of the pointer which outlasts their execution.\n\n"
- - What is a smart pointer and when should I use one?
  - "\nSmart pointer is a pointer-like type with some additional functionality, e.g.\
    \ automatic memory deallocation, reference counting etc.\nSmall intro is available\
    \ on page Smart Pointers - What, Why, Which?.\nOne of the simple smart-pointer\
    \ type is std::auto_ptr (chapter 20.4.5 of C++ standard), which allows to deallocate\
    \ memory automatically when it out of scope and which is more robust than simple\
    \ pointer usage when exceptions are thrown, although less flexible.\nAnother convenient\
    \ type is boost::shared_ptr which implements reference counting and automatically\
    \ deallocates memory when no references to object remains. This helps avoiding\
    \ memory leaks and is easy to use to implement RAII.\nSubject is covered in depth\
    \ in book \"C++ Templates: The Complete Guide\" by David Vandevoorde, Nicolai\
    \ M. Josuttis, chapter Chapter 20. Smart Pointers.\nSome topics covered:\n\nProtecting\
    \ Against Exceptions\nHolders, (note, std::auto_ptr is implementation of such\
    \ type of smart pointer)\nResource Acquisition Is Initialization (This is frequently\
    \ used for exception-safe resource management in C++)\nHolder Limitations\nReference\
    \ Counting\nConcurrent Counter Access\nDestruction and Deallocation\n\n"
- - Why can templates only be implemented in the header file?
  - "\nIt is not necessary to put the implementation in the header file, see the alternative\
    \ solution at the end of this answer.\nAnyway, the reason your code is failing\
    \ is that, when instantiating a template, the compiler creates a new class with\
    \ the given template argument. For example:\ntemplate<typename T>\nstruct Foo\n\
    {\n    T bar;\n    void doSomething(T param) {/* do stuff using T */}\n};\n\n\
    // somewhere in a .cpp\nFoo<int> f; \n\nWhen reading this line, the compiler will\
    \ create a new class (let's call it FooInt), which is equivalent to the following:\n\
    struct FooInt\n{\n    int bar;\n    void doSomething(int param) {/* do stuff using\
    \ int */}\n}\n\nConsequently, the compiler needs to have access to the implementation\
    \ of the methods, to instantiate them with the template argument (in this case\
    \ int). If these implementations were not in the header, they wouldn't be accessible,\
    \ and therefore the compiler wouldn't be able to instantiate the template.\nA\
    \ common solution to this is to write the template declaration in a header file,\
    \ then implement the class in an implementation file (for example .tpp), and include\
    \ this implementation file at the end of the header.\n// Foo.h\ntemplate <typename\
    \ T>\nstruct Foo\n{\n    void doSomething(T param);\n};\n\n#include \"Foo.tpp\"\
    \n\n// Foo.tpp\ntemplate <typename T>\nvoid Foo<T>::doSomething(T param)\n{\n\
    \    //implementation\n}\n\nThis way, implementation is still separated from declaration,\
    \ but is accessible to the compiler.\nAnother solution is to keep the implementation\
    \ separated, and explicitly instantiate all the template instances you'll need:\n\
    // Foo.h\n\n// no implementation\ntemplate <typename T> struct Foo { ... };\n\n\
    //----------------------------------------    \n// Foo.cpp\n\n// implementation\
    \ of Foo's methods\n\n// explicit instantiations\ntemplate class Foo<int>;\ntemplate\
    \ class Foo<float>;\n// You will only be able to use Foo with int or float\n\n\
    If my explanation isn't clear enough, you can have a look at the C++ Super-FAQ\
    \ on this subject.\n"
- - Why can templates only be implemented in the header file?
  - "\nPlenty correct answers here, but I wanted to add this (for completeness):\n\
    If you, at the bottom of the implementation cpp file, do explicit instantiation\
    \ of all the types the template will be used with, the linker will be able to\
    \ find them as usual.\nEdit: Adding example of explicit template instantiation.\
    \ Used after the template has been defined, and all member functions has been\
    \ defined.\ntemplate class vector<int>;\n\nThis will instantiate (and thus make\
    \ available to the linker) the class and all its member functions (only). Similar\
    \ syntax works for template functions, so if you have non-member operator overloads\
    \ you may need to do the same for those.\nThe above example is fairly useless\
    \ since vector is fully defined in headers, except when a common include file\
    \ (precompiled header?) uses extern template class vector<int> so as to keep it\
    \ from instantiating it in all the other (1000?) files that use vector. \n"
- - Why can templates only be implemented in the header file?
  - "\nIt's because of the requirement for separate compilation and because templates\
    \ are instantiation-style polymorphism.\nLets get a little closer to concrete\
    \ for an explanation. Say I've got the following files:\n\nfoo.h\n\ndeclares the\
    \ interface of class MyClass<T>\n\nfoo.cpp\n\ndefines the implementation of class\
    \ MyClass<T>\n\nbar.cpp\n\nuses MyClass<int>\n\n\nSeparate compilation means I\
    \ should be able to compile foo.cpp independently from bar.cpp. The compiler does\
    \ all the hard work of analysis, optimization, and code generation on each compilation\
    \ unit completely independently; we don't need to do whole-program analysis. It's\
    \ only the linker that needs to handle the entire program at once, and the linker's\
    \ job is substantially easier.\nbar.cpp doesn't even need to exist when I compile\
    \ foo.cpp, but I should still be able to link the foo.o I already had together\
    \ with the bar.o I've only just produced, without needing to recompile foo.cpp.\
    \ foo.cpp could even be compiled into a dynamic library, distributed somewhere\
    \ else without foo.cpp, and linked with code they write years after I wrote foo.cpp.\n\
    \"Instantiation-style polymorphism\" means that the template MyClass<T> isn't\
    \ really a generic class that can be compiled to code that can work for any value\
    \ of T. That would add overhead such as boxing, needing to pass function pointers\
    \ to allocators and constructors, etc. The intention of C++ templates is to avoid\
    \ having to write nearly identical class MyClass_int, class MyClass_float, etc,\
    \ but to still be able to end up with compiled code that is mostly as if we had\
    \ written each version separately. So a template is literally a template; a class\
    \ template is not a class, it's a recipe for creating a new class for each T we\
    \ encounter. A template cannot be compiled into code, only the result of instantiating\
    \ the template can be compiled.\nSo when foo.cpp is compiled, the compiler can't\
    \ see bar.cpp to know that MyClass<int> is needed. It can see the template MyClass<T>,\
    \ but it can't emit code for that (it's a template, not a class). And when bar.cpp\
    \ is compiled, the compiler can see that it needs to create a MyClass<int>, but\
    \ it can't see the template MyClass<T> (only its interface in foo.h) so it can't\
    \ create it.\nIf foo.cpp itself uses MyClass<int>, then code for that will be\
    \ generated while compiling foo.cpp, so when bar.o is linked to foo.o they can\
    \ be hooked up and will work. We can use that fact to allow a finite set of template\
    \ instantiations to be implemented in a .cpp file by writing a single template.\
    \ But there's no way for bar.cpp to use the template as a template and instantiate\
    \ it on whatever types it likes; it can only use pre-existing versions of the\
    \ templated class that the author of foo.cpp thought to provide.\nYou might think\
    \ that when compiling a template the compiler should \"generate all versions\"\
    , with the ones that are never used being filtered out during linking. Aside from\
    \ the huge overhead and the extreme difficulties such an approach would face because\
    \ \"type modifier\" features like pointers and arrays allow even just the built-in\
    \ types to give rise to an infinite number of types, what happens when I now extend\
    \ my program by adding:\n\nbaz.cpp\n\ndeclares and implements class BazPrivate,\
    \ and uses MyClass<BazPrivate>\n\n\nThere is no possible way that this could work\
    \ unless we either\n\nHave to recompile foo.cpp every time we change any other\
    \ file in the program, in case it added a new novel instantiation of MyClass<T>\n\
    Require that baz.cpp contains (possibly via header includes) the full template\
    \ of MyClass<T>, so that the compiler can generate MyClass<BazPrivate> during\
    \ compilation of baz.cpp.\n\nNobody likes (1), because whole-program-analysis\
    \ compilation systems take forever to compile , and because it makes it impossible\
    \ to distribute compiled libraries without the source code. So we have (2) instead.\n"
- - What are move semantics?
  - "\nI find it easiest to understand move semantics with example code. Let's start\
    \ with a very simple string class which only holds a pointer to a heap-allocated\
    \ block of memory:\n#include <cstring>\n#include <algorithm>\n\nclass string\n\
    {\n    char* data;\n\npublic:\n\n    string(const char* p)\n    {\n        size_t\
    \ size = strlen(p) + 1;\n        data = new char[size];\n        memcpy(data,\
    \ p, size);\n    }\n\nSince we chose to manage the memory ourselves, we need to\
    \ follow the rule of three. I am going to defer writing the assignment operator\
    \ and only implement the destructor and the copy constructor for now:\n    ~string()\n\
    \    {\n        delete[] data;\n    }\n\n    string(const string& that)\n    {\n\
    \        size_t size = strlen(that.data) + 1;\n        data = new char[size];\n\
    \        memcpy(data, that.data, size);\n    }\n\nThe copy constructor defines\
    \ what it means to copy string objects. The parameter const string& that binds\
    \ to all expressions of type string which allows you to make copies in the following\
    \ examples:\nstring a(x);                                    // Line 1\nstring\
    \ b(x + y);                                // Line 2\nstring c(some_function_returning_a_string());\
    \   // Line 3\n\nNow comes the key insight into move semantics. Note that only\
    \ in the first line where we copy x is this deep copy really necessary, because\
    \ we might want to inspect x later and would be very surprised if x had changed\
    \ somehow. Did you notice how I just said x three times (four times if you include\
    \ this sentence) and meant the exact same object every time? We call expressions\
    \ such as x \"lvalues\".\nThe arguments in lines 2 and 3 are not lvalues, but\
    \ rvalues, because the underlying string objects have no names, so the client\
    \ has no way to inspect them again at a later point in time.\nrvalues denote temporary\
    \ objects which are destroyed at the next semicolon (to be more precise: at the\
    \ end of the full-expression that lexically contains the rvalue). This is important\
    \ because during the initialization of b and c, we could do whatever we wanted\
    \ with the source string, and the client couldn't tell a difference!\nC++0x introduces\
    \ a new mechanism called \"rvalue reference\" which, among other things,\nallows\
    \ us to detect rvalue arguments via function overloading. All we have to do is\
    \ write a constructor with an rvalue reference parameter. Inside that constructor\
    \ we can do anything we want with the source, as long as we leave it in some valid\
    \ state:\n    string(string&& that)   // string&& is an rvalue reference to a\
    \ string\n    {\n        data = that.data;\n        that.data = nullptr;\n   \
    \ }\n\nWhat have we done here? Instead of deeply copying the heap data, we have\
    \ just copied the pointer and then set the original pointer to null. In effect,\
    \ we have \"stolen\" the data that originally belonged to the source string. Again,\
    \ the key insight is that under no circumstance could the client detect that the\
    \ source had been modified. Since we don't really do a copy here, we call this\
    \ constructor a \"move constructor\". Its job is to move resources from one object\
    \ to another instead of copying them.\nCongratulations, you now understand the\
    \ basics of move semantics! Let's continue by implementing the assignment operator.\
    \ If you're unfamiliar with the copy and swap idiom, learn it and come back, because\
    \ it's an awesome C++ idiom related to exception safety.\n    string& operator=(string\
    \ that)\n    {\n        std::swap(data, that.data);\n        return *this;\n \
    \   }\n};\n\nHuh, that's it? \"Where's the rvalue reference?\" you might ask.\
    \ \"We don't need it here!\" is my answer :)\nNote that we pass the parameter\
    \ that by value, so that has to be initialized just like any other string object.\
    \ Exactly how is that going to be initialized? In the olden days of C++98, the\
    \ answer would have been \"by the copy constructor\". In C++0x, the compiler chooses\
    \ between the copy constructor and the move constructor based on whether the argument\
    \ to the assignment operator is an lvalue or an rvalue.\nSo if you say a = b,\
    \ the copy constructor will initialize that (because the expression b is an lvalue),\
    \ and the assignment operator swaps the contents with a freshly created, deep\
    \ copy. That is the very definition of the copy and swap idiom -- make a copy,\
    \ swap the contents with the copy, and then get rid of the copy by leaving the\
    \ scope. Nothing new here.\nBut if you say a = x + y, the move constructor will\
    \ initialize that (because the expression x + y is an rvalue), so there is no\
    \ deep copy involved, only an efficient move.\nthat is still an independent object\
    \ from the argument, but its construction was trivial,\nsince the heap data didn't\
    \ have to be copied, just moved. It wasn't necessary to copy it because x + y\
    \ is an rvalue, and again, it is okay to move from string objects denoted by rvalues.\n\
    To summarize, the copy constructor makes a deep copy, because the source must\
    \ remain untouched.\nThe move constructor, on the other hand, can just copy the\
    \ pointer and then set the pointer in the source to null. It is okay to \"nullify\"\
    \ the source object in this manner, because the client has no way of inspecting\
    \ the object again.\nI hope this example got the main point across. There is a\
    \ lot more to rvalue references and move semantics which I intentionally left\
    \ out to keep it simple. If you want more details please see my supplementary\
    \ answer.\n"
- - What are move semantics?
  - "\nMy first answer was an extremely simplified introduction to move semantics,\
    \ and many details were left out on purpose to keep it simple.\nHowever, there\
    \ is a lot more to move semantics, and I thought it was time for a second answer\
    \ to fill the gaps.\nThe first answer is already quite old, and it did not feel\
    \ right to simply replace it with a completely different text. I think it still\
    \ serves well as a first introduction. But if you want to dig deeper, read on\
    \ :)\nStephan T. Lavavej took the time provide valuable feedback. Thank you very\
    \ much, Stephan!\nIntroduction\nMove semantics allows an object, under certain\
    \ conditions, to take ownership of some other object's external resources. This\
    \ is important in two ways:\n\nTurning expensive copies into cheap moves. See\
    \ my first answer for an example. Note that if an object does not manage at least\
    \ one external resource (either directly, or indirectly through its member objects),\
    \ move semantics will not offer any advantages over copy semantics. In that case,\
    \ copying an object and moving an object means the exact same thing:\nclass cannot_benefit_from_move_semantics\n\
    {\n    int a;        // moving an int means copying an int\n    float b;     \
    \ // moving a float means copying a float\n    double c;     // moving a double\
    \ means copying a double\n    char d[64];   // moving a char array means copying\
    \ a char array\n\n    // ...\n};\n\nImplementing safe \"move-only\" types; that\
    \ is, types for which copying does not make sense, but moving does.  Examples\
    \ include locks, file handles, and smart pointers with unique ownership semantics.\
    \ Note: This answer discusses std::auto_ptr, a deprecated C++98 standard library\
    \ template, which was replaced by std::unique_ptr in C++11. Intermediate C++ programmers\
    \ are probably at least somewhat familiar with std::auto_ptr, and because of the\
    \ \"move semantics\" it displays, it seems like a good starting point for discussing\
    \ move semantics in C++11. YMMV.\n\nWhat is a move?\nThe C++98 standard library\
    \ offers a smart pointer with unique ownership semantics called std::auto_ptr<T>.\
    \ In case you are unfamiliar with auto_ptr, its purpose is to guarantee that a\
    \ dynamically allocated object is always released, even in the face of exceptions:\n\
    {\n    std::auto_ptr<Shape> a(new Triangle);\n    // ...\n    // arbitrary code,\
    \ could throw exceptions\n    // ...\n}   // <--- when a goes out of scope, the\
    \ triangle is deleted automatically\n\nThe unusual thing about auto_ptr is its\
    \ \"copying\" behavior:\nauto_ptr<Shape> a(new Triangle);\n\n      +---------------+\n\
    \      | triangle data |\n      +---------------+\n        ^\n        |\n    \
    \    |\n        |\n  +-----|---+\n  |   +-|-+ |\na | p | | | |\n  |   +---+ |\n\
    \  +---------+\n\nauto_ptr<Shape> b(a);\n\n      +---------------+\n      | triangle\
    \ data |\n      +---------------+\n        ^\n        |\n        +----------------------+\n\
    \                               |\n  +---------+            +-----|---+\n  | \
    \  +---+ |            |   +-|-+ |\na | p |   | |          b | p | | | |\n  | \
    \  +---+ |            |   +---+ |\n  +---------+            +---------+\n\nNote\
    \ how the initialization of b with a does not copy the triangle, but instead transfers\
    \ the ownership of the triangle from a to b. We also say \"a is moved into b\"\
    \ or \"the triangle is moved from a to b\". This may sound confusing, because\
    \ the triangle itself always stays at the same place in memory.\n\nTo move an\
    \ object means to transfer ownership of some resource it manages to another object.\n\
    \nThe copy constructor of auto_ptr probably looks something like this (somewhat\
    \ simplified):\nauto_ptr(auto_ptr& source)   // note the missing const\n{\n  \
    \  p = source.p;\n    source.p = 0;   // now the source no longer owns the object\n\
    }\n\nDangerous and harmless moves\nThe dangerous thing about auto_ptr is that\
    \ what syntactically looks like a copy is actually a move. Trying to call a member\
    \ function on a moved-from auto_ptr will invoke undefined behavior, so you have\
    \ to be very careful not to use an auto_ptr after it has been moved from:\nauto_ptr<Shape>\
    \ a(new Triangle);   // create triangle\nauto_ptr<Shape> b(a);              //\
    \ move a into b\ndouble area = a->area();           // undefined behavior\n\n\
    But auto_ptr is not always dangerous. Factory functions are a perfectly fine use\
    \ case for auto_ptr:\nauto_ptr<Shape> make_triangle()\n{\n    return auto_ptr<Shape>(new\
    \ Triangle);\n}\n\nauto_ptr<Shape> c(make_triangle());      // move temporary\
    \ into c\ndouble area = make_triangle()->area();   // perfectly safe\n\nNote how\
    \ both examples follow the same syntactic pattern:\nauto_ptr<Shape> variable(expression);\n\
    double area = expression->area();\n\nAnd yet, one of them invokes undefined behavior,\
    \ whereas the other one does not. So what is the difference between the expressions\
    \ a and make_triangle()? Aren't they both of the same type? Indeed they are, but\
    \ they have different value categories.\nValue categories\nObviously, there must\
    \ be some profound difference between the expression a which denotes an auto_ptr\
    \ variable, and the expression make_triangle() which denotes the call of a function\
    \ that returns an auto_ptr by value, thus creating a fresh temporary auto_ptr\
    \ object every time it is called. a is an example of an lvalue, whereas make_triangle()\
    \ is an example of an rvalue.\nMoving from lvalues such as a is dangerous, because\
    \ we could later try to call a member function via a, invoking undefined behavior.\
    \ On the other hand, moving from rvalues such as make_triangle() is perfectly\
    \ safe, because after the copy constructor has done its job, we cannot use the\
    \ temporary again. There is no expression that denotes said temporary; if we simply\
    \ write make_triangle() again, we get a different temporary. In fact, the moved-from\
    \ temporary is already gone on the next line:\nauto_ptr<Shape> c(make_triangle());\n\
    \                                  ^ the moved-from temporary dies right here\n\
    \nNote that the letters l and r have a historic origin in the left-hand side and\
    \ right-hand side of an assignment. This is no longer true in C++, because there\
    \ are lvalues which cannot appear on the left-hand side of an assignment (like\
    \ arrays or user-defined types without an assignment operator), and there are\
    \ rvalues which can (all rvalues of class types with an assignment operator).\n\
    \nAn rvalue of class type is an expression whose evaluation creates a temporary\
    \ object.\n  Under normal circumstances, no other expression inside the same scope\
    \ denotes the same temporary object.\n\nRvalue references\nWe now understand that\
    \ moving from lvalues is potentially dangerous, but moving from rvalues is harmless.\
    \ If C++ had language support to distinguish lvalue arguments from rvalue arguments,\
    \ we could either completely forbid moving from lvalues, or at least make moving\
    \ from lvalues explicit at call site, so that we no longer move by accident.\n\
    C++11's answer to this problem is rvalue references. An rvalue reference is a\
    \ new kind of reference that only binds to rvalues, and the syntax is X&&. The\
    \ good old reference X& is now known as an lvalue reference. (Note that X&& is\
    \ not a reference to a reference; there is no such thing in C++.)\nIf we throw\
    \ const into the mix, we already have four different kinds of references. What\
    \ kinds of expressions of type X can they bind to?\n            lvalue   const\
    \ lvalue   rvalue   const rvalue\n---------------------------------------------------------\
    \              \nX&          yes\nconst X&    yes      yes            yes    \
    \  yes\nX&&                                 yes\nconst X&&                   \
    \        yes      yes\n\nIn practice, you can forget about const X&&. Being restricted\
    \ to read from rvalues is not very useful.\n\nAn rvalue reference X&& is a new\
    \ kind of reference that only binds to rvalues.\n\nImplicit conversions\nRvalue\
    \ references went through several versions. Since version 2.1, an rvalue reference\
    \ X&& also binds to all value categories of a different type Y, provided there\
    \ is an implicit conversion from Y to X. In that case, a temporary of type X is\
    \ created, and the rvalue reference is bound to that temporary:\nvoid some_function(std::string&&\
    \ r);\n\nsome_function(\"hello world\");\n\nIn the above example, \"hello world\"\
    \ is an lvalue of type const char[12]. Since there is an implicit conversion from\
    \ const char[12] through const char* to std::string, a temporary of type std::string\
    \ is created, and r is bound to that temporary. This is one of the cases where\
    \ the distinction between rvalues (expressions) and temporaries (objects) is a\
    \ bit blurry.\nMove constructors\nA useful example of a function with an X&& parameter\
    \ is the move constructor X::X(X&& source). Its purpose is to transfer ownership\
    \ of the managed resource from the source into the current object.\nIn C++11,\
    \ std::auto_ptr<T> has been replaced by std::unique_ptr<T> which takes advantage\
    \ of rvalue references. I will develop and discuss a simplified version of unique_ptr.\
    \ First, we encapsulate a raw pointer and overload the operators -> and *, so\
    \ our class feels like a pointer:\ntemplate<typename T>\nclass unique_ptr\n{\n\
    \    T* ptr;\n\npublic:\n\n    T* operator->() const\n    {\n        return ptr;\n\
    \    }\n\n    T& operator*() const\n    {\n        return *ptr;\n    }\n\nThe\
    \ constructor takes ownership of the object, and the destructor deletes it:\n\
    \    explicit unique_ptr(T* p = nullptr)\n    {\n        ptr = p;\n    }\n\n \
    \   ~unique_ptr()\n    {\n        delete ptr;\n    }\n\nNow comes the interesting\
    \ part, the move constructor:\n    unique_ptr(unique_ptr&& source)   // note the\
    \ rvalue reference\n    {\n        ptr = source.ptr;\n        source.ptr = nullptr;\n\
    \    }\n\nThis move constructor does exactly what the auto_ptr copy constructor\
    \ did, but it can only be supplied with rvalues:\nunique_ptr<Shape> a(new Triangle);\n\
    unique_ptr<Shape> b(a);                 // error\nunique_ptr<Shape> c(make_triangle());\
    \   // okay\n\nThe second line fails to compile, because a is an lvalue, but the\
    \ parameter unique_ptr&& source can only be bound to rvalues. This is exactly\
    \ what we wanted; dangerous moves should never be implicit. The third line compiles\
    \ just fine, because make_triangle() is an rvalue. The move constructor will transfer\
    \ ownership from the temporary to c. Again, this is exactly what we wanted.\n\n\
    The move constructor transfers ownership of a managed resource into the current\
    \ object.\n\nMove assignment operators\nThe last missing piece is the move assignment\
    \ operator. Its job is to release the old resource and acquire the new resource\
    \ from its argument:\n    unique_ptr& operator=(unique_ptr&& source)   // note\
    \ the rvalue reference\n    {\n        if (this != &source)    // beware of self-assignment\n\
    \        {\n            delete ptr;         // release the old resource\n\n  \
    \          ptr = source.ptr;   // acquire the new resource\n            source.ptr\
    \ = nullptr;\n        }\n        return *this;\n    }\n};\n\nNote how this implementation\
    \ of the move assignment operator duplicates logic of both the destructor and\
    \ the move constructor. Are you familiar with the copy-and-swap idiom? It can\
    \ also be applied to move semantics as the move-and-swap idiom:\n    unique_ptr&\
    \ operator=(unique_ptr source)   // note the missing reference\n    {\n      \
    \  std::swap(ptr, source.ptr);\n        return *this;\n    }\n};\n\nNow that source\
    \ is a variable of type unique_ptr, it will be initialized by the move constructor;\
    \ that is, the argument will be moved into the parameter. The argument is still\
    \ required to be an rvalue, because the move constructor itself has an rvalue\
    \ reference parameter. When control flow reaches the closing brace of operator=,\
    \ source goes out of scope, releasing the old resource automatically.\n\nThe move\
    \ assignment operator transfers ownership of a managed resource into the current\
    \ object, releasing the old resource.\n  The move-and-swap idiom simplifies the\
    \ implementation.\n\nMoving from lvalues\nSometimes, we want to move from lvalues.\
    \ That is, sometimes we want the compiler to treat an lvalue as if it were an\
    \ rvalue, so it can invoke the move constructor, even though it could be potentially\
    \ unsafe.\nFor this purpose, C++11 offers a standard library function template\
    \ called std::move inside the header <utility>.\nThis name is a bit unfortunate,\
    \ because std::move simply casts an lvalue to an rvalue; it does not move anything\
    \ by itself. It merely enables moving. Maybe it should have been named std::cast_to_rvalue\
    \ or std::enable_move, but we are stuck with the name by now.\nHere is how you\
    \ explicitly move from an lvalue:\nunique_ptr<Shape> a(new Triangle);\nunique_ptr<Shape>\
    \ b(a);              // still an error\nunique_ptr<Shape> c(std::move(a));   //\
    \ okay\n\nNote that after the third line, a no longer owns a triangle. That's\
    \ okay, because by explicitly writing std::move(a), we made our intentions clear:\
    \ \"Dear constructor, do whatever you want with a in order to initialize c; I\
    \ don't care about a anymore. Feel free to have your way with a.\"\n\nstd::move(some_lvalue)\
    \ casts an lvalue to an rvalue, thus enabling a subsequent move.\n\nXvalues\n\
    Note that even though std::move(a) is an rvalue, its evaluation does not create\
    \ a temporary object. This conundrum forced the committee to introduce a third\
    \ value category. Something that can be bound to an rvalue reference, even though\
    \ it is not an rvalue in the traditional sense, is called an xvalue (eXpiring\
    \ value). The traditional rvalues were renamed to prvalues (Pure rvalues).\nBoth\
    \ prvalues and xvalues are rvalues. Xvalues and lvalues are both glvalues (Generalized\
    \ lvalues). The relationships are easier to grasp with a diagram:\n        expressions\n\
    \          /     \\\n         /       \\\n        /         \\\n    glvalues \
    \  rvalues\n      /  \\       /  \\\n     /    \\     /    \\\n    /      \\ \
    \  /      \\\nlvalues   xvalues   prvalues\n\nNote that only xvalues are really\
    \ new; the rest is just due to renaming and grouping.\n\nC++98 rvalues are known\
    \ as prvalues in C++11. Mentally replace all occurrences of \"rvalue\" in the\
    \ preceding paragraphs with \"prvalue\".\n\nMoving out of functions\nSo far, we\
    \ have seen movement into local variables, and into function parameters. But moving\
    \ is also possible in the opposite direction. If a function returns by value,\
    \ some object at call site (probably a local variable or a temporary, but could\
    \ be any kind of object) is initialized with the expression after the return statement\
    \ as an argument to the move constructor:\nunique_ptr<Shape> make_triangle()\n\
    {\n    return unique_ptr<Shape>(new Triangle);\n}          \\-----------------------------/\n\
    \                  |\n                  | temporary is moved into c\n        \
    \          |\n                  v\nunique_ptr<Shape> c(make_triangle());\n\nPerhaps\
    \ surprisingly, automatic objects (local variables that are not declared as static)\
    \ can also be implicitly moved out of functions:\nunique_ptr<Shape> make_square()\n\
    {\n    unique_ptr<Shape> result(new Square);\n    return result;   // note the\
    \ missing std::move\n}\n\nHow come the move constructor accepts the lvalue result\
    \ as an argument? The scope of result is about to end, and it will be destroyed\
    \ during stack unwinding. Nobody could possibly complain afterwards that result\
    \ had changed somehow; when control flow is back at the caller, result does not\
    \ exist anymore! For that reason, C++11 has a special rule that allows returning\
    \ automatic objects from functions without having to write std::move. In fact,\
    \ you should never use std::move to move automatic objects out of functions, as\
    \ this inhibits the \"named return value optimization\" (NRVO).\n\nNever use std::move\
    \ to move automatic objects out of functions.\n\nNote that in both factory functions,\
    \ the return type is a value, not an rvalue reference. Rvalue references are still\
    \ references, and as always, you should never return a reference to an automatic\
    \ object; the caller would end up with a dangling reference if you tricked the\
    \ compiler into accepting your code, like this:\nunique_ptr<Shape>&& flawed_attempt()\
    \   // DO NOT DO THIS!\n{\n    unique_ptr<Shape> very_bad_idea(new Square);\n\
    \    return std::move(very_bad_idea);   // WRONG!\n}\n\n\nNever return automatic\
    \ objects by rvalue reference. Moving is exclusively performed by the move constructor,\
    \ not by std::move, and not by merely binding an rvalue to an rvalue reference.\n\
    \nMoving into members\nSooner or later, you are going to write code like this:\n\
    class Foo\n{\n    unique_ptr<Shape> member;\n\npublic:\n\n    Foo(unique_ptr<Shape>&&\
    \ parameter)\n    : member(parameter)   // error\n    {}\n};\n\nBasically, the\
    \ compiler will complain that parameter is an lvalue. If you look at its type,\
    \ you see an rvalue reference, but an rvalue reference simply means \"a reference\
    \ that is bound to an rvalue\"; it does not mean that the reference itself is\
    \ an rvalue! Indeed, parameter is just an ordinary variable with a name. You can\
    \ use parameter as often as you like inside the body of the constructor, and it\
    \ always denotes the same object. Implicitly moving from it would be dangerous,\
    \ hence the language forbids it.\n\nA named rvalue reference is an lvalue, just\
    \ like any other variable.\n\nThe solution is to manually enable the move:\nclass\
    \ Foo\n{\n    unique_ptr<Shape> member;\n\npublic:\n\n    Foo(unique_ptr<Shape>&&\
    \ parameter)\n    : member(std::move(parameter))   // note the std::move\n   \
    \ {}\n};\n\nYou could argue that parameter is not used anymore after the initialization\
    \ of member. Why is there no special rule to silently insert std::move just as\
    \ with return values? Probably because it would be too much burden on the compiler\
    \ implementors. For example, what if the constructor body was in another translation\
    \ unit? By contrast, the return value rule simply has to check the symbol tables\
    \ to determine whether or not the identifier after the return keyword denotes\
    \ an automatic object.\nYou can also pass parameter by value. For move-only types\
    \ like unique_ptr, it seems there is no established idiom yet. Personally, I prefer\
    \ pass by value, as it causes less clutter in the interface.\nSpecial member functions\n\
    C++98 implicitly declares three special member functions on demand, that is, when\
    \ they are needed somewhere: the copy constructor, the copy assignment operator\
    \ and the destructor.\nX::X(const X&);              // copy constructor\nX& X::operator=(const\
    \ X&);   // copy assignment operator\nX::~X();                     // destructor\n\
    \nRvalue references went through several versions. Since version 3.0, C++11 declares\
    \ two additional special member functions on demand: the move constructor and\
    \ the move assignment operator. Note that neither VC10 nor VC11 conform to version\
    \ 3.0 yet, so you will have to implement them yourself.\nX::X(X&&);          \
    \         // move constructor\nX& X::operator=(X&&);        // move assignment\
    \ operator\n\nThese two new special member functions are only implicitly declared\
    \ if none of the special member functions are declared manually. Also, if you\
    \ declare your own move constructor or move assignment operator, neither the copy\
    \ constructor nor the copy assignment operator will be declared implicitly.\n\
    What do these rules mean in practice?\n\nIf you write a class without unmanaged\
    \ resources, there is no need to declare any of the five special member functions\
    \ yourself, and you will get correct copy semantics and move semantics for free.\
    \ Otherwise, you will have to implement the special member functions yourself.\
    \ Of course, if your class does not benefit from move semantics, there is no need\
    \ to implement the special move operations.\n\nNote that the copy assignment operator\
    \ and the move assignment operator can be fused into a single, unified assignment\
    \ operator, taking its argument by value:\nX& X::operator=(X source)    // unified\
    \ assignment operator\n{\n    swap(source);            // see my first answer\
    \ for an explanation\n    return *this;\n}\n\nThis way, the number of special\
    \ member functions to implement drops from five to four. There is a tradeoff between\
    \ exception-safety and efficiency here, but I am not an expert on this issue.\n\
    Forwarding references (previously known as Universal references)\nConsider the\
    \ following function template:\ntemplate<typename T>\nvoid foo(T&&);\n\nYou might\
    \ expect T&& to only bind to rvalues, because at first glance, it looks like an\
    \ rvalue reference. As it turns out though, T&& also binds to lvalues:\nfoo(make_triangle());\
    \   // T is unique_ptr<Shape>, T&& is unique_ptr<Shape>&&\nunique_ptr<Shape> a(new\
    \ Triangle);\nfoo(a);                 // T is unique_ptr<Shape>&, T&& is unique_ptr<Shape>&\n\
    \nIf the argument is an rvalue of type X, T is deduced to be X, hence T&& means\
    \ X&&. This is what anyone would expect.\nBut if the argument is an lvalue of\
    \ type X, due to a special rule, T is deduced to be X&, hence T&& would mean something\
    \ like X& &&. But since C++ still has no notion of references to references, the\
    \ type X& && is collapsed into X&. This may sound confusing and useless at first,\
    \ but reference collapsing is essential for perfect forwarding (which will not\
    \ be discussed here).\n\nT&& is not an rvalue reference, but a forwarding reference.\
    \ It also binds to lvalues, in which case T and T&& are both lvalue references.\n\
    \nIf you want to constrain a function template to rvalues, you can combine SFINAE\
    \ with type traits:\n#include <type_traits>\n\ntemplate<typename T>\ntypename\
    \ std::enable_if<std::is_rvalue_reference<T&&>::value, void>::type\nfoo(T&&);\n\
    \nImplementation of move\nNow that you understand reference collapsing, here is\
    \ how std::move is implemented:\ntemplate<typename T>\ntypename std::remove_reference<T>::type&&\n\
    move(T&& t)\n{\n    return static_cast<typename std::remove_reference<T>::type&&>(t);\n\
    }\n\nAs you can see, move accepts any kind of parameter thanks to the forwarding\
    \ reference T&&, and it returns an rvalue reference. The std::remove_reference<T>::type\
    \ meta-function call is necessary because otherwise, for lvalues of type X, the\
    \ return type would be X& &&, which would collapse into X&. Since t is always\
    \ an lvalue (remember that a named rvalue reference is an lvalue), but we want\
    \ to bind t to an rvalue reference, we have to explicitly cast t to the correct\
    \ return type.\nThe call of a function that returns an rvalue reference is itself\
    \ an xvalue. Now you know where xvalues come from ;)\n\nThe call of a function\
    \ that returns an rvalue reference, such as std::move, is an xvalue.\n\nNote that\
    \ returning by rvalue reference is fine in this example, because t does not denote\
    \ an automatic object, but instead an object that was passed in by the caller.\n"
- - What are move semantics?
  - "\nMove semantics are based on rvalue references.\nAn rvalue is a temporary object,\
    \ which is going to be destroyed at the end of the expression. In current C++,\
    \ rvalues only bind to const references. C++1x will allow non-const rvalue references,\
    \ spelled T&&, which are references to an rvalue objects.\nSince an rvalue is\
    \ going to die at the end of an expression, you can steal its data. Instead of\
    \ copying it into another object, you move its data into it. \nclass X {\npublic:\
    \ \n  X(X&& rhs) // ctor taking an rvalue reference, so-called move-ctor\n   \
    \ : data_()\n  {\n     // since 'x' is an rvalue object, we can steal its data\n\
    \     this->swap(std::move(rhs));\n     // this will leave rhs with the empty\
    \ data\n  }\n  void swap(X&& rhs);\n  // ... \n};\n\n// ...\n\nX f();\n\nX x =\
    \ f(); // f() returns result as rvalue, so this calls move-ctor\n\nIn the above\
    \ code, with old compilers the result of f() is copied into x using X's copy constructor.\
    \ If your compiler supports move semantics and X has a move-constructor, then\
    \ that is called instead. Since its rhs argument is an rvalue, we know it's not\
    \ needed any longer and we can steal its value.\nSo the value is moved from the\
    \ unnamed temporary returned from f() to x (while the data of x, initialized to\
    \ an empty X, is moved into the temporary, which will get destroyed after the\
    \ assignment). \n"
- - "Image Processing: Algorithm Improvement for 'Coca-Cola Can' Recognition"
  - "\nAn alternative approach would be to extract features (keypoints) using the\
    \ scale-invariant feature transform (SIFT) or Speeded Up Robust Features (SURF).\n\
    It is implemented in OpenCV 2.3.1.\nYou can find a nice code example using features\
    \ in Features2D + Homography to find a known object\nBoth algorithms are invariant\
    \ to scaling and rotation. Since they work with features, you can also handle\
    \ occlusion (as long as enough keypoints are visible).\n\nImage source: tutorial\
    \ example\nThe processing takes a few hundred ms for SIFT, SURF is bit faster,\
    \ but it not suitable for real-time applications. ORB uses FAST which is weaker\
    \ regarding rotation invariance.\nThe original papers\n\nSURF: Speeded Up Robust\
    \ Features\nDistinctive Image Features\nfrom Scale-Invariant Keypoints\nORB: an\
    \ efficient alternative to SIFT or SURF\n\n"
- - "Image Processing: Algorithm Improvement for 'Coca-Cola Can' Recognition"
  - "\nTo speed things up, I would take advantage of the fact that you are not asked\
    \ to find an arbitrary image/object, but specifically one with the Coca-Cola logo.\
    \ This is significant because this logo is very distinctive, and it should have\
    \ a characteristic, scale-invariant signature in the frequency domain, particularly\
    \ in the red channel of RGB. That is to say, the alternating pattern of red-to-white-to-red\
    \ encountered by a horizontal scan line (trained on a horizontally aligned logo)\
    \ will have a distinctive \"rhythm\" as it passes through the central axis of\
    \ the logo. That rhythm will \"speed up\" or \"slow down\" at different scales\
    \ and orientations, but will remain proportionally equivalent. You could identify/define\
    \ a few dozen such scanlines, both horizontally and vertically through the logo\
    \ and several more diagonally, in a starburst pattern. Call these the \"signature\
    \ scan lines.\"\n\nSearching for this signature in the target image is a simple\
    \ matter of scanning the image in horizontal strips. Look for a high-frequency\
    \ in the red-channel (indicating moving from a red region to a white one), and\
    \ once found, see if it is followed by one of the frequency rhythms identified\
    \ in the training session. Once a match is found, you will instantly know the\
    \ scan-line's orientation and location in the logo (if you keep track of those\
    \ things during training), so identifying the boundaries of the logo from there\
    \ is trivial. \nI would be surprised if this weren't a linearly-efficient algorithm,\
    \ or nearly so. It obviously doesn't address your can-bottle discrimination, but\
    \ at least you'll have your logos. \n(Update: for bottle recognition I would look\
    \ for coke (the brown liquid) adjacent to the logo --  that is, inside the bottle.\
    \ Or, in the case of an empty bottle, I would look for a cap which will always\
    \ have the same basic shape, size, and distance from the logo and will typically\
    \ be all white or red. Search for a solid color eliptical shape where a cap should\
    \ be, relative to the logo. Not foolproof of course, but your goal here should\
    \ be to find the easy ones fast.)\n(It's been a few years since my image processing\
    \ days, so I kept this suggestion high-level and conceptual. I think it might\
    \ slightly approximate how a human eye might operate -- or at least how my brain\
    \ does!)\n"
- - "Image Processing: Algorithm Improvement for 'Coca-Cola Can' Recognition"
  - "\nFun problem: when I glanced at your bottle image I thought it was a can too.\
    \ But, as a human, what I did to tell the difference is that I then noticed it\
    \ was also a bottle...\nSo, to tell cans and bottles apart, how about simply scanning\
    \ for bottles first? If you find one, mask out the label before looking for cans.\n\
    Not too hard to implement if you're already doing cans. The real downside is it\
    \ doubles your processing time. (But thinking ahead to real-world applications,\
    \ you're going to end up wanting to do bottles anyway ;-)\n"
- - Is < faster than <=?
  - "\nNo, it will not be faster on most architectures. You didn't specify, but on\
    \ x86, all of the integral comparisons will be typically implemented in two machine\
    \ instructions:\n\nA test or cmp instruction, which sets EFLAGS\nAnd a Jcc (jump)\
    \ instruction, depending on the comparison type (and code layout):\n\njne - Jump\
    \ if not equal --> ZF = 0\njz - Jump if zero (equal) --> ZF = 1\njg - Jump if\
    \ greater --> ZF = 0 and SF = OF\n(etc...)\n\n\n\nExample (Edited for brevity)\
    \ Compiled with $ gcc -m32 -S -masm=intel test.c\n    if (a < b) {\n        //\
    \ Do something 1\n    }\n\nCompiles to:\n    mov     eax, DWORD PTR [esp+24] \
    \     ; a\n    cmp     eax, DWORD PTR [esp+28]      ; b\n    jge     .L2     \
    \                     ; jump if a is >= b\n    ; Do something 1\n.L2:\n\nAnd\n\
    \    if (a <= b) {\n        // Do something 2\n    }\n\nCompiles to:\n    mov\
    \     eax, DWORD PTR [esp+24]      ; a\n    cmp     eax, DWORD PTR [esp+28]  \
    \    ; b\n    jg      .L5                          ; jump if a is > b\n    ; Do\
    \ something 2\n.L5:\n\nSo the only difference between the two is a jg versus a\
    \ jge instruction. The two will take the same amount of time.\n\nI'd like to address\
    \ the comment that nothing indicates that the different jump instructions take\
    \ the same amount of time.  This one is a little tricky to answer, but here's\
    \ what I can give: In the Intel Instruction Set Reference, they are all grouped\
    \ together under one common instruction, Jcc (Jump if condition is met). The same\
    \ grouping is made together under the Optimization Reference Manual, in Appendix\
    \ C. Latency and Throughput.\n\nLatency â The number of clock cycles that are\
    \ required for the\n  execution core to  complete the execution of all of the\
    \ Î¼ops that form\n  an instruction.\nThroughput â The number of clock cycles required\
    \ to\n  wait before the issue  ports are free to accept the same instruction\n\
    \  again. For many instructions, the  throughput of an instruction can be\n  significantly\
    \ less than its latency\n\nThe values for Jcc are:\n      Latency   Throughput\n\
    Jcc     N/A        0.5\n\nwith the following footnote on Jcc:\n\n7) Selection\
    \ of conditional jump instructions should be based on the recommendation of section\
    \ Section 3.4.1, âBranch Prediction Optimization,â to improve the  predictability\
    \ of branches. When branches are predicted successfully, the latency of jcc is\
    \ effectively zero.\n\nSo, nothing in the Intel docs ever treats one Jcc instruction\
    \ any differently from the others.\nIf one thinks about the actual circuitry used\
    \ to implement the instructions, one can assume that there would be simple AND/OR\
    \ gates on the different bits in EFLAGS, to determine whether the conditions are\
    \ met. There is then, no reason that an instruction testing two bits should take\
    \ any more or less time than one testing only one (Ignoring gate propagation delay,\
    \ which is much less than the clock period.)\n\nEdit: Floating Point\nThis holds\
    \ true for x87 floating point as well:  (Pretty much same code as above, but with\
    \ double instead of int.)\n        fld     QWORD PTR [esp+32]\n        fld   \
    \  QWORD PTR [esp+40]\n        fucomip st, st(1)              ; Compare ST(0)\
    \ and ST(1), and set CF, PF, ZF in EFLAGS\n        fstp    st(0)\n        seta\
    \    al                     ; Set al if above (CF=0 and ZF=0).\n        test \
    \   al, al\n        je      .L2\n        ; Do something 1\n.L2:\n\n        fld\
    \     QWORD PTR [esp+32]\n        fld     QWORD PTR [esp+40]\n        fucomip\
    \ st, st(1)              ; (same thing as above)\n        fstp    st(0)\n    \
    \    setae   al                     ; Set al if above or equal (CF=0).\n     \
    \   test    al, al\n        je      .L5\n        ; Do something 2\n.L5:\n    \
    \    leave\n        ret\n\n"
- - Is < faster than <=?
  - "\nHistorically (we're talking the 1980s and early 1990s), there were some architectures\
    \ in which this was true. The root issue is that integer comparison is inherently\
    \ implemented via integer subtractions. This gives rise to the following cases.\n\
    Comparison     Subtraction\n----------     -----------\nA < B      --> A - B <\
    \ 0\nA = B      --> A - B = 0\nA > B      --> A - B > 0\n\nNow, when A < B the\
    \ subtraction has to borrow a high-bit for the subtraction to be correct, just\
    \ like you carry and borrow when adding and subtracting by hand. This \"borrowed\"\
    \ bit was usually referred to as the carry bit and would be testable by a branch\
    \ instruction. A second bit called the zero bit would be set if the subtraction\
    \ were identically zero which implied equality.\nThere were usually at least two\
    \ conditional branch instructions, one to branch on the carry bit and one on the\
    \ zero bit.\nNow, to get at the heart of the matter, let's expand the previous\
    \ table to include the carry and zero bit results.\nComparison     Subtraction\
    \  Carry Bit  Zero Bit\n----------     -----------  ---------  --------\nA < B\
    \      --> A - B < 0    0          0\nA = B      --> A - B = 0    1          1\n\
    A > B      --> A - B > 0    1          0\n\nSo, implementing a branch for A <\
    \ B can be done in one instruction, because the carry bit is clear only in this\
    \ case, , that is,\n;; Implementation of \"if (A < B) goto address;\"\ncmp  A,\
    \ B          ;; compare A to B\nbcz  address       ;; Branch if Carry is Zero\
    \ to the new address\n\nBut, if we want to do a less-than-or-equal comparison,\
    \ we need to do an additional check of the zero flag to catch the case of equality.\n\
    ;; Implementation of \"if (A <= B) goto address;\"\ncmp A, B           ;; compare\
    \ A to B\nbcz address        ;; branch if A < B\nbzs address        ;; also, Branch\
    \ if the Zero bit is Set\n\nSo, on some machines, using a \"less than\" comparison\
    \ might save one machine instruction.  This was relevant in the era of sub-megahertz\
    \ processor speed and 1:1 CPU-to-memory speed ratios, but it is almost totally\
    \ irrelevant today.\n"
- - Is < faster than <=?
  - "\nAssuming we're talking about internal integer types, there's no possible way\
    \ one could be faster than the other. They're obviously semantically identical.\
    \ They both ask the compiler to do precisely the same thing. Only a horribly broken\
    \ compiler would generate inferior code for one of these.\nIf there was some platform\
    \ where < was faster than <= for simple integer types, the compiler should always\
    \ convert <= to < for constants. Any compiler that didn't would just be a bad\
    \ compiler (for that platform).\n"
- - Why does changing 0.1f to 0 slow down performance by 10x?
  - "\nWelcome to the world of denormalized floating-point! They can wreak havoc on\
    \ performance!!!\nDenormal (or subnormal) numbers are kind of a hack to get some\
    \ extra values very close to zero out of the floating point representation. Operations\
    \ on denormalized floating-point can be tens to hundreds of times slower than\
    \ on normalized floating-point. This is because many processors can't handle them\
    \ directly and must trap and resolve them using microcode.\nIf you print out the\
    \ numbers after 10,000 iterations, you will see that they have converged to different\
    \ values depending on whether 0 or 0.1 is used.\nHere's the test code compiled\
    \ on x64:\nint main() {\n\n    double start = omp_get_wtime();\n\n    const float\
    \ x[16]={1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0,2.1,2.2,2.3,2.4,2.5,2.6};\n \
    \   const float z[16]={1.123,1.234,1.345,156.467,1.578,1.689,1.790,1.812,1.923,2.034,2.145,2.256,2.367,2.478,2.589,2.690};\n\
    \    float y[16];\n    for(int i=0;i<16;i++)\n    {\n        y[i]=x[i];\n    }\n\
    \    for(int j=0;j<9000000;j++)\n    {\n        for(int i=0;i<16;i++)\n      \
    \  {\n            y[i]*=x[i];\n            y[i]/=z[i];\n#ifdef FLOATING\n    \
    \        y[i]=y[i]+0.1f;\n            y[i]=y[i]-0.1f;\n#else\n            y[i]=y[i]+0;\n\
    \            y[i]=y[i]-0;\n#endif\n\n            if (j > 10000)\n            \
    \    cout << y[i] << \"  \";\n        }\n        if (j > 10000)\n            cout\
    \ << endl;\n    }\n\n    double end = omp_get_wtime();\n    cout << end - start\
    \ << endl;\n\n    system(\"pause\");\n    return 0;\n}\n\nOutput:\n#define FLOATING\n\
    1.78814e-007  1.3411e-007  1.04308e-007  0  7.45058e-008  6.70552e-008  6.70552e-008\
    \  5.58794e-007  3.05474e-007  2.16067e-007  1.71363e-007  1.49012e-007  1.2666e-007\
    \  1.11759e-007  1.04308e-007  1.04308e-007\n1.78814e-007  1.3411e-007  1.04308e-007\
    \  0  7.45058e-008  6.70552e-008  6.70552e-008  5.58794e-007  3.05474e-007  2.16067e-007\
    \  1.71363e-007  1.49012e-007  1.2666e-007  1.11759e-007  1.04308e-007  1.04308e-007\n\
    \n//#define FLOATING\n6.30584e-044  3.92364e-044  3.08286e-044  0  1.82169e-044\
    \  1.54143e-044  2.10195e-044  2.46842e-029  7.56701e-044  4.06377e-044  3.92364e-044\
    \  3.22299e-044  3.08286e-044  2.66247e-044  2.66247e-044  2.24208e-044\n6.30584e-044\
    \  3.92364e-044  3.08286e-044  0  1.82169e-044  1.54143e-044  2.10195e-044  2.45208e-029\
    \  7.56701e-044  4.06377e-044  3.92364e-044  3.22299e-044  3.08286e-044  2.66247e-044\
    \  2.66247e-044  2.24208e-044\n\nNote how in the second run the numbers are very\
    \ close to zero.\nDenormalized numbers are generally rare and thus most processors\
    \ don't try to handle them efficiently.\n\nTo demonstrate that this has everything\
    \ to do with denormalized numbers, if we flush denormals to zero by adding this\
    \ to the start of the code:\n_MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);\n\nThen\
    \ the version with 0 is no longer 10x slower and actually becomes faster. (This\
    \ requires that the code be compiled with SSE enabled.)\nThis means that rather\
    \ than using these weird lower precision almost-zero values, we just round to\
    \ zero instead.\nTimings: Core i7 920 @ 3.5 GHz:\n//  Don't flush denormals to\
    \ zero.\n0.1f: 0.564067\n0   : 26.7669\n\n//  Flush denormals to zero.\n0.1f:\
    \ 0.587117\n0   : 0.341406\n\nIn the end, this really has nothing to do with whether\
    \ it's an integer or floating-point. The 0 or 0.1f is converted/stored into a\
    \ register outside of both loops. So that has no effect on performance.\n"
- - Why does changing 0.1f to 0 slow down performance by 10x?
  - "\nUsing gcc and applying a diff to the generated assembly yields only this difference:\n\
    73c68,69\n<   movss   LCPI1_0(%rip), %xmm1\n---\n>   movabsq $0, %rcx\n>   cvtsi2ssq\
    \   %rcx, %xmm1\n81d76\n<   subss   %xmm1, %xmm0\n\nThe cvtsi2ssq one being 10\
    \ times slower indeed.\nApparently, the float version uses an XMM register loaded\
    \ from memory, while the int version converts a real int value 0 to float using\
    \ the cvtsi2ssq instruction, taking a lot of time. Passing -O3 to gcc doesn't\
    \ help. (gcc version 4.2.1.)\n(Using double instead of float doesn't matter, except\
    \ that it changes the cvtsi2ssq into a cvtsi2sdq.)\nUpdate \nSome extra tests\
    \ show that it is not necessarily the cvtsi2ssq instruction. Once eliminated (using\
    \ a int ai=0;float a=ai; and using a instead of 0), the speed difference remains.\
    \ So @Mysticial is right, the denormalized floats make the difference. This can\
    \ be seen by testing values between 0 and 0.1f. The turning point in the above\
    \ code is approximately at 0.00000000000000000000000000000001, when the loops\
    \ suddenly takes 10 times as long.\nUpdate<<1 \nA small visualisation of this\
    \ interesting phenomenon:\n\nColumn 1: a float, divided by 2 for every iteration\n\
    Column 2: the binary representation of this float\nColumn 3: the time taken to\
    \ sum this float 1e7 times\n\nYou can clearly see the exponent (the last 9 bits)\
    \ change to its lowest value, when denormalization sets in. At that point, simple\
    \ addition becomes 20 times slower.\n0.000000000000000000000000000000000100000004670110:\
    \ 10111100001101110010000011100000 45 ms\n0.000000000000000000000000000000000050000002335055:\
    \ 10111100001101110010000101100000 43 ms\n0.000000000000000000000000000000000025000001167528:\
    \ 10111100001101110010000001100000 43 ms\n0.000000000000000000000000000000000012500000583764:\
    \ 10111100001101110010000110100000 42 ms\n0.000000000000000000000000000000000006250000291882:\
    \ 10111100001101110010000010100000 48 ms\n0.000000000000000000000000000000000003125000145941:\
    \ 10111100001101110010000100100000 43 ms\n0.000000000000000000000000000000000001562500072970:\
    \ 10111100001101110010000000100000 42 ms\n0.000000000000000000000000000000000000781250036485:\
    \ 10111100001101110010000111000000 42 ms\n0.000000000000000000000000000000000000390625018243:\
    \ 10111100001101110010000011000000 42 ms\n0.000000000000000000000000000000000000195312509121:\
    \ 10111100001101110010000101000000 43 ms\n0.000000000000000000000000000000000000097656254561:\
    \ 10111100001101110010000001000000 42 ms\n0.000000000000000000000000000000000000048828127280:\
    \ 10111100001101110010000110000000 44 ms\n0.000000000000000000000000000000000000024414063640:\
    \ 10111100001101110010000010000000 42 ms\n0.000000000000000000000000000000000000012207031820:\
    \ 10111100001101110010000100000000 42 ms\n0.000000000000000000000000000000000000006103515209:\
    \ 01111000011011100100001000000000 789 ms\n0.000000000000000000000000000000000000003051757605:\
    \ 11110000110111001000010000000000 788 ms\n0.000000000000000000000000000000000000001525879503:\
    \ 00010001101110010000100000000000 788 ms\n0.000000000000000000000000000000000000000762939751:\
    \ 00100011011100100001000000000000 795 ms\n0.000000000000000000000000000000000000000381469876:\
    \ 01000110111001000010000000000000 896 ms\n0.000000000000000000000000000000000000000190734938:\
    \ 10001101110010000100000000000000 813 ms\n0.000000000000000000000000000000000000000095366768:\
    \ 00011011100100001000000000000000 798 ms\n0.000000000000000000000000000000000000000047683384:\
    \ 00110111001000010000000000000000 791 ms\n0.000000000000000000000000000000000000000023841692:\
    \ 01101110010000100000000000000000 802 ms\n0.000000000000000000000000000000000000000011920846:\
    \ 11011100100001000000000000000000 809 ms\n0.000000000000000000000000000000000000000005961124:\
    \ 01111001000010000000000000000000 795 ms\n0.000000000000000000000000000000000000000002980562:\
    \ 11110010000100000000000000000000 835 ms\n0.000000000000000000000000000000000000000001490982:\
    \ 00010100001000000000000000000000 864 ms\n0.000000000000000000000000000000000000000000745491:\
    \ 00101000010000000000000000000000 915 ms\n0.000000000000000000000000000000000000000000372745:\
    \ 01010000100000000000000000000000 918 ms\n0.000000000000000000000000000000000000000000186373:\
    \ 10100001000000000000000000000000 881 ms\n0.000000000000000000000000000000000000000000092486:\
    \ 01000010000000000000000000000000 857 ms\n0.000000000000000000000000000000000000000000046243:\
    \ 10000100000000000000000000000000 861 ms\n0.000000000000000000000000000000000000000000022421:\
    \ 00001000000000000000000000000000 855 ms\n0.000000000000000000000000000000000000000000011210:\
    \ 00010000000000000000000000000000 887 ms\n0.000000000000000000000000000000000000000000005605:\
    \ 00100000000000000000000000000000 799 ms\n0.000000000000000000000000000000000000000000002803:\
    \ 01000000000000000000000000000000 828 ms\n0.000000000000000000000000000000000000000000001401:\
    \ 10000000000000000000000000000000 815 ms\n0.000000000000000000000000000000000000000000000000:\
    \ 00000000000000000000000000000000 42 ms\n0.000000000000000000000000000000000000000000000000:\
    \ 00000000000000000000000000000000 42 ms\n0.000000000000000000000000000000000000000000000000:\
    \ 00000000000000000000000000000000 44 ms\n\nAn equivalent discussion about ARM\
    \ can be found in StackÂ Overflow question Denormalized floating point in Objective-C?.\n"
- - Why does changing 0.1f to 0 slow down performance by 10x?
  - "\nIt's due to denormalized floating-point use. How to get rid of both it and\
    \ the performance penalty? Having scoured the Internet for ways of killing denormal\
    \ numbers, it seems there is no \"best\" way to do this yet. I have found these\
    \ three methods that may work best in different environments:\n\nMight not work\
    \ in some GCC environments:\n// Requires #include <fenv.h>\nfesetenv(FE_DFL_DISABLE_SSE_DENORMS_ENV);\n\
    \nMight not work in some Visual Studio environments: 1\n// Requires #include <xmmintrin.h>\n\
    _mm_setcsr( _mm_getcsr() | (1<<15) | (1<<6) );\n// Does both FTZ and DAZ bits.\
    \ You can also use just hex value 0x8040 to do both.\n// You might also want to\
    \ use the underflow mask (1<<11)\n\nAppears to work in both GCC and Visual Studio:\n\
    // Requires #include <xmmintrin.h>\n// Requires #include <pmmintrin.h>\n_MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);\n\
    _MM_SET_DENORMALS_ZERO_MODE(_MM_DENORMALS_ZERO_ON);\n\nThe Intel compiler has\
    \ options to disable denormals by default on modern Intel CPUs. More details here\n\
    Compiler switches. -ffast-math, -msse or -mfpmath=sse will disable denormals and\
    \ make a few other things faster, but unfortunately also do lots of other approximations\
    \ that might break your code. Test carefully! The equivalent of fast-math for\
    \ the Visual Studio compiler is /fp:fast but I haven't been able to confirm whether\
    \ this also disables denormals.1\n\n"
- - Why should I use a pointer rather than the object itself?
  - "\nIt's very unfortunate that you see dynamic allocation so often. That just shows\
    \ how many bad C++ programmers there are.\nIn a sense, you have two questions\
    \ bundled up into one. The first is when should we use dynamic allocation (using\
    \ new)? The second is when should we use pointers?\nThe important take-home message\
    \ is that you should always use the appropriate tool for the job. In almost all\
    \ situations, there is something more appropriate and safer than performing manual\
    \ dynamic allocation and/or using raw pointers.\nDynamic allocation\nIn your question,\
    \ you've demonstrated two ways of creating an object. The main difference is the\
    \ storage duration of the object. When doing Object myObject; within a block,\
    \ the object is created with automatic storage duration, which means it will be\
    \ destroyed automatically when it goes out of scope. When you do new Object(),\
    \ the object has dynamic storage duration, which means it stays alive until you\
    \ explicitly delete it. You should only use dynamic storage duration when you\
    \ need it. \nThat is, you should always prefer creating objects with automatic\
    \ storage duration when you can.\nThe main two situations in which you might require\
    \ dynamic allocation:\n\nYou need the object to outlive the current scope - that\
    \ specific object at that specific memory location, not a copy of it. If you're\
    \ okay with copying/moving the object (most of the time you should be), you should\
    \ prefer an automatic object.\nYou need to allocate a lot of memory, which may\
    \ easily fill up the stack. It would be nice if we didn't have to concern ourselves\
    \ with this (most of the time you shouldn't have to), as it's really outside the\
    \ purview of C++, but unfortunately we have to deal with the reality of the systems\
    \ we're developing for.\n\nWhen you do absolutely require dynamic allocation,\
    \ you should encapsulate it in a smart pointer or some other type that performs\
    \ RAII (like the standard containers). Smart pointers provide ownership semantics\
    \ of dynamically allocated objects. Take a look at std::unique_ptr and std::shared_ptr,\
    \ for example. If you use them appropriately, you can almost entirely avoid performing\
    \ your own memory management (see the Rule of Zero).\nPointers\nHowever, there\
    \ are other more general uses for raw pointers beyond dynamic allocation, but\
    \ most have alternatives that you should prefer. As before, always prefer the\
    \ alternatives unless you really need pointers.\n\nYou need reference semantics.\
    \ Sometimes you want to pass an object using a pointer (regardless of how it was\
    \ allocated) because you want the function to which you're passing it to have\
    \ access that that specific object (not a copy of it). However, in most situations,\
    \ you should prefer reference types to pointers, because this is specifically\
    \ what they're designed for. Note this is not necessarily about extending the\
    \ lifetime of the object beyond the current scope, as in situation 1 above. As\
    \ before, if you're okay with passing a copy of the object, you don't need reference\
    \ semantics.\nYou need polymorphism. You can only call functions polymorphically\
    \ (that is, according to the dynamic type of an object) through a pointer or reference\
    \ to the object. If that's the behaviour you need, then you need to use pointers\
    \ or references. Again, references should be preferred.\nYou want to represent\
    \ that an object is optional by allowing a nullptr to be passed when the object\
    \ is being omitted. If it's an argument, you should prefer to use default arguments\
    \ or function overloads. Otherwise, you should prefer use a type that encapsulates\
    \ this behaviour, such as std::optional (introduced in C++17 - with earlier C++\
    \ standards, use boost::optional).\nYou want to decouple compilation units to\
    \ improve compilation time. The useful property of a pointer is that you only\
    \ require a forward declaration of the pointed-to type (to actually use the object,\
    \ you'll need a definition). This allows you to decouple parts of your compilation\
    \ process, which may significantly improve compilation time. See the Pimpl idiom.\n\
    You need to interface with a C library or a C-style library. At this point, you're\
    \ forced to use raw pointers. The best thing you can do is make sure you only\
    \ let your raw pointers loose at the last possible moment. You can get a raw pointer\
    \ from a smart pointer, for example, by using its get member function. If a library\
    \ performs some allocation for you which it expects you to deallocate via a handle,\
    \ you can often wrap the handle up in a smart pointer with a custom deleter that\
    \ will deallocate the object appropriately.\n\n"
- - Why should I use a pointer rather than the object itself?
  - "\nThere are many use cases for pointers. \nPolymorphic behavior. For polymorphic\
    \ types, pointers (or references) are used to avoid slicing:\nclass Base { ...\
    \ };\nclass Derived : public Base { ... };\n\nvoid fun(Base b) { ... }\nvoid gun(Base*\
    \ b) { ... }\nvoid hun(Base& b) { ... }\n\nDerived d;\nfun(d);    // oops, all\
    \ Derived parts silently \"sliced\" off\ngun(&d);   // OK, a Derived object IS-A\
    \ Base object\nhun(d);    // also OK, reference also doesn't slice\n\nReference\
    \ semantics and avoiding copying. For non-polymorphic types, a pointer (or a reference)\
    \ will avoid copying a potentially expensive object\nBase b;\nfun(b);  // copies\
    \ b, potentially expensive \ngun(&b); // takes a pointer to b, no copying\nhun(b);\
    \  // regular syntax, behaves as a pointer\n\nNote that C++11 has move semantics\
    \ that can avoid many copies of expensive objects into function argument and as\
    \ return values. But using a pointer will definitely avoid those and will allow\
    \ multiple pointers on the same object (whereas an object can only be moved from\
    \ once).\nResource acquisition. Creating a pointer to a resource using the new\
    \ operator is an anti-pattern in modern C++. Use a special resource class (one\
    \ of the Standard containers) or a smart pointer (std::unique_ptr<> or std::shared_ptr<>).\
    \ Consider:   \n{\n    auto b = new Base;\n    ...       // oops, if an exception\
    \ is thrown, destructor not called!\n    delete b;\n}\n\nvs. \n{\n    auto b =\
    \ std::make_unique<Base>();\n    ...       // OK, now exception safe\n}\n\nA raw\
    \ pointer should only be used as a \"view\" and not in any way involved in ownership,\
    \ be it through direct creation or implicitly through return values. See also\
    \ this Q&A from the C++ FAQ.\nMore fine-grained life-time control Every time a\
    \ shared pointer is being copied (e.g. as a function argument) the resource it\
    \ points to is being kept alive. Regular objects (not created by new, either directly\
    \ by you or inside a resource class) are destroyed when going out of scope.\n"
- - Why should I use a pointer rather than the object itself?
  - "\nThere are many excellent answers to this question, including the important\
    \ use cases of forward declarations, polymorphism etc. but I feel a part of the\
    \ \"soul\" of your question is not answered - namely what the different syntaxes\
    \ mean across Java and C++.\nLet's examine the situation comparing the two languages:\n\
    Java:\nObject object1 = new Object(); //A new object is allocated by Java\nObject\
    \ object2 = new Object(); //Another new object is allocated by Java\n\nobject1\
    \ = object2; \n//object1 now points to the object originally allocated for object2\n\
    //The object originally allocated for object1 is now \"dead\" - nothing points\
    \ to it, so it\n//will be reclaimed by the Garbage Collector.\n//If either object1\
    \ or object2 is changed, the change will be reflected to the other\n\nThe closest\
    \ equivalent to this, is:\nC++:\nObject * object1 = new Object(); //A new object\
    \ is allocated on the heap\nObject * object2 = new Object(); //Another new object\
    \ is allocated on the heap\ndelete object1;\n//Since C++ does not have a garbage\
    \ collector, if we don't do that, the next line would \n//cause a \"memory leak\"\
    , i.e. a piece of claimed memory that the app cannot use \n//and that we have\
    \ no way to reclaim...\n\nobject1 = object2; //Same as Java, object1 points to\
    \ object2.\n\nLet's see the alternative C++ way:\nObject object1; //A new object\
    \ is allocated on the STACK\nObject object2; //Another new object is allocated\
    \ on the STACK\nobject1 = object2;//!!!! This is different! The CONTENTS of object2\
    \ are COPIED onto object1,\n//using the \"copy assignment operator\", the definition\
    \ of operator =.\n//But, the two objects are still different. Change one, the\
    \ other remains unchanged.\n//Also, the objects get automatically destroyed once\
    \ the function returns...\n\nThe best way to think of it is that -- more or less\
    \ -- Java (implicitly) handles pointers to objects, while C++ may handle either\
    \ pointers to objects, or the objects themselves.\nThere are exceptions to this\
    \ -- for example, if you declare Java \"primitive\" types, they are actual values\
    \ that are copied, and not pointers.\nSo,\nJava:\nint object1; //An integer is\
    \ allocated on the stack.\nint object2; //Another integer is allocated on the\
    \ stack.\nobject1 = object2; //The value of object2 is copied to object1.\n\n\
    That said, using pointers is NOT necessarily either the correct or the wrong way\
    \ to handle things; however other answers have covered that satisfactorily. The\
    \ general idea though is that in C++ you have much more control on the lifetime\
    \ of the objects, and on where they will live.\nTake home point -- the Object\
    \ * object = new Object() construct is actually what is closest to typical Java\
    \ (or C# for that matter) semantics.\n"
- - Compiling an application for use in highly radioactive environments
  - "\nWorking for about 4-5 years with software/firmware development and environment\
    \ testing of miniaturized satellites*, I would like to share my experience here.\n\
    *(miniaturized satellites are a lot more prone to single event upsets than bigger\
    \ satellites due to its relatively small, limited sizes for its electronic components)\n\
    \nTo be very concise and direct: there is no mechanism to recover from detectable,\
    \ erroneous\n  situation by the software/firmware itself without, at least, one\n\
    \  copy of minimum working version of the software/firmware somewhere for recovery\
    \ purpose - and with the hardware supporting the recovery (functional).\n\nNow,\
    \ this situation is normally handled both in the hardware and software level.\
    \ Here, as you request, I will share what we can do in the software level.\n\n\
    ...recovery purpose.... Provide ability to update/recompile/reflash your software/firmware\
    \ in real environment. This is an almost must-have feature for any software/firmware\
    \ in highly ionized environment. Without this, you could have redundant software/hardware\
    \ as many as you want but at one point, they are all going to blow up. So, prepare\
    \ this feature!\n...minimum working version... Have responsive, multiple copies,\
    \ minimum version of the software/firmware in your code. This is like Safe mode\
    \ in Windows. Instead of having only one, fully functional version of your software,\
    \ have multiple copies of the minimum version of your software/firmware. The minimum\
    \ copy will usually having much less size than the full copy and almost always\
    \ have only the following two or three features: \n\ncapable of listening to command\
    \ from external system, \ncapable of updating the current software/firmware, \n\
    capable of monitoring the basic operation's housekeeping data.\n\n...copy... somewhere...\
    \ Have redundant software/firmware somewhere. \n\nYou could, with or without redundant\
    \ hardware, try to have redundant software/firmware in your ARM uC. This is normally\
    \ done by having two or more identical software/firmware in separate addresses\
    \ which sending heartbeat to each other - but only one will be active at a time.\
    \ If one or more software/firmware is known to be unresponsive, switch to the\
    \ other software/firmware. The benefit of using this approach is we can have functional\
    \ replacement immediately after an error occurs - without any contact with whatever\
    \ external system/party who is responsible to detect and to repair the error (in\
    \ satellite case, it is usually the Mission Control Centre (MCC)). \nStrictly\
    \ speaking, without redundant hardware, the disadvantage of doing this is you\
    \ actually cannot eliminate all single point of failures. At the very least, you\
    \ will still have one single point of failure, which is the switch itself (or\
    \ often the beginning of the code). Nevertheless, for a device limited by size\
    \ in a highly ionized environment (such as pico/femto satellites), the reduction\
    \ of the single point of failures to one point without additional hardware will\
    \ still be worth considering. Somemore, the piece of code for the switching would\
    \ certainly be much less than the code for the whole program - significantly reducing\
    \ the risk of getting Single Event in it.\nBut if you are not doing this, you\
    \ should have at least one copy in your external system which can come in contact\
    \ with the device and update the software/firmware (in the satellite case, it\
    \ is again the mission control centre). \nYou could also have the copy in your\
    \ permanent memory storage in your device which can be triggered to restore the\
    \ running system's software/firmware\n\n...detectable erroneous situation.. The\
    \ error must be detectable, usually by the hardware error correction/detection\
    \ circuit or by a small piece of code for error correction/detection. It is best\
    \ to put such code small, multiple, and independent from the main software/firmware.\
    \ Its main task is only for checking/correcting. If the hardware circuit/firmware\
    \ is reliable (such as it is more radiation hardened than the rests - or having\
    \ multiple circuits/logics), then you might consider making error-correction with\
    \ it. But if it is not, it is better to make it as error-detection. The correction\
    \ can be by external system/device. For the error correction, you could consider\
    \ making use of a basic error correction algorithm like Hamming/Golay23, because\
    \ they can be implemented more easily both in the circuit/software. But it ultimately\
    \ depends on your team's capability. For error detection, normally CRC is used.\n\
    ...hardware supporting the recovery Now, comes to the most difficult aspect on\
    \ this issue. Ultimately, the recovery requires the hardware which is responsible\
    \ for the recovery to be at least functional. If the hardware is permanently broken\
    \ (normally happen after its Total ionizing dose reaches certain level), then\
    \ there is (sadly) no way for the software to help in recovery. Thus, hardware\
    \ is rightly the utmost importance concern for a device exposed to high radiation\
    \ level (such as satellite). \n\nIn addition to the suggestion for above anticipating\
    \ firmware's error due to single event upset, I would also like to suggest you\
    \ to have:\n\nError detection and/or error correction algorithm in the inter-subsystem\
    \ communication protocol. This is another almost must have in order to avoid incomplete/wrong\
    \ signals received from other system\nFilter in your ADC reading. Do not use the\
    \ ADC reading directly. Filter it by median filter, mean filter, or any other\
    \ filters - never trust single reading value. Sample more, not less - reasonably.\n\
    \n"
- - Compiling an application for use in highly radioactive environments
  - "\nNASA has a paper on radiation-hardened software. It describes three main tasks:\n\
    \nRegular monitoring of memory for errors then scrubbing out those errors,\nrobust\
    \ error recovery mechanisms, and\nthe ability to reconfigure if something no longer\
    \ works.\n\nNote that the memory scan rate should be frequent enough that multi-bit\
    \ errors rarely occur, as most ECC memory can recover from single-bit errors,\
    \ not multi-bit errors.\nRobust error recovery includes control flow transfer\
    \ (typically restarting a process at a point before the error), resource release,\
    \ and data restoration.\nTheir main recommendation for data restoration is to\
    \ avoid the need for it, through having intermediate data be treated as temporary,\
    \ so that restarting before the error also rolls back the data to a reliable state.\
    \ This sounds similar to the concept of \"transactions\" in databases.\nThey discuss\
    \ techniques particularly suitable for object-oriented languages such as C++.\
    \ For example\n\nSoftware-based ECCs for contiguous memory objects\nProgramming\
    \ by Contract: verifying preconditions and postconditions, then checking the object\
    \ to verify it is still in a valid state.\n\nAnd, it just so happens, NASA has\
    \ used C++ for major projects such as the Mars Rover.\n\nC++ class abstraction\
    \ and encapsulation enabled rapid development and testing among multiple projects\
    \ and developers.\n\nThey avoided certain C++ features that could create problems:\n\
    \nExceptions\nTemplates\nIostream (no console)\nMultiple inheritance\nOperator\
    \ overloading (other than new and delete)\nDynamic allocation (used a dedicated\
    \ memory pool and placement new to avoid the possibility of system heap corruption).\n\
    \n"
- - Compiling an application for use in highly radioactive environments
  - "\nHere are some thoughts and ideas:\nUse ROM more creatively.\nStore anything\
    \ you can in ROM. Instead of calculating things, store look-up tables in ROM.\
    \ (Make sure your compiler is outputting your look-up tables to the read-only\
    \ section! Print out memory addresses at runtime to check!) Store your interrupt\
    \ vector table in ROM. Of course, run some tests to see how reliable your ROM\
    \ is compared to your RAM.\nUse your best RAM for the stack.\nSEUs in the stack\
    \ are probably the most likely source of crashes, because it is where things like\
    \ index variables, status variables, return addresses, and pointers of various\
    \ sorts typically live.\nImplement timer-tick and watchdog timer routines.\nYou\
    \ can run a \"sanity check\" routine every timer tick, as well as a watchdog routine\
    \ to handle the system locking up. Your main code could also periodically increment\
    \ a counter to indicate progress, and the sanity-check routine could ensure this\
    \ has occurred.\nImplement error-correcting-codes in software.\nYou can add redundancy\
    \ to your data to be able to detect and/or correct errors. This will add processing\
    \ time, potentially leaving the processor exposed to radiation for a longer time,\
    \ thus increasing the chance of errors, so you must consider the trade-off.\n\
    Remember the caches.\nCheck the sizes of your CPU caches. Data that you have accessed\
    \ or modified recently will probably be within a cache. I believe you can disable\
    \ at least some of the caches (at a big performance cost); you should try this\
    \ to see how susceptible the caches are to SEUs. If the caches are hardier than\
    \ RAM then you could regularly read and re-write critical data to make sure it\
    \ stays in cache and bring RAM back into line.\nUse page-fault handlers cleverly.\n\
    If you mark a memory page as not-present, the CPU will issue a page fault when\
    \ you try to access it. You can create a page-fault handler that does some checking\
    \ before servicing the read request. (PC operating systems use this to transparently\
    \ load pages that have been swapped to disk.)\nUse assembly language for critical\
    \ things (which could be everything).\nWith assembly language, you know what is\
    \ in registers and what is in RAM; you know what special RAM tables the CPU is\
    \ using, and you can design things in a roundabout way to keep your risk down.\n\
    Use objdump to actually look at the generated assembly language, and work out\
    \ how much code each of your routines takes up.\nIf you are using a big OS like\
    \ Linux then you are asking for trouble; there is just so much complexity and\
    \ so many things to go wrong.\nRemember it is a game of probabilities.\nA commenter\
    \ said\n\nEvery routine you write to catch errors will be subject to failing itself\
    \ from the same cause.\n\nWhile this is true, the chances of errors in the (say)\
    \ 100 bytes of code and data required for a check routine to function correctly\
    \ is much smaller than the chance of errors elsewhere. If your ROM is pretty reliable\
    \ and almost all the code/data is actually in ROM then your odds are even better.\n\
    Use redundant hardware.\nUse 2 or more identical hardware setups with identical\
    \ code. If the results differ, a reset should be triggered. With 3 or more devices\
    \ you can use a \"voting\" system to try to identify which one has been compromised.\n"
- - What is the effect of extern âCâ in C++?
  - "\nextern \"C\" makes a function-name in C++ have 'C' linkage (compiler does not\
    \ mangle the name) so that client C code can link to (i.e use) your function using\
    \ a 'C' compatible header file that contains just the declaration of your function.\
    \ Your function definition is contained in a binary format (that was compiled\
    \ by your C++ compiler) that the client 'C' linker will then link to using the\
    \ 'C' name.\nSince C++ has overloading of function names and C does not, the C++\
    \ compiler cannot just use the function name as a unique id to link to, so it\
    \ mangles the name by adding information about the arguments.  A C compiler does\
    \ not need to mangle the name since you can not overload function names in C.\
    \  When you state that a function has extern \"C\" linkage in C++, the C++ compiler\
    \ does not add argument/parameter type information to the name used for linkage.\n\
    Just so you know, you can specify \"C\" linkage to each individual declaration/definition\
    \ explicitly or use a block to group a sequence of declarations/definitions to\
    \ have a certain linkage:\nextern \"C\" void foo(int);\nextern \"C\"\n{\n   void\
    \ g(char);\n   int i;\n}\n\nIf you care about the technicalities, they are listed\
    \ in section 7.5 of the C++03 standard, here is a brief summary (with emphasis\
    \ on extern \"C\"):\n\nextern \"C\" is a linkage-specification\nEvery compiler\
    \ is required to provide \"C\" linkage\na linkage specification shall occur only\
    \ in namespace scope\n all function types, function names and variable names have\
    \ a language linkage  See Richard's Comment: Only function names and variable\
    \ names with external linkage have a language linkage\ntwo function types with\
    \ distinct language linkages are distinct types even if otherwise identical\n\
    linkage specs nest, inner one determines the final linkage\nextern \"C\" is ignored\
    \ for class members \nat most one function with a particular name can have \"\
    C\" linkage (regardless of namespace)\n extern \"C\" forces a function to have\
    \ external linkage (cannot make it static)   See Richard's comment:    'static'\
    \ inside 'extern \"C\"' is valid; an entity so declared has internal linkage,\
    \ and so does not have a language linkage \nLinkage from C++ to objects defined\
    \ in other languages and to objects defined in C++ from other languages is implementation-defined\
    \ and language-dependent. Only where the object layout strategies of two language\
    \ implementations are similar enough can such linkage be achieved  \n\n"
- - What is the effect of extern âCâ in C++?
  - "\nJust wanted to add a bit of info, since I haven't seen it posted yet.\nYou'll\
    \ very often see code in C headers like so:\n#ifdef __cplusplus\nextern \"C\"\
    \ {\n#endif\n\n// all of your legacy C code here\n\n#ifdef __cplusplus\n}\n#endif\n\
    \nWhat this accomplishes is that it allows you to use that C header file with\
    \ your C++ code, because the macro \"__cplusplus\" will be defined. But you can\
    \ also still use it with your legacy C code, where the macro is NOT defined, so\
    \ it won't see the uniquely C++ construct.\nAlthough, I have also seen C++ code\
    \ such as:\nextern \"C\" {\n#include \"legacy_C_header.h\"\n}\n\nwhich I imagine\
    \ accomplishes much the same thing.\nNot sure which way is better, but I have\
    \ seen both.\n"
- - What is the effect of extern âCâ in C++?
  - "\nIn every C++ program, all non-static functions are represented in the binary\
    \ file as symbols. These symbols are special text strings that uniquely identify\
    \ a function in the program.\nIn C, the symbol name is the same as the function\
    \ name. This is possible because in C no two non-static functions can have the\
    \ same name.\nBecause C++ allows overloading and has many features that C does\
    \ not â like classes, member functions, exception specifications - it is not possible\
    \ to simply use the function name as the symbol name. To solve that, C++ uses\
    \ so-called name mangling, which transforms the function name and all the necessary\
    \ information (like the number and size of the arguments) into some weird-looking\
    \ string processed only by the compiler and linker.\nSo if you specify a function\
    \ to be extern C, the compiler doesn't performs name mangling with it and it can\
    \ be directly \naccessed using its symbol name as the function name.\nThis comes\
    \ handy while using dlsym() and dlopen() for calling such functions.\n"
- - Easiest way to convert int to string in C++
  - "\nC++11 introduces std::stoi (and variants for each numeric type) and std::to_string,\
    \ the counterparts of the C atoi and itoa but expressed in term of std::string.\n\
    #include <string> \n\nstd::string s = std::to_string(42);\n\nis therefore the\
    \ shortest way I can think of. You can even omit naming the type, using the auto\
    \ keyword:\nauto s = std::to_string(42);\n\nNote: see [string.conversions] (21.5\
    \ in n3242)\n"
- - Easiest way to convert int to string in C++
  - "\nPicking up a discussion with @v.oddou a couple of years later, C++17 has finally\
    \ delivered a way to do the originally macro-based type-agnostic solution (preserved\
    \ below) without going through macro uglyness.\n// variadic template\ntemplate\
    \ < typename... Args >\nstd::string sstr( Args &&... args )\n{\n    std::ostringstream\
    \ sstr;\n    // fold expression\n    ( sstr << std::dec << ... << args );\n  \
    \  return sstr.str();\n}\n\nUsage:\nint i = 42;\nstd::string s = sstr( \"i is:\
    \ \", i );\nputs( sstr( i ).c_str() );\n\nFoo x( 42 );\nthrow std::runtime_error(\
    \ sstr( \"Foo is '\", x, \"', i is \", i ) );\n\n\nOriginal answer:\nSince \"\
    converting ... to string\" is a recurring problem, I always define the SSTR()\
    \ macro in a central header of my C++ sources:\n#include <sstream>\n\n#define\
    \ SSTR( x ) static_cast< std::ostringstream & >( \\\n        ( std::ostringstream()\
    \ << std::dec << x ) ).str()\n\nUsage is as easy as could be:\nint i = 42;\nstd::string\
    \ s = SSTR( \"i is: \" << i );\nputs( SSTR( i ).c_str() );\n\nFoo x( 42 );\nthrow\
    \ std::runtime_error( SSTR( \"Foo is '\" << x << \"', i is \" << i ) );\n\nThe\
    \ above is C++98 compatible (if you cannot use C++11 std::to_string), and does\
    \ not need any third-party includes (if you cannot use Boost lexical_cast<>);\
    \ both these other solutions have a better performance though.\n"
- - Easiest way to convert int to string in C++
  - "\nI usually use the following method:\n#include <sstream>\n\ntemplate <typename\
    \ T>\n  std::string NumberToString ( T Number )\n  {\n     std::ostringstream\
    \ ss;\n     ss << Number;\n     return ss.str();\n  }\n\ndescribed in details\
    \ here.\n"
- - When to use virtual destructors?
  - "\nVirtual destructors are useful when you might potentially delete an instance\
    \ of a derived class through a pointer to base class:\nclass Base \n{\n    //\
    \ some virtual methods\n};\n\nclass Derived : public Base\n{\n    ~Derived()\n\
    \    {\n        // Do some important cleanup\n    }\n};\n\nHere, you'll notice\
    \ that I didn't declare Base's destructor to be virtual. Now, let's have a look\
    \ at the following snippet:\nBase *b = new Derived();\n// use b\ndelete b; //\
    \ Here's the problem!\n\nSince Base's destructor is not virtual and b is a Base*\
    \ pointing to a Derived object, delete b has undefined behaviour:\n\n[In delete\
    \ b], if the static type of the\n  object to be deleted is different from its\
    \ dynamic type, the static\n  type shall be a base class of the dynamic type of\
    \ the object to be\n  deleted and the static type shall have a virtual destructor\
    \ or the\n  behavior is undefined.\n\nIn most implementations, the call to the\
    \ destructor will be resolved like any non-virtual code, meaning that the destructor\
    \ of the base class will be called but not the one of the derived class, resulting\
    \ in a resources leak.\nTo sum up, always make base classes' destructors virtual\
    \ when they're meant to be manipulated polymorphically.\nIf you want to prevent\
    \ the deletion of an instance through a base class pointer, you can make the base\
    \ class destructor protected and nonvirtual; by doing so, the compiler won't let\
    \ you call delete on a base class pointer.\nYou can learn more about virtuality\
    \ and virtual base class destructor in this article from Herb Sutter.\n"
- - When to use virtual destructors?
  - "\nDeclare destructors virtual in polymorphic base classes.  This is Item 7 in\
    \ Scott Meyers' Effective C++.  Meyers goes on to summarize that if a class has\
    \ any virtual function, it should have a virtual destructor, and that classes\
    \ not designed to be base classes or not designed to be used polymorphically should\
    \ not declare virtual destructors.\n"
- - When to use virtual destructors?
  - "\nA virtual constructor is not possible but virtual destructor is possible.\n\
    Let us experiment....\n#include <iostream>\n\nusing namespace std;\n\nclass Base\n\
    {\npublic:\n    Base(){\n        cout << \"Base Constructor Called\\n\";\n   \
    \ }\n    ~Base(){\n        cout << \"Base Destructor called\\n\";\n    }\n};\n\
    \nclass Derived1: public Base\n{\npublic:\n    Derived1(){\n        cout << \"\
    Derived constructor called\\n\";\n    }\n    ~Derived1(){\n        cout << \"\
    Derived destructor called\\n\";\n    }\n};\n\nint main()\n{\n    Base *b = new\
    \ Derived1();\n    delete b;\n}\n\nThe above code output the following:\nBase\
    \ Constructor Called\nDerived constructor called\nBase Destructor called\n\nThe\
    \ construction of derived object follow the construction rule but when we delete\
    \ the \"b\" pointer(base pointer) we have found that only the base destructor\
    \ is call.But this must not be happened. To do the appropriate thing we have to\
    \ make the base destructor virtual.\nNow let see what happen in the following:\n\
    #include <iostream>\n\nusing namespace std;\n\nclass Base\n{ \npublic:\n    Base(){\n\
    \        cout << \"Base Constructor Called\\n\";\n    }\n    virtual ~Base(){\n\
    \        cout << \"Base Destructor called\\n\";\n    }\n};\n\nclass Derived1:\
    \ public Base\n{\npublic:\n    Derived1(){\n        cout << \"Derived constructor\
    \ called\\n\";\n    }\n    ~Derived1(){\n        cout << \"Derived destructor\
    \ called\\n\";\n    }\n};\n\nint main()\n{\n    Base *b = new Derived1();\n  \
    \  delete b;\n}\n\nThe output changed as following:\nBase Constructor Called\n\
    Derived constructor called\nDerived destructor called\nBase Destructor called\n\
    \nSo the destruction of base pointer(which take an allocation on derived object!)\
    \ follow the destruction rule i.e first the derived then the base.\nOn the other\
    \ hand for constructor there are nothing like virtual constructor.\n"
- - What is an undefined reference/unresolved external symbol error and how do I fix
    it?
  - "\nCompiling a C++ program takes place in several steps, as specified by 2.2 (credits\
    \ to Keith Thompson for the reference):\n\nThe precedence among the syntax rules\
    \ of translation is specified by the following phases [see footnote].\n\nPhysical\
    \ source file characters are mapped, in an implementation-defined manner, to the\
    \ basic source character set\n  (introducing new-line characters for end-of-line\
    \ indicators) if\n  necessary. [SNIP]\nEach instance of a backslash character\
    \ (\\) immediately followed by a new-line character is deleted, splicing physical\
    \ source lines to\n  form logical source lines. [SNIP]\nThe source file is decomposed\
    \ into preprocessing tokens (2.5) and sequences of white-space characters (including\
    \ comments). [SNIP]\nPreprocessing directives are executed, macro invocations\
    \ are expanded, and _Pragma unary operator expressions are executed. [SNIP]\n\
    Each source character set member in a character literal or a string literal, as\
    \ well as each escape sequence and universal-character-name\n  in a character\
    \ literal or a non-raw string literal, is converted to\n  the corresponding member\
    \ of the execution character set; [SNIP]\nAdjacent string literal tokens are concatenated.\n\
    White-space characters separating tokens are no longer significant. Each preprocessing\
    \ token is converted into a token. (2.7). The\n  resulting tokens are syntactically\
    \ and semantically analyzed and\n  translated as a translation unit. [SNIP]\n\
    Translated translation units and instantiation units are combined as follows:\
    \ [SNIP]\nAll external entity references are resolved. Library components are\
    \ linked to satisfy external references to entities not defined in the\n  current\
    \ translation. All such translator output is collected into a\n  program image\
    \ which contains information needed for execution in its\n  execution environment.\
    \ (emphasis mine)\n\n[footnote] Implementations must behave as if these separate\
    \ phases occur, although in practice different phases might be folded together.\n\
    \nThe specified errors occur during this last stage of compilation, most commonly\
    \ referred to as linking. It basically means that you compiled a bunch of implementation\
    \ files into object files or libraries and now you want to get them to work together.\n\
    Say you defined symbol a in a.cpp. Now, b.cpp declared that symbol and used it.\
    \ Before linking, it simply assumes that that symbol was defined somewhere, but\
    \ it doesn't yet care where. The linking phase is responsible for finding the\
    \ symbol and correctly linking it to b.cpp (well, actually to the object or library\
    \ that uses it).\nIf you're using Microsoft Visual Studio, you'll see that projects\
    \ generate .lib files. These contain a table of exported symbols, and a table\
    \ of imported symbols. The imported symbols are resolved against the libraries\
    \ you link against, and the exported symbols are provided for the libraries that\
    \ use that .lib (if any).\nSimilar mechanisms exist for other compilers/ platforms.\n\
    Common error messages are error LNK2001, error LNK1120, error LNK2019 for Microsoft\
    \ Visual Studio and undefined reference to symbolName for GCC.\nThe code:\nstruct\
    \ X\n{\n   virtual void foo();\n};\nstruct Y : X\n{\n   void foo() {}\n};\nstruct\
    \ A\n{\n   virtual ~A() = 0;\n};\nstruct B: A\n{\n   virtual ~B(){}\n};\nextern\
    \ int x;\nvoid foo();\nint main()\n{\n   x = 0;\n   foo();\n   Y y;\n   B b;\n\
    }\n\nwill generate the following errors with GCC:\n/home/AbiSfw/ccvvuHoX.o: In\
    \ function `main':\nprog.cpp:(.text+0x10): undefined reference to `x'\nprog.cpp:(.text+0x19):\
    \ undefined reference to `foo()'\nprog.cpp:(.text+0x2d): undefined reference to\
    \ `A::~A()'\n/home/AbiSfw/ccvvuHoX.o: In function `B::~B()':\nprog.cpp:(.text._ZN1BD1Ev[B::~B()]+0xb):\
    \ undefined reference to `A::~A()'\n/home/AbiSfw/ccvvuHoX.o: In function `B::~B()':\n\
    prog.cpp:(.text._ZN1BD0Ev[B::~B()]+0x12): undefined reference to `A::~A()'\n/home/AbiSfw/ccvvuHoX.o:(.rodata._ZTI1Y[typeinfo\
    \ for Y]+0x8): undefined reference to `typeinfo for X'\n/home/AbiSfw/ccvvuHoX.o:(.rodata._ZTI1B[typeinfo\
    \ for B]+0x8): undefined reference to `typeinfo for A'\ncollect2: ld returned\
    \ 1 exit status\n\nand similar errors with Microsoft Visual Studio:\n1>test2.obj\
    \ : error LNK2001: unresolved external symbol \"void __cdecl foo(void)\" (?foo@@YAXXZ)\n\
    1>test2.obj : error LNK2001: unresolved external symbol \"int x\" (?x@@3HA)\n\
    1>test2.obj : error LNK2001: unresolved external symbol \"public: virtual __thiscall\
    \ A::~A(void)\" (??1A@@UAE@XZ)\n1>test2.obj : error LNK2001: unresolved external\
    \ symbol \"public: virtual void __thiscall X::foo(void)\" (?foo@X@@UAEXXZ)\n1>...\\\
    test2.exe : fatal error LNK1120: 4 unresolved externals\n\nCommon causes include:\n\
    \nFailure to link against appropriate libraries/object files or compile implementation\
    \ files\nDeclared and undefined variable or function.\nCommon issues with class-type\
    \ members\nTemplate implementations not visible.\nSymbols were defined in a C\
    \ program and used in C++ code.\nIncorrectly importing/exporting methods/classes\
    \ across modules/dll. (MSVS specific)\nCircular library dependency\nundefined\
    \ reference to `WinMain@16'\nInterdependent library order\nMultiple source files\
    \ of the same name\nMistyping or not including the .lib extension when using the\
    \ #pragma (Microsoft Visual Studio)\nProblems with template friends\nInconsistent\
    \ UNICODE definitions\n\n"
- - What is an undefined reference/unresolved external symbol error and how do I fix
    it?
  - "\nClass members:\nA pure virtual destructor needs an implementation.\nDeclaring\
    \ a destructor pure still requires you to define it (unlike a regular function):\n\
    struct X\n{\n    virtual ~X() = 0;\n};\nstruct Y : X\n{\n    ~Y() {}\n};\nint\
    \ main()\n{\n    Y y;\n}\n//X::~X(){} //uncomment this line for successful definition\n\
    \nThis happens because base class destructors are called when the object is destroyed\
    \ implicitly, so a definition is required.    \nvirtual methods must either be\
    \ implemented or defined as pure.\nThis is similar to non-virtual methods with\
    \ no definition, with the added reasoning that \nthe pure declaration generates\
    \ a dummy vtable and you might get the linker error without using the function:\n\
    struct X\n{\n    virtual void foo();\n};\nstruct Y : X\n{\n   void foo() {}\n\
    };\nint main()\n{\n   Y y; //linker error although there was no call to X::foo\n\
    }\n\nFor this to work, declare X::foo() as pure:\nstruct X\n{\n    virtual void\
    \ foo() = 0;\n};\n\nNon-virtual class members\nSome members need to be defined\
    \ even if not used explicitly:\nstruct A\n{ \n    ~A();\n};\n\nThe following would\
    \ yield the error:\nA a;      //destructor undefined\n\nThe implementation can\
    \ be inline, in the class definition itself:\nstruct A\n{ \n    ~A() {}\n};\n\n\
    or outside:\nA::~A() {}\n\nIf the implementation is outside the class definition,\
    \ but in a header, the methods have to be marked as inline to prevent a multiple\
    \ definition.\nAll used member methods need to be defined if used.\nA common mistake\
    \ is forgetting to qualify the name:\nstruct A\n{\n   void foo();\n};\n\nvoid\
    \ foo() {}\n\nint main()\n{\n   A a;\n   a.foo();\n}\n\nThe definition should\
    \ be\nvoid A::foo() {}\n\nstatic data members must be defined outside the class\
    \ in a single translation unit:\nstruct X\n{\n    static int x;\n};\nint main()\n\
    {\n    int x = X::x;\n}\n//int X::x; //uncomment this line to define X::x\n\n\
    An initializer can be provided for a static const data member of integral or enumeration\
    \ type within the class definition; however, odr-use of this member will still\
    \ require a namespace scope definition as described above. C++11 allows initialization\
    \ inside the class for all static const data members.\n"
- - What is an undefined reference/unresolved external symbol error and how do I fix
    it?
  - "\nFailure to link against appropriate libraries/object files or compile implementation\
    \ files\nCommonly, each translation unit will generate an object file that contains\
    \ the definitions of the symbols defined in that translation unit. \nTo use those\
    \ symbols, you have to link against those object files.\nUnder gcc you would specify\
    \ all object files that are to be linked together in the command line, or compile\
    \ the implementation files together.\ng++ -o test objectFile1.o objectFile2.o\
    \ -lLibraryName\n\nThe libraryName here is just the bare name of the library,\
    \ without platform-specific additions. So e.g. on Linux library files are usually\
    \ called libfoo.so but you'd only write -lfoo. On Windows that same file might\
    \ be called foo.lib, but you'd use the same argument. You might have to add the\
    \ directory where those files can be found using -Lâ¹directoryâº. Make sure to not\
    \ write a space after -l or -L.\nFor XCode:  Add the User Header Search Paths\
    \ -> add the Library Search Path -> drag and drop the actual library reference\
    \ into the project folder.\nUnder MSVS, files added to a project automatically\
    \ have their object files linked together and a lib file would be generated (in\
    \ common usage). To use the symbols in a separate project, you'd \nneed to include\
    \ the lib files in the project settings. This is done in the Linker section of\
    \ the project properties, in Input -> Additional Dependencies. (the path to the\
    \ lib file should be\nadded in Linker -> General -> Additional Library Directories)\
    \ When using a third-party library that is provided with a lib file, failure to\
    \ do so usually results in the error.\nIt can also happen that you forget to add\
    \ the file to the compilation, in which case the object file won't be generated.\
    \ In gcc you'd add the files to the command line. In MSVS adding the file to the\
    \ project will make it compile it automatically (albeit files can, manually, be\
    \ individually excluded from the build).\nIn Windows programming, the tell-tale\
    \ sign that you did not link a necessary library is that the name of the unresolved\
    \ symbol begins with __imp_. Look up the name of the function in the documentation,\
    \ and it should say which library you need to use. For example, MSDN puts the\
    \ information in a box at the bottom of each function in a section called \"Library\"\
    .\n"
- - What is a lambda expression in C++11?
  - "\nThe problem\nC++ includes useful generic functions like std::for_each and std::transform,\
    \ which can be very handy. Unfortunately they can also be quite cumbersome to\
    \ use, particularly if the functor you would like to apply is unique to the particular\
    \ function.\n#include <algorithm>\n#include <vector>\n\nnamespace {\n  struct\
    \ f {\n    void operator()(int) {\n      // do something\n    }\n  };\n}\n\nvoid\
    \ func(std::vector<int>& v) {\n  f f;\n  std::for_each(v.begin(), v.end(), f);\n\
    }\n\nIf you only use f once and in that specific place it seems overkill to be\
    \ writing a whole class just to do something trivial and one off.\nIn C++03 you\
    \ might be tempted to write something like the following, to keep the functor\
    \ local:\nvoid func2(std::vector<int>& v) {\n  struct {\n    void operator()(int)\
    \ {\n       // do something\n    }\n  } f;\n  std::for_each(v.begin(), v.end(),\
    \ f);\n}\n\nhowever this is not allowed, f cannot be passed to a template function\
    \ in C++03.\nThe new solution\nC++11 introduces lambdas allow you to write an\
    \ inline, anonymous functor to replace the struct f. For small simple examples\
    \ this can be cleaner to read (it keeps everything in one place) and potentially\
    \ simpler to maintain, for example in the simplest form:\nvoid func3(std::vector<int>&\
    \ v) {\n  std::for_each(v.begin(), v.end(), [](int) { /* do something here*/ });\n\
    }\n\nLambda functions are just syntactic sugar for anonymous functors.\nReturn\
    \ types\nIn simple cases the return type of the lambda is deduced for you, e.g.:\n\
    void func4(std::vector<double>& v) {\n  std::transform(v.begin(), v.end(), v.begin(),\n\
    \                 [](double d) { return d < 0.00001 ? 0 : d; }\n             \
    \    );\n}\n\nhowever when you start to write more complex lambdas you will quickly\
    \ encounter cases where the return type cannot be deduced by the compiler, e.g.:\n\
    void func4(std::vector<double>& v) {\n    std::transform(v.begin(), v.end(), v.begin(),\n\
    \        [](double d) {\n            if (d < 0.0001) {\n                return\
    \ 0;\n            } else {\n                return d;\n            }\n       \
    \ });\n}\n\nTo resolve this you are allowed to explicitly specify a return type\
    \ for a lambda function, using -> T:\nvoid func4(std::vector<double>& v) {\n \
    \   std::transform(v.begin(), v.end(), v.begin(),\n        [](double d) -> double\
    \ {\n            if (d < 0.0001) {\n                return 0;\n            } else\
    \ {\n                return d;\n            }\n        });\n}\n\n\"Capturing\"\
    \ variables\nSo far we've not used anything other than what was passed to the\
    \ lambda within it, but we can also use other variables, within the lambda. If\
    \ you want to access other variables you can use the capture clause (the [] of\
    \ the expression), which has so far been unused in these examples, e.g.:\nvoid\
    \ func5(std::vector<double>& v, const double& epsilon) {\n    std::transform(v.begin(),\
    \ v.end(), v.begin(),\n        [epsilon](double d) -> double {\n            if\
    \ (d < epsilon) {\n                return 0;\n            } else {\n         \
    \       return d;\n            }\n        });\n}\n\nYou can capture by both reference\
    \ and value, which you can specify using & and = respectively:\n\n[&epsilon] capture\
    \ by reference\n[&] captures all variables used in the lambda by reference\n[=]\
    \ captures all variables used in the lambda by value\n[&, epsilon] captures variables\
    \ like with [&], but epsilon by value\n[=, &epsilon] captures variables like with\
    \ [=], but epsilon by reference\n\nThe generated operator() is const by default,\
    \ with the implication that captures will be const when you access them by default.\
    \ This has the effect that each call with the same input would produce the same\
    \ result, however you can mark the lambda as mutable to request that the operator()\
    \ that is produced is not const.\n"
- - What is a lambda expression in C++11?
  - "\nWhat is a lambda function?\nThe C++ concept of a lambda function originates\
    \ in the lambda calculus and functional programming. A lambda is an unnamed function\
    \ that is useful (in actual programming, not theory) for short snippets of code\
    \ that are impossible to reuse and are not worth naming.\nIn C++ a lambda function\
    \ is defined like this\n[]() { } // barebone lambda\n\nor in all its glory\n[]()\
    \ mutable -> T { } // T is the return type, still lacking throw()\n\n[] is the\
    \ capture list, () the argument list and {} the function body.\nThe capture list\n\
    The capture list defines what from the outside of the lambda should be available\
    \ inside the function body and how.\nIt can be either:\n\na value: [x]\na reference\
    \ [&x]\nany variable currently in scope by reference [&]\nsame as 3, but by value\
    \ [=]\n\nYou can mix any of the above in a comma separated list [x, &y].\nThe\
    \ argument list\nThe argument list is the same as in any other C++ function. \n\
    The function body\nThe code that will be executed when the lambda is actually\
    \ called.\nReturn type deduction\nIf a lambda has only one return statement, the\
    \ return type can be omitted and has the implicit type of decltype(return_statement).\n\
    Mutable\nIf a lambda is marked mutable (e.g. []() mutable { }) it is allowed to\
    \ mutate the values that have been captured by value.\nUse cases\nThe library\
    \ defined by the ISO standard benefits heavily from lambdas and raises the usability\
    \ several bars as now users don't have to clutter their code with small functors\
    \ in some accessible scope.\nC++14\nIn C++14 lambdas have been extended by various\
    \ proposals.\nInitialized Lambda Captures\nAn element of the capture list can\
    \ now be initialized with =. This allows renaming of variables and to capture\
    \ by moving. An example taken from the standard:\nint x = 4;\nauto y = [&r = x,\
    \ x = x+1]()->int {\n            r += 2;\n            return x+2;\n         }();\
    \  // Updates ::x to 6, and initializes y to 7.\n\nand one taken from Wikipedia\
    \ showing how to capture with std::move:\nauto ptr = std::make_unique<int>(10);\
    \ // See below for std::make_unique\nauto lambda = [ptr = std::move(ptr)] {return\
    \ *ptr;};\n\nGeneric Lambdas\nLambdas can now be generic (auto would be equivalent\
    \ to T here if\nT were a type template argument somewhere in the surrounding scope):\n\
    auto lambda = [](auto x, auto y) {return x + y;};\n\nImproved Return Type Deduction\n\
    C++14 allows deduced return types for every function and does not restrict it\
    \ to functions of the form return expression;. This is also extended to lambdas.\n"
- - What is a lambda expression in C++11?
  - "\nLambda expressions are typically used to encapsulate algorithms so that they\
    \ can be passed to another function.  However, it is possible to execute a lambda\
    \ immediately upon definition:\n[&](){ ...your code... }(); // immediately executed\
    \ lambda expression\n\nis functionally equivalent to\n{ ...your code... } // simple\
    \ code block\n\nThis makes lambda expressions a powerful tool for refactoring\
    \ complex functions.  You start by wrapping a code section in a lambda function\
    \ as shown above.  The process of explicit parameterization can then be performed\
    \ gradually with intermediate testing after each step.  Once you have the code-block\
    \ fully parameterized (as demonstrated by the removal of the &), you can move\
    \ the code to an external location and make it a normal function.\nSimilarly,\
    \ you can use lambda expressions to initialize variables based on the result of\
    \ an algorithm...\nint a = []( int b ){ int r=1; while (b>0) r*=b--; return r;\
    \ }(5); // 5!\n\nAs a way of partitioning your program logic, you might even find\
    \ it useful to pass a lambda expression as an argument to another lambda expression...\n\
    [&]( std::function<void()> algorithm ) // wrapper section\n   {\n   ...your wrapper\
    \ code...\n   algorithm();\n   ...your wrapper code...\n   }\n([&]() // algorithm\
    \ section\n   {\n   ...your algorithm code...\n   });\n\nLambda expressions also\
    \ let you create named nested functions, which can be a convenient way of avoiding\
    \ duplicate logic.  Using named lambdas also tends to be a little easier on the\
    \ eyes (compared to anonymous inline lambdas) when passing a non-trivial function\
    \ as a parameter to another function.  Note: don't forget the semicolon after\
    \ the closing curly brace.\nauto algorithm = [&]( double x, double m, double b\
    \ ) -> double\n   {\n   return m*x+b;\n   };\n\nint a=algorithm(1,2,3), b=algorithm(4,5,6);\n\
    \nIf subsequent profiling reveals significant initialization overhead for the\
    \ function object, you might choose to rewrite this as a normal function.\n"
- - Replacing a 32-bit loop counter with 64-bit introduces crazy performance deviations
  - "\nCulprit: False Data Dependency (and the compiler isn't even aware of it)\n\
    On Sandy/Ivy Bridge and Haswell processors, the instruction:\npopcnt  src, dest\n\
    \nappears to have a false dependency on the destination register dest. Even though\
    \ the instruction only writes to it, the instruction will wait until dest is ready\
    \ before executing.\nThis dependency doesn't just hold up the 4 popcnts from a\
    \ single loop iteration. It can carry across loop iterations making it impossible\
    \ for the processor to parallelize different loop iterations.\nThe unsigned vs.\
    \ uint64_t and other tweaks don't directly affect the problem. But they influence\
    \ the register allocator which assigns the registers to the variables.\nIn your\
    \ case, the speeds are a direct result of what is stuck to the (false) dependency\
    \ chain depending on what the register allocator decided to do.\n\n13 GB/s has\
    \ a chain: popcnt-add-popcnt-popcnt â next iteration\n15 GB/s has a chain: popcnt-add-popcnt-add\
    \ â next iteration\n20 GB/s has a chain: popcnt-popcnt â next iteration\n26 GB/s\
    \ has a chain: popcnt-popcnt â next iteration\n\nThe difference between 20 GB/s\
    \ and 26 GB/s seems to be a minor artifact of the indirect addressing. Either\
    \ way, the processor starts to hit other bottlenecks once you reach this speed.\n\
    \nTo test this, I used inline assembly to bypass the compiler and get exactly\
    \ the assembly I want. I also split up the count variable to break all other dependencies\
    \ that might mess with the benchmarks.\nHere are the results:\nSandy Bridge Xeon\
    \ @ 3.5 GHz: (full test code can be found at the bottom)\n\nGCC 4.6.3: g++ popcnt.cpp\
    \ -std=c++0x -O3 -save-temps -march=native\nUbuntu 12\n\nDifferent Registers:\
    \ 18.6195 GB/s\n.L4:\n    movq    (%rbx,%rax,8), %r8\n    movq    8(%rbx,%rax,8),\
    \ %r9\n    movq    16(%rbx,%rax,8), %r10\n    movq    24(%rbx,%rax,8), %r11\n\
    \    addq    $4, %rax\n\n    popcnt %r8, %r8\n    add    %r8, %rdx\n    popcnt\
    \ %r9, %r9\n    add    %r9, %rcx\n    popcnt %r10, %r10\n    add    %r10, %rdi\n\
    \    popcnt %r11, %r11\n    add    %r11, %rsi\n\n    cmpq    $131072, %rax\n \
    \   jne .L4\n\nSame Register: 8.49272 GB/s\n.L9:\n    movq    (%rbx,%rdx,8), %r9\n\
    \    movq    8(%rbx,%rdx,8), %r10\n    movq    16(%rbx,%rdx,8), %r11\n    movq\
    \    24(%rbx,%rdx,8), %rbp\n    addq    $4, %rdx\n\n    # This time reuse \"rax\"\
    \ for all the popcnts.\n    popcnt %r9, %rax\n    add    %rax, %rcx\n    popcnt\
    \ %r10, %rax\n    add    %rax, %rsi\n    popcnt %r11, %rax\n    add    %rax, %r8\n\
    \    popcnt %rbp, %rax\n    add    %rax, %rdi\n\n    cmpq    $131072, %rdx\n \
    \   jne .L9\n\nSame Register with broken chain: 17.8869 GB/s\n.L14:\n    movq\
    \    (%rbx,%rdx,8), %r9\n    movq    8(%rbx,%rdx,8), %r10\n    movq    16(%rbx,%rdx,8),\
    \ %r11\n    movq    24(%rbx,%rdx,8), %rbp\n    addq    $4, %rdx\n\n    # Reuse\
    \ \"rax\" for all the popcnts.\n    xor    %rax, %rax    # Break the cross-iteration\
    \ dependency by zeroing \"rax\".\n    popcnt %r9, %rax\n    add    %rax, %rcx\n\
    \    popcnt %r10, %rax\n    add    %rax, %rsi\n    popcnt %r11, %rax\n    add\
    \    %rax, %r8\n    popcnt %rbp, %rax\n    add    %rax, %rdi\n\n    cmpq    $131072,\
    \ %rdx\n    jne .L14\n\n\nSo what went wrong with the compiler?\nIt seems that\
    \ neither GCC nor Visual Studio are aware that popcnt has such a false dependency.\
    \ Nevertheless, these false dependencies aren't uncommon. It's just a matter of\
    \ whether the compiler is aware of it.\npopcnt isn't exactly the most used instruction.\
    \ So it's not really a surprise that a major compiler could miss something like\
    \ this. There also appears to be no documentation anywhere that mentions this\
    \ problem. If Intel doesn't disclose it, then nobody outside will know until someone\
    \ runs into it by chance.\n(Update: As of version 4.9.2, GCC is aware of this\
    \ false-dependency and generates code to compensate it when optimizations are\
    \ enabled. Major compilers from other vendors, including Clang, MSVC, and even\
    \ Intel's own ICC are not yet aware of this microarchitectural erratum and will\
    \ not emit code that compensates for it.)\nWhy does the CPU have such a false\
    \ dependency?\nWe can only speculate, but it's likely that Intel has the same\
    \ handling for a lot of two-operand instructions. Common instructions like add,\
    \ sub take two operands both of which are inputs. So Intel probably shoved popcnt\
    \ into the same category to keep the processor design simple.\nAMD processors\
    \ do not appear to have this false dependency.\n\nThe full test code is below\
    \ for reference:\n#include <iostream>\n#include <chrono>\n#include <x86intrin.h>\n\
    \nint main(int argc, char* argv[]) {\n\n   using namespace std;\n   uint64_t size=1<<20;\n\
    \n   uint64_t* buffer = new uint64_t[size/8];\n   char* charbuffer=reinterpret_cast<char*>(buffer);\n\
    \   for (unsigned i=0;i<size;++i) charbuffer[i]=rand()%256;\n\n   uint64_t count,duration;\n\
    \   chrono::time_point<chrono::system_clock> startP,endP;\n   {\n      uint64_t\
    \ c0 = 0;\n      uint64_t c1 = 0;\n      uint64_t c2 = 0;\n      uint64_t c3 =\
    \ 0;\n      startP = chrono::system_clock::now();\n      for( unsigned k = 0;\
    \ k < 10000; k++){\n         for (uint64_t i=0;i<size/8;i+=4) {\n            uint64_t\
    \ r0 = buffer[i + 0];\n            uint64_t r1 = buffer[i + 1];\n            uint64_t\
    \ r2 = buffer[i + 2];\n            uint64_t r3 = buffer[i + 3];\n            __asm__(\n\
    \                \"popcnt %4, %4  \\n\\t\"\n                \"add %4, %0     \\\
    n\\t\"\n                \"popcnt %5, %5  \\n\\t\"\n                \"add %5, %1\
    \     \\n\\t\"\n                \"popcnt %6, %6  \\n\\t\"\n                \"\
    add %6, %2     \\n\\t\"\n                \"popcnt %7, %7  \\n\\t\"\n         \
    \       \"add %7, %3     \\n\\t\"\n                : \"+r\" (c0), \"+r\" (c1),\
    \ \"+r\" (c2), \"+r\" (c3)\n                : \"r\"  (r0), \"r\"  (r1), \"r\"\
    \  (r2), \"r\"  (r3)\n            );\n         }\n      }\n      count = c0 +\
    \ c1 + c2 + c3;\n      endP = chrono::system_clock::now();\n      duration=chrono::duration_cast<std::chrono::nanoseconds>(endP-startP).count();\n\
    \      cout << \"No Chain\\t\" << count << '\\t' << (duration/1.0E9) << \" sec\
    \ \\t\"\n            << (10000.0*size)/(duration) << \" GB/s\" << endl;\n   }\n\
    \   {\n      uint64_t c0 = 0;\n      uint64_t c1 = 0;\n      uint64_t c2 = 0;\n\
    \      uint64_t c3 = 0;\n      startP = chrono::system_clock::now();\n      for(\
    \ unsigned k = 0; k < 10000; k++){\n         for (uint64_t i=0;i<size/8;i+=4)\
    \ {\n            uint64_t r0 = buffer[i + 0];\n            uint64_t r1 = buffer[i\
    \ + 1];\n            uint64_t r2 = buffer[i + 2];\n            uint64_t r3 = buffer[i\
    \ + 3];\n            __asm__(\n                \"popcnt %4, %%rax   \\n\\t\"\n\
    \                \"add %%rax, %0      \\n\\t\"\n                \"popcnt %5, %%rax\
    \   \\n\\t\"\n                \"add %%rax, %1      \\n\\t\"\n                \"\
    popcnt %6, %%rax   \\n\\t\"\n                \"add %%rax, %2      \\n\\t\"\n \
    \               \"popcnt %7, %%rax   \\n\\t\"\n                \"add %%rax, %3\
    \      \\n\\t\"\n                : \"+r\" (c0), \"+r\" (c1), \"+r\" (c2), \"+r\"\
    \ (c3)\n                : \"r\"  (r0), \"r\"  (r1), \"r\"  (r2), \"r\"  (r3)\n\
    \                : \"rax\"\n            );\n         }\n      }\n      count =\
    \ c0 + c1 + c2 + c3;\n      endP = chrono::system_clock::now();\n      duration=chrono::duration_cast<std::chrono::nanoseconds>(endP-startP).count();\n\
    \      cout << \"Chain 4   \\t\"  << count << '\\t' << (duration/1.0E9) << \"\
    \ sec \\t\"\n            << (10000.0*size)/(duration) << \" GB/s\" << endl;\n\
    \   }\n   {\n      uint64_t c0 = 0;\n      uint64_t c1 = 0;\n      uint64_t c2\
    \ = 0;\n      uint64_t c3 = 0;\n      startP = chrono::system_clock::now();\n\
    \      for( unsigned k = 0; k < 10000; k++){\n         for (uint64_t i=0;i<size/8;i+=4)\
    \ {\n            uint64_t r0 = buffer[i + 0];\n            uint64_t r1 = buffer[i\
    \ + 1];\n            uint64_t r2 = buffer[i + 2];\n            uint64_t r3 = buffer[i\
    \ + 3];\n            __asm__(\n                \"xor %%rax, %%rax   \\n\\t\" \
    \  // <--- Break the chain.\n                \"popcnt %4, %%rax   \\n\\t\"\n \
    \               \"add %%rax, %0      \\n\\t\"\n                \"popcnt %5, %%rax\
    \   \\n\\t\"\n                \"add %%rax, %1      \\n\\t\"\n                \"\
    popcnt %6, %%rax   \\n\\t\"\n                \"add %%rax, %2      \\n\\t\"\n \
    \               \"popcnt %7, %%rax   \\n\\t\"\n                \"add %%rax, %3\
    \      \\n\\t\"\n                : \"+r\" (c0), \"+r\" (c1), \"+r\" (c2), \"+r\"\
    \ (c3)\n                : \"r\"  (r0), \"r\"  (r1), \"r\"  (r2), \"r\"  (r3)\n\
    \                : \"rax\"\n            );\n         }\n      }\n      count =\
    \ c0 + c1 + c2 + c3;\n      endP = chrono::system_clock::now();\n      duration=chrono::duration_cast<std::chrono::nanoseconds>(endP-startP).count();\n\
    \      cout << \"Broken Chain\\t\"  << count << '\\t' << (duration/1.0E9) << \"\
    \ sec \\t\"\n            << (10000.0*size)/(duration) << \" GB/s\" << endl;\n\
    \   }\n\n   free(charbuffer);\n}\n\n\nAn equally interesting benchmark can be\
    \ found here: http://pastebin.com/kbzgL8si\n\nThis benchmark varies the number\
    \ of popcnts that are in the (false) dependency chain.\nFalse Chain 0:  41959360000\
    \ 0.57748 sec     18.1578 GB/s\nFalse Chain 1:  41959360000 0.585398 sec    17.9122\
    \ GB/s\nFalse Chain 2:  41959360000 0.645483 sec    16.2448 GB/s\nFalse Chain\
    \ 3:  41959360000 0.929718 sec    11.2784 GB/s\nFalse Chain 4:  41959360000 1.23572\
    \ sec     8.48557 GB/s\n\n"
- - Replacing a 32-bit loop counter with 64-bit introduces crazy performance deviations
  - "\nI coded up an equivalent C program to experiment, and I can confirm this strange\
    \ behaviour. What's more, gcc believes the 64-bit integer (which should probably\
    \ be a size_t anyway...) to be better, as using uint_fast32_t causes gcc to use\
    \ a 64-bit uint.\nI did a bit of mucking around with the assembly:\nSimply take\
    \ the 32-bit version, replace all 32-bit instructions/registers with the 64-bit\
    \ version in the inner popcount-loop of the program. Observation: the code is\
    \ just as fast as the 32-bit version!\nThis is obviously a hack, as the size of\
    \ the variable isn't really 64 bit, as other parts of the program still use the\
    \ 32-bit version, but as long as the inner popcount-loop dominates performance,\
    \ this is a good start.\nI then copied the inner loop code from the 32-bit version\
    \ of the program, hacked it up to be 64 bit, fiddled with the registers to make\
    \ it a replacement for the inner loop of the 64-bit version. This code also runs\
    \ as fast as the 32-bit version.\nMy conclusion is that this is bad instruction\
    \ scheduling by the compiler, not actual speed/latency advantage of 32-bit instructions.\
    \ (Caveat: I hacked up assembly, could have broken something without noticing.\
    \ I don't think so.)\n"
- - Replacing a 32-bit loop counter with 64-bit introduces crazy performance deviations
  - "\nThis is not an answer, but it's hard to read if I put results in comment.\n\
    I get these results with a Mac Pro (Westmere 6-Cores Xeon 3.33Â GHz). I compiled\
    \ it with clang -O3 -msse4 -lstdc++ a.cpp -o a (-O2 get same result).\nclang with\
    \ uint64_t size=atol(argv[1])<<20;\nunsigned    41950110000 0.811198 sec    12.9263\
    \ GB/s\nuint64_t    41950110000 0.622884 sec    16.8342 GB/s\n\nclang with uint64_t\
    \ size=1<<20;\nunsigned    41950110000 0.623406 sec    16.8201 GB/s\nuint64_t\
    \    41950110000 0.623685 sec    16.8126 GB/s\n\nI also tried to:\n\nReverse the\
    \ test order, the result is the same so it rules out the cache factor.\nHave the\
    \ for statement in reverse: for (uint64_t i=size/8;i>0;i-=4). This gives the same\
    \ result and proves the compile is smart enough to not divide size by 8 every\
    \ iteration (as expected).\n\nHere is my wild guess:\nThe speed factor comes in\
    \ three parts:\n\ncode cache: uint64_t version has larger code size, but this\
    \ does not have an effect on my Xeon CPU. This makes the 64-bit version slower.\n\
    Instructions used. Note not only the loop count, but the buffer is accessed with\
    \ a 32-bit and 64-bit index on the two versions. Accessing a pointer with a 64-bit\
    \ offset requests a dedicated 64-bit register and addressing, while you can use\
    \ immediate for a 32-bit offset. This may make the 32-bit version faster.\nInstructions\
    \ are only emitted on the 64-bit compile (that is, prefetch). This makes 64-bit\
    \ faster.\n\nThe three factors together match with the observed seemingly conflicting\
    \ results.\n"
- - What are rvalues, lvalues, xvalues, glvalues, and prvalues?
  - "\nI guess this document might serve as a not so short introduction : n3055\n\
    The whole massacre began with the move semantics. Once we have expressions that\
    \ can be moved and not copied, suddenly easy to grasp rules demanded distinction\
    \ between expressions that can be moved, and in which direction.\nFrom what I\
    \ guess based on the draft, the r/l value distinction stays the same, only in\
    \ the context of moving things get messy. \nAre they needed? Probably not if we\
    \ wish to forfeit the new features. But to allow better optimization we should\
    \ probably embrace them.\nQuoting n3055:\n\nAn lvalue (so-called, historically,\n\
    because lvalues could appear on the\nleft-hand side  of an assignment\nexpression)\
    \ designates a function or\nan object.  [Example: If E is  an\nexpression of pointer\
    \ type, then *E\nis an lvalue expression referring to\nthe object  or function\
    \ to which E\npoints.  As another example, the\nresult of calling a function \
    \ whose\nreturn type is an lvalue reference is\nan lvalue.] \nAn xvalue (an\n\
    âeXpiringâ value) also refers to an\nobject, usually near the end of its \nlifetime\
    \ (so that its resources may\nbe moved, for example).  An xvalue is\nthe result\
    \  of certain kinds of\nexpressions involving rvalue\nreferences.  [Example: The\
    \ \nresult of calling a function whose\nreturn type is an rvalue reference is\n\
    an xvalue.]\nA glvalue   (âgeneralizedâ lvalue) is an lvalue\nor an xvalue. \n\
    An rvalue (so-called,\nhistorically, because rvalues could\nappear on the right-hand\
    \  side of an\nassignment expression) is an xvalue,\na temporary object or\nsubobject\
    \ thereof, or a value that is\nnot associated with an object. \nA\nprvalue (âpureâ\
    \ rvalue) is an rvalue\nthat is not an xvalue.  [Example: The\nresult  of calling\
    \ a function whose\nreturn type is not a reference is a\nprvalue]\n\nThe document\
    \ in question is a great reference for this question, because it shows the exact\
    \ changes in the standard that have happened as a result of the introduction of\
    \ the new nomenclature.\n"
- - What are rvalues, lvalues, xvalues, glvalues, and prvalues?
  - "\n\nWhat are these new categories of expressions?\n\nThe FCD (n3092) has an excellent\
    \ description:\n\nâ An lvalue (so called, historically, because lvalues could\
    \ appear on the\n  left-hand side of an assignment\n  expression) designates a\
    \ function or\n  an object. [ Example: If E is an\n  expression of pointer type,\
    \ then\n  *E is an lvalue expression referring to the object or function to which\
    \ E\n  points. As another example, the result\n  of calling a function whose return\n\
    \  type is an lvalue reference is an\n  lvalue. âend example ] \nâ An xvalue (an\n\
    \  âeXpiringâ value) also refers to an\n  object, usually near the end of its\n\
    \  lifetime (so that its resources may be\n  moved, for example). An xvalue is\
    \ the\n  result of certain kinds of expressions\n  involving rvalue references\
    \ (8.3.2). [\n  Example: The result of calling a\n  function whose return type\
    \ is an\n  rvalue reference is an xvalue. âend\n  example ] \nâ A glvalue (âgeneralizedâ\n\
    \  lvalue) is an lvalue or an xvalue.\nâ\n  An rvalue (so called, historically,\n\
    \  because rvalues could appear on the\n  right-hand side of an assignment\n \
    \ expressions) is an xvalue, a temporary\n  object (12.2) or subobject thereof,\
    \ or\n  a value that is not associated with an\n  object.\nâ A prvalue (âpureâ\
    \ rvalue) is\n  an rvalue that is not an xvalue. [\n  Example: The result of calling\
    \ a\n  function whose return type is not a\n  reference is a prvalue. The value\
    \ of a\n  literal such as 12, 7.3e5, or true is\n  also a prvalue. âend example\
    \ ]\nEvery\n  expression belongs to exactly one of\n  the fundamental classifications\
    \ in\n  this taxonomy: lvalue, xvalue, or\n  prvalue. This property of an\n  expression\
    \ is called its value\n  category. [ Note: The discussion of\n  each built-in\
    \ operator in Clause 5\n  indicates the category of the value it\n  yields and\
    \ the value categories of the\n  operands it expects. For example, the\n  built-in\
    \ assignment operators expect\n  that the left operand is an lvalue and\n  that\
    \ the right operand is a prvalue\n  and yield an lvalue as the result.\n  User-defined\
    \ operators are functions,\n  and the categories of values they\n  expect and\
    \ yield are determined by\n  their parameter and return types. âend\n  note\n\n\
    I suggest you read the entire section 3.10 Lvalues and rvalues though.\n\nHow\
    \ do these new categories relate to the existing rvalue and lvalue categories?\
    \ \n\nAgain: \n\n\nAre the rvalue and lvalue categories in C++0x the same as they\
    \ are in C++03?\n\nThe semantics of rvalues has evolved particularly with the\
    \ introduction of move semantics.\n\nWhy are these new categories needed?\n\n\
    So that move construction/assignment could be defined and supported.\n"
- - What are rvalues, lvalues, xvalues, glvalues, and prvalues?
  - "\nI'll start with your last question:\n\nWhy are these new categories needed?\
    \ \n\nThe C++ standard contains many rules that deal with the value category of\
    \ an expression. Some rules make a distinction between lvalue and rvalue. For\
    \ example, when it comes to overload resolution. Other rules make a distinction\
    \ between glvalue and prvalue. For example, you can have a glvalue with an incomplete\
    \ or abstract type but there is no prvalue with an incomplete or abstract type.\
    \ Before we had this terminology the rules that actually need to distinguish between\
    \ glvalue/prvalue referred to lvalue/rvalue and they were either unintentionally\
    \ wrong or contained lots of explaining and exceptions to the rule a la \"...unless\
    \ the rvalue is due to unnamed rvalue reference...\". So, it seems like a good\
    \ idea to just give the concepts of glvalues and prvalues their own name.\n\n\
    What are these new categories of expressions?\n  How do these new categories relate\
    \ to the existing rvalue and lvalue categories?\n\nWe still have the terms lvalue\
    \ and rvalue that are compatible with C++98. We just divided the rvalues into\
    \ two subgroups, xvalues and prvalues, and we refer to lvalues and xvalues as\
    \ glvalues. Xvalues are a new kind of value category for unnamed rvalue references.\
    \ Every expression is one of these three: lvalue, xvalue, prvalue. A Venn diagram\
    \ would look like this:\n    ______ ______\n   /      X      \\\n  /      / \\\
    \      \\\n |   l  | x |  pr  |\n  \\      \\ /      /\n   \\______X______/\n\
    \       gl    r\n\nExamples with functions:\nint   prvalue();\nint&  lvalue();\n\
    int&& xvalue();\n\nBut also don't forget that named rvalue references are lvalues:\n\
    void foo(int&& t) {\n  // t is initialized with an rvalue expression\n  // but\
    \ is actually an lvalue expression itself\n}\n\n"
- - What is the difference between const int*, const int * const, and int const *?
  - "\nRead it backwards (as driven by Clockwise/Spiral Rule):\n\nint* - pointer to\
    \ int\nint const * - pointer to const int\nint * const - const pointer to int\n\
    int const * const - const pointer to const int\n\nNow the first const can be on\
    \ either side of the type so:\n\nconst int * == int const *\nconst int * const\
    \ == int const * const\n\nIf you want to go really crazy you can do things like\
    \ this:\n\nint ** - pointer to pointer to int\nint ** const - a const pointer\
    \ to a pointer to an int\nint * const * - a pointer to a const pointer to an int\n\
    int const ** - a pointer to a pointer to a const int\nint * const * const - a\
    \ const pointer to a const pointer to an int\n...\n\nAnd to make sure we are clear\
    \ on the meaning of const\nconst int* foo;\nint *const bar; //note, you actually\
    \ need to set the pointer \n                //here because you can't change it\
    \ later ;)\n\nfoo is a variable pointer to a constant integer. This lets you change\
    \ what you point to but not the value that you point to. Most often this is seen\
    \ with C-style strings where you have a pointer to a const char. You may change\
    \ which string you point to but you can't change the content of these strings.\
    \ This is important when the string itself is in the data segment of a program\
    \ and shouldn't be changed.\nbar is a constant or fixed pointer to a value that\
    \ can be changed. This is like a reference without the extra syntactic sugar.\
    \ Because of this fact, usually you would use a reference where you would use\
    \ a T* const pointer unless you need to allow NULL pointers.\n"
- - What is the difference between const int*, const int * const, and int const *?
  - "\nFor those who don't know about Clockwise/Spiral Rule:\nStart from the name\
    \ of the variable, move clockwisely (in this case, move backward) to the next\
    \ pointer or type. Repeat until expression ends.\nhere is a demo:\n\n\n\n\n\n"
- - What is the difference between const int*, const int * const, and int const *?
  - "\nI think everything is answered here already, but I just want to add that you\
    \ should beware of typedefs! They're NOT just text replacements.\nFor example:\n\
    typedef char *ASTRING;\nconst ASTRING astring;\n\nThe type of astring is char\
    \ * const, not const char *. This is one reason I always tend to put const to\
    \ the right of the type, and never at the start.\n"
- - Why do we need virtual functions in C++?
  - "\nHere is how I understood not just what virtual functions are, but why they're\
    \ required:\nLet's say you have these two classes:\nclass Animal\n{\n    public:\n\
    \        void eat() { std::cout << \"I'm eating generic food.\"; }\n};\n\nclass\
    \ Cat : public Animal\n{\n    public:\n        void eat() { std::cout << \"I'm\
    \ eating a rat.\"; }\n};\n\nIn your main function:\nAnimal *animal = new Animal;\n\
    Cat *cat = new Cat;\n\nanimal->eat(); // Outputs: \"I'm eating generic food.\"\
    \ncat->eat();    // Outputs: \"I'm eating a rat.\"\n\nSo far so good, right? Animals\
    \ eat generic food, cats eat rats, all without virtual.\nLet's change it a little\
    \ now so that eat() is called via an intermediate function (a trivial function\
    \ just for this example):\n// This can go at the top of the main.cpp file\nvoid\
    \ func(Animal *xyz) { xyz->eat(); }\n\nNow our main function is:\nAnimal *animal\
    \ = new Animal;\nCat *cat = new Cat;\n\nfunc(animal); // Outputs: \"I'm eating\
    \ generic food.\"\nfunc(cat);    // Outputs: \"I'm eating generic food.\"\n\n\
    Uh oh... we passed a Cat into func(), but it won't eat rats. Should you overload\
    \ func() so it takes a Cat*? If you have to derive more animals from Animal they\
    \ would all need their own func().\nThe solution is to make eat() from the Animal\
    \ class a virtual function:\nclass Animal\n{\n    public:\n        virtual void\
    \ eat() { std::cout << \"I'm eating generic food.\"; }\n};\n\nclass Cat : public\
    \ Animal\n{\n    public:\n        void eat() { std::cout << \"I'm eating a rat.\"\
    ; }\n};\n\nMain:\nfunc(animal); // Outputs: \"I'm eating generic food.\"\nfunc(cat);\
    \    // Outputs: \"I'm eating a rat.\"\n\nDone.\n"
- - Why do we need virtual functions in C++?
  - "\nWithout \"virtual\" you get \"early binding\". Which implementation of the\
    \ method is used gets decided at compile time based on the type of the pointer\
    \ that you call through.\nWith \"virtual\" you get \"late binding\". Which implementation\
    \ of the method is used gets decided at run time based on the type of the pointed-to\
    \ object - what it was originally constructed as. This is not necessarily what\
    \ you'd think based on the type of the pointer that points to that object.\nclass\
    \ Base\n{\n  public:\n            void Method1 ()  {  std::cout << \"Base::Method1\"\
    \ << std::endl;  }\n    virtual void Method2 ()  {  std::cout << \"Base::Method2\"\
    \ << std::endl;  }\n};\n\nclass Derived : public Base\n{\n  public:\n    void\
    \ Method1 ()  {  std::cout << \"Derived::Method1\" << std::endl;  }\n    void\
    \ Method2 ()  {  std::cout << \"Derived::Method2\" << std::endl;  }\n};\n\nBase*\
    \ obj = new Derived ();\n  //  Note - constructed as Derived, but pointer stored\
    \ as Base*\n\nobj->Method1 ();  //  Prints \"Base::Method1\"\nobj->Method2 ();\
    \  //  Prints \"Derived::Method2\"\n\nEDIT - see this question.\nAlso - this tutorial\
    \ covers early and late binding in C++.\n"
- - Why do we need virtual functions in C++?
  - "\nYou need at least 1 level of inheritance and a downcast to demonstrate it.\
    \ Here is a very simple  example:\nclass Animal\n{        \n    public: \n   \
    \   // turn the following virtual modifier on/off to see what happens\n      //virtual\
    \   \n      std::string Says() { return \"?\"; }  \n};\n\nclass Dog: public Animal\n\
    {\n    public: std::string Says() { return \"Woof\"; }\n};\n\nvoid test()\n{\n\
    \    Dog* d = new Dog();\n    Animal* a = d;       // refer to Dog instance with\
    \ Animal pointer\n\n    cout << d->Says();   // always Woof\n    cout << a->Says();\
    \   // Woof or ?, depends on virtual\n}\n\n"
- - Where and why do I have to put the âtemplateâ and âtypenameâ keywords?
  - "\nIn order to parse a C++ program, the compiler needs to know whether certain\
    \ names are types or not. The following example demonstrates that:\nt * f;\n\n\
    How should this be parsed? For many languages a compiler doesn't need to know\
    \ the meaning of a name in order to parse and basically know what action a line\
    \ of code does. In C++, the above however can yield vastly different interpretations\
    \ depending on what t means. If it's a type, then it will be a declaration of\
    \ a pointer f. However if it's not a type, it will be a multiplication. So the\
    \ C++ Standard says at paragraph (3/7):\n\nSome names denote types or templates.\
    \ In general, whenever a name is encountered it is necessary to determine whether\
    \ that name denotes one of these entities before continuing to parse the program\
    \ that contains it. The process that determines this is called name lookup.\n\n\
    How will the compiler find out what a name t::x refers to, if t refers to a template\
    \ type parameter? x could be a static int data member that could be multiplied\
    \ or could equally well be a nested class or typedef that could yield to a declaration.\
    \ If a name has this property - that it can't be looked up until the actual template\
    \ arguments are known - then it's called a dependent name (it \"depends\" on the\
    \ template parameters). \nYou might recommend to just wait till the user instantiates\
    \ the template: \n\nLet's wait until the user instantiates the template, and then\
    \ later find out the real meaning of t::x * f;. \n\nThis will work and actually\
    \ is allowed by the Standard as a possible implementation approach. These compilers\
    \ basically copy the template's text into an internal buffer, and only when an\
    \ instantiation is needed, they parse the template and possibly detect errors\
    \ in the definition. But instead of bothering the template's users (poor colleagues!)\
    \ with errors made by a template's author, other implementations choose to check\
    \ templates early on and give errors in the definition as soon as possible, before\
    \ an instantiation even takes place. \nSo there has to be a way to tell the compiler\
    \ that certain names are types and that certain names aren't. \nThe \"typename\"\
    \ keyword\nThe answer is: We decide how the compiler should parse this. If t::x\
    \ is a dependent name, then we need to prefix it by typename to tell the compiler\
    \ to parse it in a certain way. The Standard says at (14.6/2):\n\nA name used\
    \ in a template declaration or definition and that is dependent on a template-parameter\
    \ is\n  assumed not to name a type unless the applicable name lookup finds a type\
    \ name or the name is qualified\n  by the keyword typename. \n\nThere are many\
    \ names for which typename is not necessary, because the compiler can, with the\
    \ applicable name lookup in the template definition, figure out how to parse a\
    \ construct itself - for example with T *f;, when T is a type template parameter.\
    \ But for t::x * f; to be a declaration, it must be written as typename t::x *f;.\
    \ If you omit the keyword and the name is taken to be a non-type, but when instantiation\
    \ finds it denotes a type, the usual error messages are emitted by the compiler.\
    \ Sometimes, the error consequently is given at definition time:\n// t::x is taken\
    \ as non-type, but as an expression the following misses an\n// operator between\
    \ the two names or a semicolon separating them.\nt::x f;\n\nThe syntax allows\
    \ typename only before qualified names - it is therefor taken as granted that\
    \ unqualified names are always known to refer to types if they do so.\nA similar\
    \ gotcha exists for names that denote templates, as hinted at by the introductory\
    \ text.\nThe \"template\" keyword\nRemember the initial quote above and how the\
    \ Standard requires special handling for templates as well? Let's take the following\
    \ innocent-looking example: \nboost::function< int() > f;\n\nIt might look obvious\
    \ to a human reader. Not so for the compiler. Imagine the following arbitrary\
    \ definition of boost::function and f:\nnamespace boost { int function = 0; }\n\
    int main() { \n  int f = 0;\n  boost::function< int() > f; \n}\n\nThat's actually\
    \ a valid expression! It uses the less-than operator to compare boost::function\
    \ against zero (int()), and then uses the greater-than operator to compare the\
    \ resulting bool against f. However as you might well know, boost::function in\
    \ real life is a template, so the compiler knows (14.2/3):\n\nAfter name lookup\
    \ (3.4) finds that a name is a template-name, if this name is followed by a <,\
    \ the < is\n  always taken as the beginning of a template-argument-list and never\
    \ as a name followed by the less-than\n  operator.\n\nNow we are back to the same\
    \ problem as with typename. What if we can't know yet whether the name is a template\
    \ when parsing the code? We will need to insert template immediately before the\
    \ template name, as specified by 14.2/4. This looks like:\nt::template f<int>();\
    \ // call a function template\n\nTemplate names can not only occur after a ::\
    \ but also after a -> or . in a class member access. You need to insert the keyword\
    \ there too:\nthis->template f<int>(); // call a function template\n\n\nDependencies\n\
    For the people that have thick Standardese books on their shelf and that want\
    \ to know what exactly I was talking about, I'll talk a bit about how this is\
    \ specified in the Standard.\nIn template declarations some constructs have different\
    \ meanings depending on what template arguments you use to instantiate the template:\
    \ Expressions may have different types or values, variables may have different\
    \ types or function calls might end up calling different functions. Such constructs\
    \ are generally said to depend on template parameters.\nThe Standard defines precisely\
    \ the rules by whether a construct is dependent or not. It separates them into\
    \ logically different groups: One catches types, another catches expressions.\
    \ Expressions may depend by their value and/or their type. So we have, with typical\
    \ examples appended:\n\nDependent types (e.g: a type template parameter T)\nValue-dependent\
    \ expressions (e.g: a non-type template parameter N)\nType-dependent expressions\
    \ (e.g: a cast to a type template parameter (T)0)\n\nMost of the rules are intuitive\
    \ and are built up recursively: For example, a type constructed as T[N] is a dependent\
    \ type if N is a value-dependent expression or T is a dependent type. The details\
    \ of this can be read in section (14.6.2/1) for dependent types, (14.6.2.2) for\
    \ type-dependent expressions and (14.6.2.3) for value-dependent expressions. \n\
    Dependent names\nThe Standard is a bit unclear about what exactly is a dependent\
    \ name. On a simple read (you know, the principle of least surprise), all it defines\
    \ as a dependent name is the special case for function names below. But since\
    \ clearly T::x also needs to be looked up in the instantiation context, it also\
    \ needs to be a dependent name (fortunately, as of mid C++14 the committee has\
    \ started to look into how to fix this confusing definition). \nTo avoid this\
    \ problem, I have resorted to a simple interpretation of the Standard text. Of\
    \ all the constructs that denote dependent types or expressions, a subset of them\
    \ represent names. Those names are therefore \"dependent names\". A name can take\
    \ different forms - the Standard says:\n\nA name is a use of an identifier (2.11),\
    \ operator-function-id (13.5), conversion-function-id (12.3.2), or template-id\
    \ (14.2) that denotes an entity or label (6.6.4, 6.1)\n\nAn identifier is just\
    \ a plain sequence of characters / digits, while the next two are the operator\
    \ + and operator type form. The last form is template-name <argument list>. All\
    \ these are names, and by conventional use in the Standard, a name can also include\
    \ qualifiers that say what namespace or class a name should be looked up in.\n\
    A value dependent expression 1 + N is not a name, but N is. The subset of all\
    \ dependent constructs that are names is called dependent name. Function names,\
    \ however, may have different meaning in different instantiations of a template,\
    \ but unfortunately are not caught by this general rule. \nDependent function\
    \ names\nNot primarily a concern of this article, but still worth mentioning:\
    \ Function names are an exception that are handled separately. An identifier function\
    \ name is dependent not by itself, but by the type dependent argument expressions\
    \ used in a call. In the example f((T)0), f is a dependent name. In the Standard,\
    \ this is specified at (14.6.2/1).\nAdditional notes and examples\nIn enough cases\
    \ we need both of typename and template. Your code should look like the following\n\
    template <typename T, typename Tail>\nstruct UnionNode : public Tail {\n    //\
    \ ...\n    template<typename U> struct inUnion {\n        typedef typename Tail::template\
    \ inUnion<U> dummy;\n    };\n    // ...\n};\n\nThe keyword template doesn't always\
    \ have to appear in the last part of a name. It can appear in the middle before\
    \ a class name that's used as a scope, like in the following example\ntypename\
    \ t::template iterator<int>::value_type v;\n\nIn some cases, the keywords are\
    \ forbidden, as detailed below\n\nOn the name of a dependent base class you are\
    \ not allowed to write typename. It's assumed that the name given is a class type\
    \ name. This is true for both names in the base-class list and the constructor\
    \ initializer list:\n template <typename T>\n struct derive_from_Has_type : /*\
    \ typename */ SomeBase<T>::type \n { };\n\nIn using-declarations it's not possible\
    \ to use template after the last ::, and the C++ committee said not to work on\
    \ a solution. \n template <typename T>\n struct derive_from_Has_type : SomeBase<T>\
    \ {\n    using SomeBase<T>::template type; // error\n    using typename SomeBase<T>::type;\
    \ // typename *is* allowed\n };\n\n\n"
- - Where and why do I have to put the âtemplateâ and âtypenameâ keywords?
  - "\nC++11\nProblem\nWhile the rules in C++03 about when you need typename and template\
    \ are largely reasonable, there is one annoying disadvantage of its formulation\n\
    template<typename T>\nstruct A {\n  typedef int result_type;\n\n  void f() {\n\
    \    // error, \"this\" is dependent, \"template\" keyword needed\n    this->g<float>();\n\
    \n    // OK\n    g<float>();\n\n    // error, \"A<T>\" is dependent, \"typename\"\
    \ keyword needed\n    A<T>::result_type n1;\n\n    // OK\n    result_type n2;\
    \ \n  }\n\n  template<typename U>\n  void g();\n};\n\nAs can be seen, we need\
    \ the disambiguation keyword even if the compiler could perfectly figure out itself\
    \ that A::result_type can only be int (and is hence a type), and this->g can only\
    \ be the member template g declared later (even if A is explicitly specialized\
    \ somewhere, that would not affect the code within that template, so its meaning\
    \ cannot be affected by a later specialization of A!). \nCurrent instantiation\n\
    To improve the situation, in C++11 the language tracks when a type refers to the\
    \ enclosing template. To know that, the type must have been formed by using a\
    \ certain form of name, which is its own name (in the above, A, A<T>, ::A<T>).\
    \ A type referenced by such a name is known to be  the current instantiation.\
    \ There may be multiple types that are all the current instantiation if the type\
    \ from which the name is formed is a member/nested class (then, A::NestedClass\
    \ and A are both current instantiations). \nBased on this notion, the language\
    \ says that CurrentInstantiation::Foo, Foo and CurrentInstantiationTyped->Foo\
    \ (such as A *a = this; a->Foo) are all member of the current instantiation if\
    \ they are found to be members of a class that is the current instantiation or\
    \ one of its non-dependent base classes (by just doing the name lookup immediately).\
    \ \nThe keywords typename and template are now not required anymore if the qualifier\
    \ is a member of the current instantiation. A keypoint here to remember is that\
    \ A<T> is still a type-dependent name (after all T is also type dependent). But\
    \ A<T>::result_type is known to be a type - the compiler will \"magically\" look\
    \ into this kind of dependent types to figure this out. \nstruct B {\n  typedef\
    \ int result_type;\n};\n\ntemplate<typename T>\nstruct C { }; // could be specialized!\n\
    \ntemplate<typename T>\nstruct D : B, C<T> {\n  void f() {\n    // OK, member\
    \ of current instantiation!\n    // A::result_type is not dependent: int\n   \
    \ D::result_type r1;\n\n    // error, not a member of the current instantiation\n\
    \    D::questionable_type r2;\n\n    // OK for now - relying on C<T> to provide\
    \ it\n    // But not a member of the current instantiation\n    typename D::questionable_type\
    \ r3;        \n  }\n};\n\nThat's impressive, but can we do better? The language\
    \ even goes further and requires that an implementation again looks up D::result_type\
    \ when instantiating D::f (even if it found its meaning already at definition\
    \ time). When now the lookup result differs or results in ambiguity, the program\
    \ is ill-formed and a diagnostic must be given. Imagine what happens if we defined\
    \ C like this\ntemplate<>\nstruct C<int> {\n  typedef bool result_type;\n  typedef\
    \ int questionable_type;\n};\n\nA compiler is required to catch the error when\
    \ instantiating D<int>::f. So you get the best of the two worlds: \"Delayed\"\
    \ lookup protecting you if you could get in trouble with dependent base classes,\
    \ and also \"Immediate\" lookup that frees you from typename and template. \n\
    Unknown specializations\nIn the code of D, the name typename D::questionable_type\
    \ is not a member of the current instantiation. Instead the language marks it\
    \ as a member of an unknown specialization. In particular, this is always the\
    \ case when you are doing DependentTypeName::Foo or DependentTypedName->Foo and\
    \ either the dependent type is not the current instantiation (in which case the\
    \ compiler can give up and say \"we will look later what Foo is) or it is the\
    \ current instantiation and the name was not found in it or its non-dependent\
    \ base classes and there are also dependent base classes. \nImagine what happens\
    \ if we had a member function h within the above defined A class template\nvoid\
    \ h() {\n  typename A<T>::questionable_type x;\n}\n\nIn C++03, the language allowed\
    \ to catch this error because there could never be a valid way to instantiate\
    \ A<T>::h (whatever argument you give to T). In C++11, the language now has a\
    \ further check to give more reason for compilers to implement this rule. Since\
    \ A has no dependent base classes, and A declares no member questionable_type,\
    \ the name A<T>::questionable_type is neither a member of the current instantiation\
    \ nor a member of an unknown specialization. In that case, there should be no\
    \ way that that code could validly compile at instantiation time, so the language\
    \ forbids a name where the qualifier is the current instantiation to be neither\
    \ a member of an unknown specialization nor a member of the current instantiation\
    \ (however, this violation is still not required to be diagnosed).\nExamples and\
    \ trivia\nYou can try this knowledge on this answer and see whether the above\
    \ definitions make sense for you on a real-world example (they are repeated slightly\
    \ less detailed in that answer).  \nThe C++11 rules make the following valid C++03\
    \ code ill-formed (which was not intended by the C++ committee, but will probably\
    \ not be fixed)\nstruct B { void f(); };\nstruct A : virtual B { void f(); };\n\
    \ntemplate<typename T>\nstruct C : virtual B, T {\n  void g() { this->f(); }\n\
    };\n\nint main() { \n  C<A> c; c.g(); \n}\n\nThis valid C++03 code would bind\
    \ this->f to A::f at instantiation time and everything is fine. C++11 however\
    \ immediately binds it to B::f and requires a double-check when instantiating,\
    \ checking whether the lookup still matches. However when instantiating C<A>::g,\
    \ the Dominance Rule applies and lookup will find A::f instead.\n"
- - Where and why do I have to put the âtemplateâ and âtypenameâ keywords?
  - "\n\nPREFACE\nThis post is meant to be an easy-to-read alternative to litb's post.\n\
    The underlying purpose is the same; an explanation to \"When?\" and \"Why?\" typename\
    \ and template must be applied.\n\n\nWhat's the purpose of typename and template?\n\
    typename and template are usable in circumstances other than when declaring a\
    \ template.\nThere are certain contexts in C++ where the compiler must explicitly\
    \ be told how to treat a name, and all these contexts have one thing in common;\
    \ they depend on at least one template-parameter.\nWe refer to such names, where\
    \ there can be an ambiguity in interpretation, as; \"dependent names\".\nThis\
    \ post will offer an explanation to the relationship between dependent-names,\
    \ and the two keywords.\n\nA SNIPPET SAYS MORE THAN 1000 WORDS\nTry to explain\
    \ what is going on in the following function-template, either to yourself, a friend,\
    \ or perhaps your cat; what is happening in the statement marked (A)?\ntemplate<class\
    \ T> void f_tmpl () { T::foo * x; /* <-- (A) */ }\n\n\nIt might not be as easy\
    \ as one thinks, more specifically the result of evaluating (A) heavily depends\
    \ on the definition of the type passed as template-parameter T.\nDifferent Ts\
    \ can drastically change the semantics involved.\nstruct X { typedef int     \
    \  foo;       }; /* (C) --> */ f_tmpl<X> ();\nstruct Y { static  int const foo\
    \ = 123; }; /* (D) --> */ f_tmpl<Y> ();\n\n\nThe two different scenarios:\n\n\
    If we instantiate the function-template with type X, as in (C), we will have a\
    \ declaration of a pointer-to int named x, but;\nif we instantiate the template\
    \ with type Y, as in (D), (A) would instead consist of an expression that calculates\
    \ the product of 123 multiplied with some already declared variable x.\n\n\n\n\
    THE RATIONALE\nThe C++ Standard cares about our safety and well-being, at least\
    \ in this case.\nTo prevent an implementation from potentially suffering from\
    \ nasty surprises, the Standard mandates that we sort out the ambiguity of a dependent-name\
    \ by explicitly stating the intent anywhere we'd like to treat the name as either\
    \ a type-name, or a template-id.\nIf nothing is stated, the dependent-name will\
    \ be considered to be either a variable, or a function.\n\n\nHOW TO HANDLE DEPENDENT\
    \ NAMES?\nIf this was a Hollywood film, dependent-names would be the disease that\
    \ spreads through body contact, instantly affects its host to make it confused.\
    \ Confusion that could, possibly, lead to an ill-formed perso-, erhm.. program.\n\
    A dependent-name is any name that directly, or indirectly, depends on a template-parameter.\n\
    \ntemplate<class T> void g_tmpl () {\n   SomeTrait<T>::type                  \
    \ foo; // (E), ill-formed\n   SomeTrait<T>::NestedTrait<int>::type bar; // (F),\
    \ ill-formed\n   foo.data<int> ();                         // (G), ill-formed\
    \    \n}\n\nWe have four dependent names in the above snippet:\n\nE)\n\n\n\"type\"\
    \ depends on the instantiation of SomeTrait<T>, which include T, and;\n\nF)\n\n\
    \n\"NestedTrait\", which is a template-id, depends on SomeTrait<T>, and;\n\"type\"\
    \ at the end of (F) depends on NestedTrait, which depends on SomeTrait<T>, and;\n\
    \nG)\n\n\n\"data\", which looks like a member-function template, is indirectly\
    \ a dependent-name since the type of foo depends on the instantiation of SomeTrait<T>.\n\
    \n\nNeither of statement (E), (F) or (G) is valid if the compiler would interpret\
    \ the dependent-names as variables/functions (which as stated earlier is what\
    \ happens if we don't explicitly say otherwise).\n\nTHE SOLUTION\nTo make g_tmpl\
    \ have a valid definition we must explicitly tell the compiler that we expect\
    \ a type in (E), a template-id and a type in (F), and a template-id in (G).\n\
    template<class T> void g_tmpl () {\n   typename SomeTrait<T>::type foo;      \
    \                      // (G), legal\n   typename SomeTrait<T>::template NestedTrait<int>::type\
    \ bar; // (H), legal\n   foo.template data<int> ();                          \
    \        // (I), legal\n}\n\nEvery time a name denotes a type, all names involved\
    \ must be either type-names or namespaces, with this in mind it's quite easy to\
    \ see that we apply typename at the beginning of our fully qualified name.\ntemplate\
    \ however, is different in this regard, since there's no way of coming to a conclusion\
    \ such as; \"oh, this is a template, than this other thing must also be a template\"\
    . This means that we apply template directly in front of any name that we'd like\
    \ to treat as such.\n\n\nCAN I JUST STICK THE KEYWORDS IN FRONT OF ANY NAME?\n\
    \n\"Can I just stick typename and template in front of any name? I don't want\
    \ to worry about the context in which they appear...\" - Some C++ Developer\n\n\
    The rules in the Standard states that you may apply the keywords as long as you\
    \ are dealing with a qualified-name (K), but if the name isn't qualified the application\
    \ is ill-formed (L).\nnamespace N {\n  template<class T>\n  struct X { };\n}\n\
    \n\n         N::         X<int> a; // ...  legal\ntypename N::template X<int>\
    \ b; // (K), legal\ntypename template    X<int> c; // (L), ill-formed\n\nNote:\
    \ Applying typename or template in a context where it is not required is not considered\
    \ good practice; just because you can do something, doesn't mean that you should.\n\
    \nAdditionally there are contexts where typename and template are explicitly disallowed:\n\
    \nWhen specifying the bases of which a class inherits\nEvery name written in a\
    \ derived class's base-specifier-list is already treated as a type-name, explicitly\
    \ specifying typename is both ill-formed, and redundant.\n                   //\
    \ .------- the base-specifier-list\n template<class T> // v\n struct Derived \
    \     : typename SomeTrait<T>::type /* <- ill-formed */ {\n   ...\n };\n\n\nWhen\
    \ the template-id is the one being referred to in a derived class's using-directive\n\
    \ struct Base {\n   template<class T>\n   struct type { };\n };\n\n struct Derived\
    \ : Base {\n   using Base::template type; // ill-formed\n   using Base::type;\
    \          // legal\n };\n\n\n"
- - Undefined behavior and sequence points
  - "\nC++98 and C++03\nThis answer is for the older versions of the C++ standard.\
    \  The C++11 and C++14 versions of the standard do not formally contain 'sequence\
    \ points'; operations are 'sequenced before' or 'unsequenced' or 'indeterminately\
    \ sequenced' instead.  The net effect is essentially the same, but the terminology\
    \ is different.\n\nDisclaimer : Okay. This answer is a bit long. So have patience\
    \ while reading it. If you already know these things, reading them again won't\
    \ make you crazy. \nPre-requisites : An elementary knowledge of C++ Standard \n\
    \nWhat are Sequence Points?\nThe Standard says \n\nAt  certain specified points\
    \ in the execution sequence called sequence points, all side effects of previous\
    \ evaluations \n  shall be complete and no side effects of subsequent evaluations\
    \ shall have taken place. (Â§1.9/7)\n\nSide effects? What are side effects?\nEvaluation\
    \  of  an  expression produces something and if in addition there is a change\
    \ in the state of the execution environment it is said that the expression (its\
    \ evaluation) has some side effect(s).\nFor example:\nint x = y++; //where y is\
    \ also an int\n\nIn addition to the initialization operation the value of y gets\
    \ changed due to the side effect of ++ operator. \nSo far so good. Moving on to\
    \ sequence points. An alternation definition of seq-points given by the comp.lang.c\
    \ author Steve Summit:\n\nSequence point is a point in time at which the dust\
    \ has settled and all side effects which have been seen so far are guaranteed\
    \ to be complete.\n\n\nWhat are the common sequence points listed in the C++ Standard\
    \ ?\nThose are:\n\nat the end of the evaluation of full expression (Â§1.9/16) (A\
    \ full-expression is an expression that is not a subexpression of another expression.)1\n\
    \nExample :\nint a = 5; // ; is a sequence point here\n\n\nin the evaluation of\
    \ each of the following expressions after the evaluation of the first expression(Â§1.9/18)\
    \ 2\n\na && b (Â§5.14) \na || b (Â§5.15)\na ? b : c (Â§5.16)\na , b (Â§5.18) (here\
    \ a , b is a comma operator; in func(a,a++) , is not a comma operator, it's merely\
    \ a separator between the arguments a and a++. Thus the behaviour is undefined\
    \ in that case (if a is considered to be a primitive type)) \n\nat a function\
    \ call (whether or not the function is inline), after the evaluation of all function\
    \ arguments (if any) which \ntakes place before execution of any expressions or\
    \ statements in the function body (Â§1.9/17).\n\n1 : Note : the evaluation of a\
    \ full-expression can include the evaluation of subexpressions that are not lexically\n\
    part of the full-expression.  For example, subexpressions involved in evaluating\
    \ default argument expressions (8.3.6) are considered to be created in the expression\
    \ that calls the function, not the expression that defines the default argument\n\
    2 : The operators indicated are the built-in operators, as described in clause\
    \ 5.  When one of these operators is overloaded (clause 13) in a valid context,\
    \ thus designating a user-defined operator function, the expression designates\
    \ a function invocation and the operands form an argument list, without an implied\
    \ sequence point between them.\n\nWhat is Undefined Behaviour?\nThe Standard defines\
    \ Undefined Behaviour in Section Â§1.3.12 as\n\nbehaviour, such as might arise\
    \ upon use of an erroneous program construct or erroneous data, for which this\
    \ International Standard imposes no  requirements 3.\nUndefined  behaviour  may\
    \  also  be  expected  when  this\n  International Standard omits the description\
    \ of any explicit definition of behavior.\n\n 3 : permissible undefined behavior\
    \ ranges from ignoring the situation completely with unpredictable results, to\
    \ behaving during translation or program execution in a documented manner characteristic\
    \ of the environment (with or with-\nout the issuance of a diagnostic message),\
    \ to terminating a translation or execution (with the issuance of a diagnostic\
    \ message).\nIn short, undefined behaviour means anything can happen from daemons\
    \ flying out of your nose to  your girlfriend getting pregnant.\n\nWhat is the\
    \ relation between Undefined Behaviour and Sequence Points?\nBefore I get into\
    \ that you must know the difference(s) between Undefined Behaviour, Unspecified\
    \ Behaviour and Implementation Defined Behaviour.\nYou must also know that the\
    \ order of evaluation of operands of individual operators and subexpressions of\
    \ individual expressions, and the order in which side effects take place, is unspecified.\n\
    For example:\nint x = 5, y = 6;\n\nint z = x++ + y++; //it is unspecified whether\
    \ x++ or y++ will be evaluated first.\n\nAnother example here.\n\nNow the Standard\
    \ in Â§5/4 says\n\n1) Between the previous and next sequence point a scalar object\
    \ shall have its stored value modified at most once by the evaluation of an expression.\
    \ \n\nWhat does it mean?\nInformally it means that between two sequence points\
    \ a variable must not be modified more than once.\nIn an expression statement,\
    \ the next sequence point is usually at the terminating semicolon, and the previous\
    \ sequence point is at the end of the previous statement. An expression may also\
    \ contain intermediate sequence points.\nFrom the above sentence the following\
    \ expressions invoke Undefined Behaviour:\ni++ * ++i;   // UB, i is modified more\
    \ than once btw two SPs\ni = ++i;     // UB, same as above\n++i = 2;     // UB,\
    \ same as above\ni = ++i + 1; // UB, same as above\n++++++i;     // UB, parsed\
    \ as (++(++(++i)))\n\ni = (i, ++i, ++i); // UB, there's no SP between `++i` (right\
    \ most) and assignment to `i` (`i` is modified more than once btw two SPs)\n\n\
    But the following expressions are fine:\ni = (i, ++i, 1) + 1; // well defined\
    \ (AFAIK)\ni = (++i, i++, i);   // well defined \nint j = i;\nj = (++i, i++, j*i);\
    \ // well defined\n\n\n\n2) Furthermore, the prior value shall be accessed only\
    \ to determine the value to be stored.\n\nWhat does it mean? It means if an object\
    \ is written to within a full expression, any and all accesses to it within the\
    \ same expression must be directly involved in the computation of the value to\
    \ be written. \nFor example in i = i + 1 all the access of i (in L.H.S and in\
    \ R.H.S) are directly involved in computation of the value to be written. So it\
    \ is fine.\nThis rule effectively constrains legal expressions to those in which\
    \ the accesses demonstrably precede the modification.\nExample 1:\nstd::printf(\"\
    %d %d\", i,++i); // invokes Undefined Behaviour because of Rule no 2\n\nExample\
    \ 2:\na[i] = i++ // or a[++i] = i or a[i++] = ++i etc\n\nis disallowed because\
    \ one of the accesses of i (the one in a[i]) has nothing to do with the value\
    \ which ends up being stored in i (which happens over in i++), and so there's\
    \ no good way to define--either for our understanding or the compiler's--whether\
    \ the access should take place before or after the incremented value is stored.\
    \ So the behaviour is undefined.\nExample 3 :\nint x = i + i++ ;// Similar to\
    \ above\n\n\nFollow up answer for C++11 here. \n"
- - Undefined behavior and sequence points
  - "\nThis is a follow up to my previous answer and contains C++11 related material..\n\
    \nPre-requisites : An elementary knowledge of Relations (Mathematics).\n\nIs it\
    \ true that there are no Sequence Points in C++11?\nYes! This is very true. \n\
    Sequence Points have been replaced by Sequenced Before and Sequenced After (and\
    \ Unsequenced and Indeterminately Sequenced) relations in C++11.\n\nWhat exactly\
    \ is this 'Sequenced before' thing?\nSequenced Before(Â§1.9/13) is a relation which\
    \ is: \n\nAsymmetric \nTransitive\n\nbetween evaluations executed by a single\
    \ thread and induces a strict partial order1\nFormally it means given any two\
    \ evaluations(See below) A and B, if A is sequenced before B, then the execution\
    \ of A shall precede the execution of B. If A is not sequenced before B and B\
    \ is not sequenced before A, then A and B are unsequenced 2.\nEvaluations A and\
    \ B are indeterminately sequenced when either A is sequenced before B or B is\
    \ sequenced before A, but it is unspecified which3.\n[NOTES]\n\n  1 :  A strict\
    \ partial order is a binary relation \"<\" over a set P which is asymmetric, and\
    \ transitive, i.e., for all a, b, and c in P, we have that: \n  ........(i). if\
    \ a < b then Â¬ (b < a) (asymmetry);\n  ........(ii). if a < b and b < c then a\
    \ < c (transitivity).\n  2 : The execution of unsequenced evaluations can overlap.\n\
    \  3 : Indeterminately sequenced evaluations cannot overlap, but either could\
    \ be executed first.\n\n\n What is the meaning of the word 'evaluation' in context\
    \ of C++11? \nIn C++11, evaluation of an expression (or a sub-expression) in general\
    \ includes: \n\nvalue computations (including determining the identity of an object\
    \ for glvalue evaluation and fetching a value previously assigned to an object\
    \ for prvalue evaluation) and\ninitiation of side effects.\n\nNow  (Â§1.9/14) says:\n\
    \nEvery value computation and side effect associated with a full-expression is\
    \ sequenced before every value computation and side effect associated with the\
    \ next full-expression to be evaluated.\n\n\nTrivial example: \nint x;\nx = 10;\n\
    ++x;\nValue computation and side effect associated with ++x is sequenced after\
    \ the value computation and side effect of x = 10; \n\n\nSo there must be some\
    \ relation between Undefined Behaviour and the above-mentioned things, right?\
    \ \nYes! Right.\nIn (Â§1.9/15) it has been mentioned that \n\nExcept where noted,\
    \ evaluations of operands of individual operators and of subexpressions of individual\
    \ expressions are unsequenced4.\n\nFor example :\nint main()\n{\n     int num\
    \ = 19 ;\n     num = (num << 3) + (num >> 3);\n} \n\n\nEvaluation of operands\
    \ of + operator are unsequenced relative to each other.\nEvaluation of operands\
    \ of << and >> operators are unsequenced relative to each other.\n\n 4: In an\
    \ expression that is evaluated more than once during the execution\nof a program,\
    \ unsequenced and indeterminately sequenced evaluations of its subexpressions\
    \ need not be performed consistently in different evaluations. \n\n(Â§1.9/15)\n\
    \  The value computations of the operands of an\n  operator are sequenced before\
    \ the value computation of the result of the operator.\n\nThat means in x + y\
    \ the value computation of x and y are sequenced before the value computation\
    \ of (x + y).\nMore importantly\n\n(Â§1.9/15) If a side effect on a scalar object\
    \ is unsequenced relative to either\n(a) another side effect on the same scalar\
    \ object \nor \n(b) a value computation using the value of the same scalar object.\n\
    the behaviour is undefined.\n\nExamples:\nint i = 5, v[10] = { };\nvoid  f(int,\
    \  int);\n\n\ni = i++ * ++i;  // Undefined Behaviour\ni = ++i + i++;  // Undefined\
    \ Behaviour \ni = ++i + ++i;  // Undefined Behaviour\ni = v[i++];     // Undefined\
    \ Behaviour\ni = v[++i]:     // Well-defined Behavior\ni = i++ + 1;    // Undefined\
    \ Behaviour\ni = ++i + 1;    // Well-defined Behaviour\n++++i;          // Well-defined\
    \ Behaviour\nf(i = -1, i = -1); // Undefined Behaviour (see below)\n\n\nWhen calling\
    \ a function (whether or not the function is inline), every value computation\
    \ and side effect associated with any argument expression, or with the postfix\
    \ expression designating the called function, is sequenced before execution of\
    \ every expression or statement in the body of the called function. [Note: Value\
    \ computations and side effects associated with different argument expressions\
    \ are unsequenced.  â end note]\n\nExpressions (5), (7) and (8) do not invoke\
    \ undefined behaviour. Check out the following answers for a more detailed explanation.\n\
    \nMultiple preincrement operations on a variable in C++0x\nUnsequenced Value Computations\n\
    \n\nFinal Note : \nIf you find any flaw in the post please leave a comment. Power-users\
    \ (With rep >20000) please do not hesitate to edit the post for correcting typos\
    \ and other mistakes.\n"
- - Undefined behavior and sequence points
  - "\nC++17 (N4659) includes a proposal Refining Expression Evaluation Order for\
    \ Idiomatic C++\nwhich defines a stricter order of expression evaluation.\nIn\
    \ particular, the following sentence was added:\n\n8.18 Assignment and compound\
    \ assignment operators:....\nIn all cases, the assignment is sequenced after the\
    \ value\n  computation of the right and left operands, and before the value computation\
    \ of the assignment expression.\n  The right operand is sequenced before the left\
    \ operand.\n\nIt makes several cases of previously undefined behavior valid, including\
    \ the one in question:\na[++i] = i;\n\nHowever several other similar cases still\
    \ lead to undefined behavior.\nIn N4140:\ni = i++ + 1; // the behavior is undefined\n\
    \nBut in N4659\ni = i++ + 1; // the value of i is incremented\ni = i++ + i; //\
    \ the behavior is undefined\n\nOf course, using a C++17 compliant compiler does\
    \ not necessarily mean that one should start writing such expressions.\n"
- - Do the parentheses after the type name make a difference with new?
  - "\nLet's get pedantic, because there are differences that can actually affect\
    \ your code's behavior. Much of the following is taken from comments made to an\
    \ \"Old New Thing\" article.\nSometimes the memory returned by the new operator\
    \ will be initialized, and sometimes it won't depending on whether the type you're\
    \ newing up is a POD (plain old data), or if it's a class that contains POD members\
    \ and is using a compiler-generated default constructor.\n\nIn C++1998 there are\
    \ 2 types of initialization: zero and default\nIn C++2003 a 3rd type of initialization,\
    \ value initialization was added.\n\nAssume:\nstruct A { int m; }; // POD\nstruct\
    \ B { ~B(); int m; }; // non-POD, compiler generated default ctor\nstruct C {\
    \ C() : m() {}; ~C(); int m; }; // non-POD, default-initialising m\n\nIn a C++98\
    \ compiler, the following should occur:\n\nnew A   - indeterminate value\nnew\
    \ A() - zero-initialize\nnew B   - default construct (B::m is uninitialized)\n\
    new B() - default construct (B::m is uninitialized)\nnew C   - default construct\
    \ (C::m is zero-initialized)\nnew C() - default construct (C::m is zero-initialized)\n\
    \nIn a C++03 conformant compiler, things should work like so:\n\nnew A    - indeterminate\
    \ value\nnew A()  - value-initialize A, which is zero-initialization since it's\
    \ a POD.\nnew B    - default-initializes (leaves B::m uninitialized)\nnew B()\
    \  - value-initializes B which zero-initializes all fields since its default ctor\
    \ is compiler generated as opposed to user-defined.\nnew C    - default-initializes\
    \ C, which calls the default ctor.\nnew C()  - value-initializes C, which calls\
    \ the default ctor.\n\nSo in all versions of C++ there's a difference between\
    \ new A and new A() because A is a POD.\nAnd there's a difference in behavior\
    \ between C++98 and C++03 for the case new B().\nThis is one of the dusty corners\
    \ of C++ that can drive you crazy. When constructing an object, sometimes you\
    \ want/need the parens, sometimes you absolutely cannot have them, and sometimes\
    \ it doesn't matter.\n"
- - Do the parentheses after the type name make a difference with new?
  - "\nnew Thing(); is explicit that you want a constructor called whereas new Thing;\
    \ is taken to imply you don't mind if the constructor isn't called.\nIf used on\
    \ a struct/class with a user-defined constructor, there is no difference. If called\
    \ on a trivial struct/class (e.g. struct Thing { int i; };) then new Thing; is\
    \ like malloc(sizeof(Thing)); whereas new Thing(); is like calloc(sizeof(Thing));\
    \ - it gets zero initialized.\nThe gotcha lies in-between:\nstruct Thingy {\n\
    \  ~Thingy(); // No-longer a trivial class\n  virtual WaxOn();\n  int i;\n};\n\
    \nThe behavior of new Thingy; vs new Thingy(); in this case changed between C++98\
    \ and C++2003. See Michael Burr's explanation for how and why.\n"
- - Do the parentheses after the type name make a difference with new?
  - "\nNo, they are the same. But there is a difference between:\nTest t;      //\
    \ create a Test called t\n\nand\nTest t();   // declare a function called t which\
    \ returns a Test\n\nThis is because of the basic C++ (and C) rule: If something\
    \ can possibly be a declaration, then it is a declaration.\nEdit: Re the initialisation\
    \ issues regarding POD and non-POD data, while I agree with everything that has\
    \ been said, I would just like to point out that these issues only apply if the\
    \ thing being new'd or otherwise constructed does not have a user-defined constructor.\
    \ If there is such a constructor it will be used. For 99.99% of sensibly designed\
    \ classes there will be such a constructor, and so the issues can be ignored.\
    \  \n"
- - Can a local variable's memory be accessed outside its scope?
  - "\n\nHow can it be? Isn't the memory of a local variable inaccessible outside\
    \ its function?\n\nYou rent a hotel room. You put a book in the top drawer of\
    \ the bedside table and go to sleep.  You check out the next morning, but \"forget\"\
    \ to give back your key. You steal the key!\nA week later, you return to the hotel,\
    \ do not check in, sneak into your old room with your stolen key, and look in\
    \ the drawer. Your book is still there. Astonishing!\nHow can that be? Aren't\
    \ the contents of a hotel room drawer inaccessible if you haven't rented the room?\n\
    Well, obviously that scenario can happen in the real world no problem. There is\
    \ no mysterious force that causes your book to disappear when you are no longer\
    \ authorized to be in the room. Nor is there a mysterious force that prevents\
    \ you from entering a room with a stolen key.\nThe hotel management is not required\
    \ to remove your book. You didn't make a contract with them that said that if\
    \ you leave stuff behind, they'll shred it for you. If you illegally re-enter\
    \ your room with a stolen key to get it back, the hotel security staff is not\
    \ required to catch you sneaking in. You didn't make a contract with them that\
    \ said \"if I try to sneak back into my room later, you are required to stop me.\"\
    \ Rather, you signed a contract with them that said \"I promise not to sneak back\
    \ into my room later\", a contract which you broke.\nIn this situation anything\
    \ can happen. The book can be there -- you got lucky. Someone else's book can\
    \ be there and yours could be in the hotel's furnace. Someone could be there right\
    \ when you come in, tearing your book to pieces. The hotel could have removed\
    \ the table and book entirely and replaced it with a wardrobe. The entire hotel\
    \ could be just about to be torn down and replaced with a football stadium, and\
    \ you are going to die in an explosion while you are sneaking around. \nYou don't\
    \ know what is going to happen; when you checked out of the hotel and stole a\
    \ key to illegally use later, you gave up the right to live in a predictable,\
    \ safe world because you chose to break the rules of the system.\nC++ is not a\
    \ safe language. It will cheerfully allow you to break the rules of the system.\
    \ If you try to do something illegal and foolish like going back into a room you're\
    \ not authorized to be in and rummaging through a desk that might not even be\
    \ there anymore, C++ is not going to stop you. Safer languages than C++ solve\
    \ this problem by restricting your power -- by having much stricter control over\
    \ keys, for example.\nUPDATE\nHoly goodness, this answer is getting a lot of attention.\
    \ (I'm not sure why -- I considered it to be just a \"fun\" little analogy, but\
    \ whatever.)\nI thought it might be germane to update this a bit with a few more\
    \ technical thoughts.\nCompilers are in the business of generating code which\
    \ manages the storage of the data manipulated by that program. There are lots\
    \ of different ways of generating code to manage memory, but over time two basic\
    \ techniques have become entrenched. \nThe first is to have some sort of \"long\
    \ lived\" storage area where the \"lifetime\" of each byte in the storage -- that\
    \ is, the period of time when it is validly associated with some program variable\
    \ -- cannot be easily predicted ahead of time. The compiler generates calls into\
    \ a \"heap manager\" that knows how to dynamically allocate storage when it is\
    \ needed and reclaim it when it is no longer needed.\nThe second method is to\
    \ have a âshort-livedâ storage area where the lifetime of each byte is well known.\
    \ Here, the lifetimes follow a ânestingâ pattern. The longest-lived of these short-lived\
    \ variables will be allocated before any other short-lived variables, and will\
    \ be freed last. Shorter-lived variables will be allocated after the longest-lived\
    \ ones, and will be freed before them. The lifetime of these shorter-lived variables\
    \ is ânestedâ within the lifetime of longer-lived ones.\nLocal variables follow\
    \ the latter pattern; when a method is entered, its local variables come alive.\
    \ When that method calls another method, the new method's local variables come\
    \ alive. They'll be dead before the first method's local variables are dead. \
    \ The relative order of the beginnings and endings of lifetimes of storages associated\
    \ with local variables can be worked out ahead of time.\nFor this reason, local\
    \ variables are usually generated as storage on a \"stack\" data structure, because\
    \ a stack has the property that the first thing pushed on it is going to be the\
    \ last thing popped off. \nIt's like the hotel decides to only rent out rooms\
    \ sequentially, and you can't check out until everyone with a room number higher\
    \ than you has checked out. \nSo let's think about the stack. In many operating\
    \ systems you get one stack per thread and the stack is allocated to be a certain\
    \ fixed size. When you call a method, stuff is pushed onto the stack. If you then\
    \ pass a pointer to the stack back out of your method, as the original poster\
    \ does here, that's just a pointer to the middle of some entirely valid million-byte\
    \ memory block. In our analogy, you check out of the hotel; when you do, you just\
    \ checked out of the highest-numbered occupied room.  If no one else checks in\
    \ after you, and you go back to your room illegally, all your stuff is guaranteed\
    \ to still be there in this particular hotel.\nWe use stacks for temporary stores\
    \ because they are really cheap and easy. An implementation of C++ is not required\
    \ to use a stack for storage of locals; it could use the heap. It doesn't, because\
    \ that would make the program slower. \nAn implementation of C++ is not required\
    \ to leave the garbage you left on the stack untouched so that you can come back\
    \ for it later illegally; it is perfectly legal for the compiler to generate code\
    \ that turns back to zero everything in the \"room\" that you just vacated. It\
    \ doesn't because again, that would be expensive.\nAn implementation of C++ is\
    \ not required to ensure that when the stack logically shrinks, the addresses\
    \ that used to be valid are still mapped into memory. The implementation is allowed\
    \ to tell the operating system \"we're done using this page of stack now. Until\
    \ I say otherwise, issue an exception that destroys the process if anyone touches\
    \ the previously-valid stack page\".  Again, implementations do not actually do\
    \ that because it is slow and unnecessary.\nInstead, implementations let you make\
    \ mistakes and get away with it. Most of the time. Until one day something truly\
    \ awful goes wrong and the process explodes.\nThis is problematic. There are a\
    \ lot of rules and it is very easy to break them accidentally. I certainly have\
    \ many times. And worse, the problem often only surfaces when memory is detected\
    \ to be corrupt billions of nanoseconds after the corruption happened, when it\
    \ is very hard to figure out who messed it up.\nMore memory-safe languages solve\
    \ this problem by restricting your power. In \"normal\" C# there simply is no\
    \ way to take the address of a local and return it or store it for later. You\
    \ can take the address of a local, but the language is cleverly designed so that\
    \ it is impossible to use it after the lifetime of the local ends. In order to\
    \ take the address of a local and pass it back, you have to put the compiler in\
    \ a special \"unsafe\" mode, and put the word \"unsafe\" in your program, to call\
    \ attention to the fact that you are probably doing something dangerous that could\
    \ be breaking the rules. \nFor further reading:\n\nWhat if C# did allow returning\
    \ references? Coincidentally that is the subject of today's blog post:\nhttp://blogs.msdn.com/b/ericlippert/archive/2011/06/23/ref-returns-and-ref-locals.aspx\n\
    Why do we use stacks to manage memory? Are value types in C# always stored on\
    \ the stack? How does virtual memory work? And many more topics in how the C#\
    \ memory manager works. Many of these articles are also germane to C++ programmers:\n\
    https://blogs.msdn.microsoft.com/ericlippert/tag/memory-management/\n\n"
- - Can a local variable's memory be accessed outside its scope?
  - "\nWhat you're doing here is simply reading and writing to memory that used to\
    \ be the address of a. Now that you're outside of foo, it's just a pointer to\
    \ some random memory area. It just so happens that in your example, that memory\
    \ area does exist and nothing else is using it at the moment. You don't break\
    \ anything by continuing to use it, and nothing else has overwritten it yet. Therefore,\
    \ the 5 is still there. In a real program, that memory would be re-used almost\
    \ immediately and you'd break something by doing this (though the symptoms may\
    \ not appear until much later!)\nWhen you return from foo, you tell the OS that\
    \ you're no longer using that memory and it can be reassigned to something else.\
    \ If you're lucky and it never does get reassigned, and the OS doesn't catch you\
    \ using it again, then you'll get away with the lie. Chances are though you'll\
    \ end up writing over whatever else ends up with that address.\nNow if you're\
    \ wondering why the compiler doesn't complain, it's probably because foo got eliminated\
    \ by optimization. It usually will warn you about this sort of thing. C assumes\
    \ you know what you're doing though, and technically you haven't violated scope\
    \ here (there's no reference to a itself outside of foo), only memory access rules,\
    \ which only triggers a warning rather than an error.\nIn short: this won't usually\
    \ work, but sometimes will by chance.\n"
- - Can a local variable's memory be accessed outside its scope?
  - "\nBecause the storage space wasn't stomped on just yet. Don't count on that behavior.\n"
- - What are the new features in C++17?
  - "\nLanguage features:\nTemplates and Generic Code\n\nTemplate argument deduction\
    \ for class templates\n\nLike how functions deduce template arguments, now constructors\
    \ can deduce the template arguments of the class\nhttp://wg21.link/p0433r2 http://wg21.link/p0620r0\
    \ http://wg21.link/p0512r0\n\ntemplate <auto>\n\nRepresents a value of any (non-type\
    \ template argument) type.\n\nNon-type template arguments fixes\ntemplate<template<class...>typename\
    \ bob> struct foo {}\n( Folding + ... + expressions )  and Revisions\nauto x{8};\
    \ is an int\nmodernizing using with ... and lists\n\nLambda\n\nconstexpr lambdas\n\
    \nLambdas are implicitly constexpr if they qualify\n\nCapturing *this in lambdas\n\
    \n[*this]{ std::cout << could << \" be \" << useful << '\\n'; }\n\n\nAttributes\n\
    \n[[fallthrough]], [[nodiscard]], [[maybe_unused]] attributes \n[[attributes]]\
    \ on namespaces and enum { erator[[s]] }\nusing in attributes to avoid having\
    \ to repeat an attribute namespace.\nCompilers are now required to ignore non-standard\
    \ attributes they don't recognize.\n\nThe C++14 wording allowed compilers to reject\
    \ unknown scoped attributes.\n\n\nSyntax cleanup\n\nInline variables\n\nLike inline\
    \ functions\nCompiler picks where the instance is instantiated\nDeprecate static\
    \ constexpr redeclaration, now implicitly inline.\n\nnamespace A::B\nSimple static_assert(expression);\
    \ with no string\nno throw unless throw(), and throw() is noexcept(true).\n\n\
    Cleaner multi-return and flow control\n\nStructured bindings\n\nBasically, first-class\
    \ std::tie with auto\nExample:\n\n\nconst auto [it, inserted] = map.insert( {\"\
    foo\", bar} );\nCreates variables it and inserted with deduced type from the pair\
    \ that map::insert returns.\n\nWorks with tuple/pair-likes & std::arrays and relatively\
    \ flat structs\nActually named structured bindings in standard\n\nif (init; condition)\
    \ and switch (init; condition)\n\nif (const auto [it, inserted] = map.insert(\
    \ {\"foo\", bar} ); inserted)\nExtends the if(decl) to cases where decl isn't\
    \ convertible-to-bool sensibly.\n\nGeneralizing range-based for loops\n\nAppears\
    \ to be mostly support for sentinels, or end iterators that are not the same type\
    \ as begin iterators, which helps with null-terminated loops and the like.\n\n\
    if constexpr\n\nMuch requested feature to simplify almost-generic code.\n\n\n\
    Misc\n\nHexadecimal float point literals\nDynamic memory allocation for over-aligned\
    \ data\nGuaranteed copy elision\n\nFinally!\nNot in all cases, but distinguishes\
    \ syntax where you are \"just creating something\" that was called elision, from\
    \ \"genuine elision\".\n\nFixed order-of-evaluation for (some) expressions with\
    \ some modifications\n\nNot including function arguments, but function argument\
    \ evaluation interleaving now banned\nMakes a bunch of broken code work mostly,\
    \ and makes .then on future work.\n\nDirect list-initialization of enums\nForward\
    \ progress guarantees (FPG) (also, FPGs for parallel algorithms)\n\nI think this\
    \ is saying \"the implementation may not stall threads forever\"?\n\nu8'U', u8'T',\
    \ u8'F', u8'8' character literals (string already existed)\n\"noexcept\" in the\
    \ type system\n__has_include\n\nTest if a header file include would be an error\n\
    makes migrating from experimental to std almost seamless\n\nArrays of pointer\
    \ conversion fixes\ninherited constructors fixes to some corner cases (see P0136R0\
    \ for examples of behavior changes)\naggregate initialization with inheritance.\n\
    std::launder, type punning, etc\n\nLibrary additions:\nData types\n\nstd::variant<Ts...>\n\
    \nAlmost-always non-empty last I checked?\nTagged union type\n{awesome|useful}\n\
    \nstd::optional\n\nMaybe holds one of something\nRidiculously useful\n\nstd::any\n\
    \nHolds one of anything (that is copyable)\n\nstd::string_view\n\nstd::string\
    \ like reference-to-character-array or substring\nNever take a string const& again.\
    \  Also can make parsing a bajillion times faster.\n\"hello world\"sv\nconstexpr\
    \ char_traits\n\nstd::byte off more than they could chew.\n\nNeither an integer\
    \ nor a character, just data\n\n\nInvoke stuff\n\nstd::invoke\n\nCall any callable\
    \ (function pointer, function, member pointer) with one syntax.  From the standard\
    \ INVOKE concept.\n\nstd::apply\n\nTakes a function-like and a tuple, and unpacks\
    \ the tuple into the call.\n\nstd::make_from_tuple, std::apply applied to object\
    \ construction\nis_invocable, is_invocable_r, invoke_result\n\nhttp://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0077r2.html\n\
    http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0604r0.html\nDeprecates\
    \ result_of\nis_invocable<Foo(Args...), R> is \"can you call Foo with Args...\
    \ and get something compatible with R\", where R=void is default.\ninvoke_result<Foo,\
    \ Args...> is std::result_of_t<Foo(Args...)> but apparently less confusing?\n\n\
    \nFile System TS v1\n\n[class.path]\n[class.filesystem.error]\n[class.file_status]\n\
    [class.directory_entry]\n[class.directory_iterator] and [class.recursive_directory_iterator]\n\
    [fs.ops.funcs]\nfstreams can be opened with paths, as well as with const path::value_type*\
    \ strings.\n\nNew algorithms\n\nfor_each_n\nreduce\ntransform_reduce\nexclusive_scan\n\
    inclusive_scan\ntransform_exclusive_scan\ntransform_inclusive_scan\nAdded for\
    \ threading purposes, exposed even if you aren't using them threaded\n\nThreading\n\
    \nstd::shared_mutex\n\nUntimed, which can be more efficient if you don't need\
    \ it.\n\natomic<T>::is_always_lockfree\nscoped_lock<Mutexes...>\n\nSaves some\
    \ std::lock pain when locking more than one mutex at a time.\n\nParallelism TS\
    \ v1\n\nThe linked paper from 2014, may be out of date\nParallel versions of std\
    \ algorithms, and related machinery\n\nhardware_*_interference_size\n\n(parts\
    \ of) Library Fundamentals TS v1 not covered above or below\n\n[func.searchers]\
    \ and [alg.search]\n\nA searching algorithm and techniques\n\n[pmr]\n\nPolymorphic\
    \ allocator, like std::function for allocators\nAnd some standard memory resources\
    \ to go with it.\nhttp://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0358r1.html\n\
    \nstd::sample, sampling from a range?\n\nContainer Improvements\n\ntry_emplace\
    \ and insert_or_assign\n\ngives better guarantees in some cases where spurious\
    \ move/copy would be bad\n\nSplicing for map<>, unordered_map<>, set<>, and unordered_set<>\n\
    \nMove nodes between containers cheaply.\nMerge whole containers cheaply.\n\n\
    non-const .data() for string.\nnon-member std::size, std::empty, std::data\n\n\
    like std::begin/end\n\nMinimal incomplete type support in containers\nContiguous\
    \ iterator \"concept\"\nconstexpr iterators\nThe emplace family of functions now\
    \ returns a reference to the created object.\n\nSmart pointer changes\n\nunique_ptr<T[]>\
    \ fixes and other unique_ptr tweaks.\nweak_from_this and some fixed to shared\
    \ from this\n\nOther std datatype improvements:\n\n{} construction of std::tuple\
    \ and other improvements\nTriviallyCopyable reference_wrapper, can be performance\
    \ boost\n\nMisc\n\nC++17 library is based on C11 instead of C99\nReserved std[0-9]+\
    \ for future standard libraries\ndestroy(_at|_n), uninitialized_move(_n), uninitialized_value_construct(_n),\
    \ uninitialized_default_construct(_n)\n\nutility code already in most std implementations\
    \ exposed\n\nSpecial math functions\n\nscientists may like them\n\nstd::clamp()\n\
    \nstd::clamp( a, b, c ) == std::max( b, std::min( a, c ) ) roughly\n\ngcd and\
    \ lcm\nstd::uncaught_exceptions\n\nRequired if you want to only throw if safe\
    \ from destructors\n\nstd::as_const\nstd::bool_constant\nA whole bunch of _v template\
    \ variables\nstd::void_t<T>\n\nSurprisingly useful when writing templates\n\n\
    std::owner_less<void>\n\nlike std::less<void>, but for smart pointers to sort\
    \ based on contents\n\nstd::chrono polish\nstd::conjunction, std::disjunction,\
    \ std::negation exposed\nstd::not_fn\n\nhttp://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0358r1.html\n\
    \nRules for noexcept within std\nstd::is_contiguous_layout, useful for efficient\
    \ hashing\nstd::to_chars/std::from_chars, high performance, locale agnostic number\
    \ conversion; finally a way to serialize/deserialize to human readable formats\
    \ (JSON & co) \nstd::default_order, indirection over std::less. (breaks ABI of\
    \ some compilers due to name mangling, removed.)\n\nTraits\n\nswap\nis_aggregate\n\
    has_unique_object_representations\n\nDeprecated\n\nSome C libraries, \n<codecvt>\n\
    memory_order_consume\nresult_of, replaced with invoke_result\nshared_ptr::unique,\
    \ it isn't very threadsafe\n\nIsocpp.org has has an independent list of changes\
    \ since C++14; it has been partly pillaged.\nNaturally TS work continues in parallel,\
    \ so there are some TS that are not-quite-ripe that will have to wait for the\
    \ next iteration.  The target for the next iteration is C++20 as previously planned,\
    \ not C++19 as some rumors implied.  C++1O has been avoided.\nInitial list taken\
    \ from this reddit post and this reddit post, with links added via googling or\
    \ from the above isocpp.org page.\nAdditional entries pillaged from SD-6 feature-test\
    \ list.\nclang's feature list and library feature list are next to be pillaged.\
    \  This doesn't seem to be reliable, as it is C++1z, not C++17.\nthese slides\
    \ had some features missing elsewhere.\nWhile \"what was removed\" was not asked,\
    \ here is a short list of a few things ((mostly?) previous deprecated) that are\
    \ removed in C++17 from C++:\nRemoved:\n\nregister, keyword reserved for future\
    \ use\nbool b; ++b;\ntrigraphs\n\nif you still need them, they are now part of\
    \ your source file encoding, not part of language\n\nios aliases\nauto_ptr, old\
    \ <functional> stuff, random_shuffle\nallocators in std::function\n\nThere were\
    \ rewordings.  I am unsure if these have any impact on code, or if they are just\
    \ cleanups in the standard:\nPapers not yet integrated into above:\n\nP0505R0\
    \ (constexpr chrono)\nP0418R2 (atomic tweaks)\nP0512R0 (template argument deduction\
    \ tweaks)\nP0490R0 (structured binding tweaks)\nP0513R0 (changes to std::hash)\n\
    P0502R0 (parallel exceptions)\nP0509R1 (updating restrictions on exception handling)\n\
    P0012R1 (make exception specifications be part of the type system)\nP0510R0 (restrictions\
    \ on variants)\nP0504R0 (tags for optional/variant/any)\nP0497R0 (shared ptr tweaks)\n\
    P0508R0 (structured bindings node handles)\nP0521R0 (shared pointer use count\
    \ and unique changes?)\n\nSpec changes:\n\nexception specs and throw expressions\n\
    \nFurther reference:\n\npapers grouped by year; not all accepted\nhttps://isocpp.org/files/papers/p0636r0.html\n\
    \nShould be updated to \"Modifications to existing features\" here.\n\n\n"
- - Difference between private, public, and protected inheritance
  - "\nTo answer that question, I'd like to describe member's accessors first in my\
    \ own words. If you already know this, skip to the heading \"next:\".\nThere are\
    \ three accessors that I'm aware of: public, protected and private. \nLet:\nclass\
    \ Base {\n    public:\n        int publicMember;\n    protected:\n        int\
    \ protectedMember;\n    private:\n        int privateMember;\n};\n\n\nEverything\
    \ that is aware of Base is also aware that Base contains publicMember.\nOnly the\
    \ children (and their children) are aware that Base contains protectedMember.\n\
    No one but Base is aware of privateMember.\n\nBy \"is aware of\", I mean \"acknowledge\
    \ the existence of, and thus be able to access\".\nnext:\nThe same happens with\
    \ public, private and protected inheritance. Let's consider a class Base and a\
    \ class Child that inherits from Base.\n\nIf the inheritance is public, everything\
    \ that is aware of Base and Child is also aware that Child inherits from Base.\n\
    If the inheritance is protected, only Child, and its children, are aware that\
    \ they inherit from Base.\nIf the inheritance is private, no one other than Child\
    \ is aware of the inheritance.\n\n"
- - Difference between private, public, and protected inheritance
  - "\nclass A \n{\npublic:\n    int x;\nprotected:\n    int y;\nprivate:\n    int\
    \ z;\n};\n\nclass B : public A\n{\n    // x is public\n    // y is protected\n\
    \    // z is not accessible from B\n};\n\nclass C : protected A\n{\n    // x is\
    \ protected\n    // y is protected\n    // z is not accessible from C\n};\n\n\
    class D : private A    // 'private' is default for classes\n{\n    // x is private\n\
    \    // y is private\n    // z is not accessible from D\n};\n\nIMPORTANT NOTE:\
    \ Classes B, C and D all contain the variables x, y and z. It is just question\
    \ of access.\nAbout usage of protected and private inheritance you could read\
    \ here.\n"
- - Difference between private, public, and protected inheritance
  - "\nLimiting the visibility of inheritance will make code not able to see that\
    \ some class inherits another class: Implicit conversions from the derived to\
    \ the base won't work, and static_cast from the base to the derived won't work\
    \ either. \nOnly members/friends of a class can see private inheritance, and only\
    \ members/friends and derived classes can see protected inheritance. \npublic\
    \ inheritance\n\nIS-A inheritance. A button is-a window, and anywhere where a\
    \ window is needed, a button can be passed too. \nclass button : public window\
    \ { };\n\n\nprotected inheritance\n\nProtected implemented-in-terms-of. Rarely\
    \ useful. Used in boost::compressed_pair to derive from empty classes and save\
    \ memory using empty base class optimization (example below doesn't use template\
    \ to keep being at the point):\nstruct empty_pair_impl : protected empty_class_1\
    \ \n{ non_empty_class_2 second; };\n\nstruct pair : private empty_pair_impl {\n\
    \  non_empty_class_2 &second() {\n    return this->second;\n  }\n\n  empty_class_1\
    \ &first() {\n    return *this; // notice we return *this!\n  }\n};\n\n\nprivate\
    \ inheritance\n\nImplemented-in-terms-of. The usage of the base class is only\
    \ for implementing the derived class. Useful with traits and if size matters (empty\
    \ traits that only contain functions will make use of the empty base class optimization).\
    \ Often containment is the better solution, though. The size for strings is critical,\
    \ so it's an often seen usage here\ntemplate<typename StorageModel>\nstruct string\
    \ : private StorageModel {\npublic:\n  void realloc() {\n    // uses inherited\
    \ function\n    StorageModel::realloc();\n  }\n};\n\n\n\npublic member\n\nAggregate\n\
    class pair {\npublic:\n  First first;\n  Second second;\n};\n\nAccessors\nclass\
    \ window {\npublic:\n    int getWidth() const;\n};\n\n\nprotected member\n\nProviding\
    \ enhanced access for derived classes\nclass stack {\nprotected:\n  vector<element>\
    \ c;\n};\n\nclass window {\nprotected:\n  void registerClass(window_descriptor\
    \ w);\n};\n\n\nprivate member\n\nKeep implementation details\nclass window {\n\
    private:\n  int width;\n};\n\n\n\nNote that C-style casts purposely allows casting\
    \ a derived class to a protected or private base class in a defined and safe manner\
    \ and to cast into the other direction too. This should be avoided at all costs,\
    \ because it can make code dependent on implementation details - but if necessary,\
    \ you can make use of this technique. \n"
- - What are the rules about using an underscore in a C++ identifier?
  - "\nThe rules (which did not change in C++11):\n\nReserved in any scope, including\
    \ for use as implementation macros:\n\n\nidentifiers beginning with an underscore\
    \ followed immediately by an uppercase letter\nidentifiers containing adjacent\
    \ underscores (or \"double underscore\")\n\nReserved in the global namespace:\n\
    \n\nidentifiers beginning with an underscore\n\nAlso, everything in the std namespace\
    \ is reserved. (You are allowed to add template specializations, though.) \n\n\
    From the 2003 C++ Standard:\n\n17.4.3.1.2 Global names [lib.global.names]\nCertain\
    \ sets of names and function signatures are always reserved to the implementation:\n\
    \nEach name that contains a double underscore (__) or begins with an underscore\
    \ followed by an uppercase letter (2.11) is reserved to the implementation for\
    \ any use.\nEach name that begins with an underscore is reserved to the implementation\
    \ for use as a name in the global namespace.165\n\n165) Such names are also reserved\
    \ in namespace ::std (17.4.3.1). \n\nBecause C++ is based on the C standard (1.1/2,\
    \ C++03) and C99 is a normative reference (1.2/1, C++03) these also apply, from\
    \ the 1999 C Standard:\n\n7.1.3 Reserved identifiers\nEach header declares or\
    \ defines all identifiers listed in its associated subclause, and\n  optionally\
    \ declares or defines identifiers listed in its associated future library directions\
    \ subclause and identifiers which are always reserved either for any use or for\
    \ use as file scope identifiers.\n\nAll identifiers that begin with an underscore\
    \ and either an uppercase letter or another\n  underscore are always reserved\
    \ for any use.\nAll identifiers that begin with an underscore are always reserved\
    \ for use as identifiers\n  with file scope in both the ordinary and tag name\
    \ spaces.\nEach macro name in any of the following subclauses (including the future\
    \ library\n  directions) is reserved for use as specified if any of its associated\
    \ headers is included;\n  unless explicitly stated otherwise (see 7.1.4).\nAll\
    \ identifiers with external linkage in any of the following subclauses (including\
    \ the\n  future library directions) are always reserved for use as identifiers\
    \ with external\n  linkage.154\nEach identifier with file scope listed in any\
    \ of the following subclauses (including the\n  future library directions) is\
    \ reserved for use as a macro name and as an identifier with\n  file scope in\
    \ the same name space if any of its associated headers is included.\n\nNo other\
    \ identifiers are reserved. If the program declares or defines an identifier in\
    \ a\n  context in which it is reserved (other than as allowed by 7.1.4), or defines\
    \ a reserved\n  identifier as a macro name, the behavior is undefined.\nIf the\
    \ program removes (with #undef) any macro definition of an identifier in the first\n\
    \  group listed above, the behavior is undefined.\n154) The list of reserved identifiers\
    \ with external linkage includes errno, math_errhandling, setjmp, and va_end.\n\
    \nOther restrictions might apply. For example, the POSIX standard reserves a lot\
    \ of identifiers that are likely to show up in normal code:\n\nNames beginning\
    \ with a capital E followed a digit or uppercase letter:\n\n\nmay be used for\
    \ additional error code names.\n\nNames that begin with either is or to followed\
    \ by a lowercase letter\n\n\nmay be used for additional character testing and\
    \ conversion functions.\n\nNames that begin with LC_ followed by an uppercase\
    \ letter\n\n\nmay be used for additional macros specifying locale attributes.\n\
    \nNames of all existing mathematics functions suffixed with f or l are reserved\n\
    \n\nfor corresponding functions that operate on float and long double arguments,\
    \ respectively.\n\nNames that begin with SIG followed by an uppercase letter are\
    \ reserved\n\n\nfor additional signal names.\n\nNames that begin with SIG_ followed\
    \ by an uppercase letter are reserved\n\n\nfor additional signal actions.\n\n\
    Names beginning with str, mem, or wcs followed by a lowercase letter are reserved\n\
    \n\nfor additional string and array functions.\n\nNames beginning with PRI or\
    \ SCN followed by any lowercase letter or X are reserved\n\n\nfor additional format\
    \ specifier macros\n\nNames that end with _t are reserved\n\n\nfor additional\
    \ type names.\n\n\nWhile using these names for your own purposes right now might\
    \ not cause a problem, they do raise the possibility of conflict with future versions\
    \ of that standard.\n\nPersonally I just don't start identifiers with underscores.\
    \ New addition to my rule: Don't use double underscores anywhere, which is easy\
    \ as I rarely use underscore.\nAfter doing research on this article I no longer\
    \ end my identifiers with _t\nas this is reserved by the POSIX standard.\nThe\
    \ rule about any identifier ending with _t surprised me a lot. I think that is\
    \ a POSIX standard (not sure yet) looking for clarification and official chapter\
    \ and verse. This is from the GNU libtool manual, listing reserved names.\nCesarB\
    \ provided the following link to the POSIX 2004 reserved symbols and notes 'that\
    \ many other reserved prefixes and suffixes ... can be found there'.  The\nPOSIX\
    \ 2008 reserved symbols are defined here.  The restrictions are somewhat more\
    \ nuanced than those above.\n"
- - What are the rules about using an underscore in a C++ identifier?
  - "\nThe rules to avoid collision of names are both in the C++ standard (see Stroustrup\
    \ book) and mentioned by C++ gurus (Sutter, etc.).\nPersonal rule\nBecause I did\
    \ not want to deal with cases, and wanted a simple rule, I have designed a personal\
    \ one that is both simple and correct:\n\nWhen naming a symbol, you will avoid\
    \ collision with compiler/OS/standard libraries if you:\n\nnever start a symbol\
    \ with an underscore\nnever name a symbol with two consecutive underscores inside.\n"
- - What are the rules about using an underscore in a C++ identifier?
  - "\nFrom MSDN:\n\nUse of two sequential underscore characters ( __ ) at the beginning\
    \ of an identifier, or a single leading underscore followed by a capital letter,\
    \ is reserved for C++ implementations in all scopes. You should avoid using one\
    \ leading underscore followed by a lowercase letter for names with file scope\
    \ because of possible conflicts with current or future reserved identifiers.\n\
    \nThis means that you can use a single underscore as a member variable prefix,\
    \ as long as it's followed by a lower-case letter.\nThis is apparently taken from\
    \ section 17.4.3.1.2 of the C++ standard, but I can't find an original source\
    \ for the full standard online.\nSee also this question.\n"
- - Why can't variables be declared in a switch statement?
  - "\nCase statements are only labels. This means the compiler will interpret this\
    \ as a jump directly to the label. In C++, the problem here is one of scope. Your\
    \ curly brackets define the scope as everything inside the switch statement. This\
    \ means that you are left with a scope where a jump will be performed further\
    \ into the code skipping the initialization. The correct way to handle this is\
    \ to define a scope specific to that case statement and define your variable within\
    \ it. \nswitch (val)\n{   \ncase VAL:  \n{\n  // This will work\n  int newVal\
    \ = 42;  \n  break;\n}\ncase ANOTHER_VAL:  \n...\nbreak;\n}\n\n"
- - Why can't variables be declared in a switch statement?
  - "\nThis question is was originally tagged as [C] and [C++] at the same time. The\
    \ original code is indeed invalid in both C and C++, but for completely different\
    \ unrelated reasons. I believe this important detail was missed (or obfuscated)\
    \ by the existing answers.\n\nIn C++ this code is invalid because the case ANOTHER_VAL:\
    \ label jumps into the scope of variable newVal bypassing its initialization.\
    \ Jumps that bypass initialization of local objects are illegal in C++. This side\
    \ of the issue is correctly addressed by most answers.\nHowever, in C language\
    \ bypassing variable initialization is not an error. Jumping into the scope of\
    \ a variable over its initialization is legal in C. It simply means that the variable\
    \ is left uninitialized. The original code does not compile in C for a completely\
    \ different reason. Label case VAL: in the original code is attached to the declaration\
    \ of variable newVal. In C language declarations are not statements. They cannot\
    \ be labeled. And this is what causes the error when this code is interpreted\
    \ as C code.\nswitch (val)  \n{  \ncase VAL:             /* <- C error is here\
    \ */\n  int newVal = 42;  \n  break;\ncase ANOTHER_VAL:     /* <- C++ error is\
    \ here */\n  ...\n  break;\n}\n\n\nAdding an extra {} block fixes both C++ and\
    \ C problems, even though these problems happen to be very different. On the C++\
    \ side it restricts the scope of newVal, making sure that case ANOTHER_VAL: no\
    \ longer jumps into that scope, which eliminates the C++ issue. On the C side\
    \ that extra {} introduces a compound statement, thus making the case VAL: label\
    \ to apply to a statement, which eliminates the C issue.\n\nIn C case the problem\
    \ can be easily solved without the {}. Just add an empty statement after the case\
    \ VAL: label and the code will become valid\nswitch (val)  \n{  \ncase VAL:; \
    \           /* Now it works in C! */\n  int newVal = 42;  \n  break;\ncase ANOTHER_VAL:\
    \  \n  ...\n  break;\n}\n\nNote that even though it is now valid from C point\
    \ of view, it remains invalid from C++ point of view.\nSymmetrically, in C++ case\
    \ the the problem can be easily solved without the {}. Just remove the initializer\
    \ from variable declaration and the code will become valid\nswitch (val)  \n{\
    \  \ncase VAL: \n  int newVal;\n  newVal = 42;  \n  break;\ncase ANOTHER_VAL:\
    \     /* Now it works in C++! */\n  ...\n  break;\n}\n\nNote that even though\
    \ it is now valid from C++ point of view, it remains invalid from C point of view.\n\
    \n"
- - Why can't variables be declared in a switch statement?
  - "\nOk.  Just to clarify this strictly has nothing to do with the declaration.\
    \  It relates only to \"jumping over the initialization\" (ISO C++ '03 6.7/3)\n\
    A lot of the posts here have mentioned that jumping over the declaration may result\
    \ in the variable \"not being declared\".  This is not true.  An POD object can\
    \ be declared without an initializer but it will have an indeterminate value.\
    \   For example:\nswitch (i)\n{\n   case 0:\n     int j; // 'j' has indeterminate\
    \ value\n     j = 0; // 'j' initialized to 0, but this statement\n           \
    \ // is jumped when 'i == 1'\n     break;\n   case 1:\n     ++j;   // 'j' is in\
    \ scope here - but it has an indeterminate value\n     break;\n}\n\nWhere the\
    \ object is a non-POD or aggregate the compiler implicitly adds an initializer,\
    \ and so it is not possible to jump over such a declaration:\nclass A {\npublic:\n\
    \  A ();\n};\n\nswitch (i)  // Error - jumping over initialization of 'A'\n{\n\
    \   case 0:\n     A j;   // Compiler implicitly calls default constructor\n  \
    \   break;\n   case 1:\n     break;\n}\n\nThis limitation is not limited to the\
    \ switch statement.  It is also an error to use 'goto' to jump over an initialization:\n\
    goto LABEL;    // Error jumping over initialization\nint j = 0; \nLABEL:\n  ;\n\
    \nA bit of trivia is that this is a difference between C++ and C.  In C, it is\
    \ not an error to jump over the initialization.\nAs others have mentioned, the\
    \ solution is to add a nested block so that the lifetime of the variable is limited\
    \ to the individual case label.\n"
- - What are POD types in C++?
  - "\nPOD stands for Plain Old Data - that is, a class (whether defined with the\
    \ keyword struct or the keyword class) without constructors, destructors and virtual\
    \ members functions. Wikipedia's article on POD goes into a bit more detail and\
    \ defines it as:\n\nA Plain Old Data Structure in C++ is an aggregate class that\
    \ contains only PODS as members, has no user-defined destructor, no user-defined\
    \ copy assignment operator, and no nonstatic members of pointer-to-member type.\n\
    \nGreater detail can be found in this answer for C++98/03. C++11 changed the rules\
    \ surrounding POD, relaxing them greatly, thus necessitating a follow-up answer\
    \ here.\n"
- - What are POD types in C++?
  - "\nVery informally:\nA POD is a type (including classes) where the C++ compiler\
    \ guarantees that there will be no \"magic\" going on in the structure: for example\
    \ hidden pointers to vtables, offsets that get applied to the address when it\
    \ is cast to other types (at least if the target's POD too), constructors, or\
    \ destructors. Roughly speaking, a type is a POD when the only things in it are\
    \ built-in types and combinations of them. The result is something that \"acts\
    \ like\" a C type.\nLess informally:\n\nint, char, wchar_t, bool, float, double\
    \ are PODs, as are long/short and signed/unsigned versions of them.\npointers\
    \ (including pointer-to-function and pointer-to-member) are PODs, \nenums are\
    \ PODs\na const or volatile POD is a POD.\na class, struct or union of PODs is\
    \ a POD provided that all non-static data members are public, and it has no base\
    \ class and no constructors, destructors, or virtual methods. Static members don't\
    \ stop something being a POD under this rule. This rule has changed in C++11 and\
    \ certain private members are allowed: Can a class with all private members be\
    \ a POD class?\nWikipedia is wrong to say that a POD cannot have members of type\
    \ pointer-to-member. Or rather, it's correct for the C++98 wording, but TC1 made\
    \ explicit that pointers-to-member are POD.\n\nFormally (C++03 Standard):\n\n\
    3.9(10): \"Arithmetic types (3.9.1), enumeration types, pointer types, and pointer\
    \ to member types (3.9.2) and cv-qualified versions of these types (3.9.3) are\
    \ collectively caller scalar types. Scalar types, POD-struct types, POD-union\
    \ types (clause 9), arrays of such types and cv-qualified versions of these types\
    \ (3.9.3) are collectively called POD types\"\n9(4): \"A POD-struct is an aggregate\
    \ class that has no non-static data members of type non-POD-struct, non-POD-union\
    \ (or array of such types) or reference, and has no user-define copy operator\
    \ and no user-defined destructor. Similarly a POD-union is an aggregate union\
    \ that has no non-static data members of type non-POD-struct, non-POD-union (or\
    \ array of such types) or reference, and has no user-define copy operator and\
    \ no user-defined destructor.\n8.5.1(1): \"An aggregate is an array or class (clause\
    \ 9) with no user-declared constructors (12.1), no private or protected non-static\
    \ data members (clause 11), no base classes (clause 10) and no virtual functions\
    \ (10.3).\"\n\n"
- - What are POD types in C++?
  - "\nPlain Old Data \nIn short, it is all built-in data types (e.g. int, char, float,\
    \ long, unsigned char, double, etc.) and all aggregation of POD data. Yes, it's\
    \ a recursive definition. ;)\nTo be more clear, a POD is what we call \"a struct\"\
    : a unit or a group of units that just store data.\n"
- - How to convert a std::string to const char* or char*?
  - "\nIf you just want to pass a std::string to a function that needs const char*\
    \ you can use \nstd::string str;\nconst char * c = str.c_str();\n\nIf you want\
    \ to get a writable copy, like char *, you can do that with this:\nstd::string\
    \ str;\nchar * writable = new char[str.size() + 1];\nstd::copy(str.begin(), str.end(),\
    \ writable);\nwritable[str.size()] = '\\0'; // don't forget the terminating 0\n\
    \n// don't forget to free the string after finished using it\ndelete[] writable;\n\
    \nEdit: Notice that the above is not exception safe. If anything between the new\
    \ call and the delete call throws, you will leak memory, as nothing will call\
    \ delete for you automatically. There are two immediate ways to solve this.\n\
    boost::scoped_array\nboost::scoped_array will delete the memory for you upon going\
    \ out of scope:\nstd::string str;\nboost::scoped_array<char> writable(new char[str.size()\
    \ + 1]);\nstd::copy(str.begin(), str.end(), writable.get());\nwritable[str.size()]\
    \ = '\\0'; // don't forget the terminating 0\n\n// get the char* using writable.get()\n\
    \n// memory is automatically freed if the smart pointer goes \n// out of scope\n\
    \nstd::vector\nThis is the standard way (does not require any external library).\
    \ You use std::vector, which completely manages the memory for you.\nstd::string\
    \ str;\nstd::vector<char> writable(str.begin(), str.end());\nwritable.push_back('\\\
    0');\n\n// get the char* using &writable[0] or &*writable.begin()\n\n"
- - How to convert a std::string to const char* or char*?
  - "\nGiven say...\nstd::string x = \"hello\";\n\n Getting a `char *` or `const char*`\
    \ from a `string`\nHow to get a character pointer that's valid while x remains\
    \ in scope and isn't modified further\nC++11 simplifies things; the following\
    \ all give access to the same internal string buffer:\nconst char* p_c_str = x.c_str();\n\
    const char* p_data  = x.data();\nconst char* p_x0    = &x[0];\n\n      char* p_x0_rw\
    \ = &x[0];  // compiles iff x is not const...\n\nAll the above pointers will hold\
    \ the same value - the address of the first character in the buffer.  Even an\
    \ empty string has a \"first character in the buffer\", because C++11 guarantees\
    \ to always keep an extra NUL/0 terminator character after the explicitly assigned\
    \ string content (e.g. std::string(\"this\\0that\", 9) will have a buffer holding\
    \ \"this\\0that\\0\").\nGiven any of the above pointers:\nchar c = p[n];   //\
    \ valid for n <= x.size()\n                 // i.e. you can safely read the NUL\
    \ at p[x.size()]\n\nOnly for the non-const pointer from &x[0]:\np_x0_rw[n] = c;\
    \  // valid for n <= x.size() - 1\n                 // i.e. don't overwrite the\
    \ implementation maintained NUL\n\nWriting a NUL elsewhere in the string does\
    \ not change the string's size(); string's are allowed to contain any number of\
    \ NULs - they are given no special treatment by std::string (same in C++03).\n\
    In C++03, things were considerably more complicated (key differences highlighted):\n\
    \nx.data()\n\nreturns const char* to the string's internal buffer which wasn't\
    \ required by the Standard to conclude with a NUL (i.e. might be ['h', 'e', 'l',\
    \ 'l', 'o'] followed by uninitialised or garbage values, with accidental accesses\
    \ thereto having undefined behaviour).\n\n\nx.size() characters are safe to read,\
    \ i.e. x[0] through x[x.size() - 1]\nfor empty strings, you're guaranteed some\
    \ non-NULL pointer to which 0 can be safely added (hurray!), but you shouldn't\
    \ dereference that pointer.\n\n\n&x[0]\n\nfor empty strings this has undefined\
    \ behaviour (21.3.4)\n\n\ne.g. given f(const char* p, size_t n) { if (n == 0)\
    \ return; ...whatever... } you mustn't call f(&x[0], x.size()); when x.empty()\
    \ - just use f(x.data(), ...).\n\notherwise, as per x.data() but:\n\n\nfor non-const\
    \ x this yields a non-const char* pointer; you can overwrite string content\n\n\
    \nx.c_str()\n\nreturns const char* to an ASCIIZ (NUL-terminated) representation\
    \ of the value (i.e. ['h', 'e', 'l', 'l', 'o', '\\0']).\nalthough few if any implementations\
    \ chose to do so, the C++03 Standard was worded to allow the string implementation\
    \ the freedom to create a distinct NUL-terminated buffer on the fly, from the\
    \ potentially non-NUL terminated buffer \"exposed\" by x.data() and &x[0]\nx.size()\
    \ + 1 characters are safe to read.\nguaranteed safe even for empty strings (['\\\
    0']).\n\n\nConsequences of accessing outside legal indices\nWhichever way you\
    \ get a pointer, you must not access memory further along from the pointer than\
    \ the characters guaranteed present in the descriptions above.  Attempts to do\
    \ so have undefined behaviour, with a very real chance of application crashes\
    \ and garbage results even for reads, and additionally wholesale data, stack corruption\
    \ and/or security vulnerabilities for writes.\nWhen do those pointers get invalidated?\n\
    If you call some string member function that modifies the string or reserves further\
    \ capacity, any pointer values returned beforehand by any of the above methods\
    \ are invalidated.  You can use those methods again to get another pointer.  (The\
    \ rules are the same as for iterators into strings).\nSee also How to get a character\
    \ pointer valid even after x leaves scope or is modified further below....\nSo,\
    \ which is better to use?\nFrom C++11, use .c_str() for ASCIIZ data, and .data()\
    \ for \"binary\" data (explained further below).\nIn C++03, use .c_str() unless\
    \ certain that .data() is adequate, and prefer .data() over &x[0] as it's safe\
    \ for empty strings....\n...try to understand the program enough to use data()\
    \ when appropriate, or you'll probably make other mistakes...\nThe ASCII NUL '\\\
    0' character guaranteed by .c_str() is used by many functions as a sentinel value\
    \ denoting the end of relevant and safe-to-access data.  This applies to both\
    \ C++-only functions like say fstream::fstream(const char* filename, ...) and\
    \ shared-with-C functions like strchr(), and printf().\nGiven C++03's .c_str()'s\
    \ guarantees about the returned buffer are a super-set of .data()'s, you can always\
    \ safely use .c_str(), but people sometimes don't because:\n\nusing .data() communicates\
    \ to other programmers reading the source code that the data is not ASCIIZ (rather,\
    \ you're using the string to store a block of data (which sometimes isn't even\
    \ really textual)), or that you're passing it to another function that treats\
    \ it as a block of \"binary\" data.  This can be a crucial insight in ensuring\
    \ that other programmers' code changes continue to handle the data properly.\n\
    C++03 only: there's a slight chance that your string implementation will need\
    \ to do some extra memory allocation and/or data copying in order to prepare the\
    \ NUL terminated buffer\n\nAs a further hint, if a function's parameters require\
    \ the (const) char* but don't insist on getting x.size(), the function probably\
    \ needs an ASCIIZ input, so .c_str() is a good choice (the function needs to know\
    \ where the text terminates somehow, so if it's not a separate parameter it can\
    \ only be a convention like a length-prefix or sentinel or some fixed expected\
    \ length).\nHow to get a character pointer valid even after x leaves scope or\
    \ is modified further\nYou'll need to copy the contents of the string x to a new\
    \ memory area outside x.  This external buffer could be in many places such as\
    \ another string or character array variable, it may or may not have a different\
    \ lifetime than x due to being in a different scope (e.g. namespace, global, static,\
    \ heap, shared memory, memory mapped file).\nTo copy the text from std::string\
    \ x into an independent character array:\n// USING ANOTHER STRING - AUTO MEMORY\
    \ MANAGEMENT, EXCEPTION SAFE\nstd::string old_x = x;\n// - old_x will not be affected\
    \ by subsequent modifications to x...\n// - you can use `&old_x[0]` to get a writable\
    \ char* to old_x's textual content\n// - you can use resize() to reduce/expand\
    \ the string\n//   - resizing isn't possible from within a function passed only\
    \ the char* address\n\nstd::string old_x = x.c_str(); // old_x will terminate\
    \ early if x embeds NUL\n// Copies ASCIIZ data but could be less efficient as\
    \ it needs to scan memory to\n// find the NUL terminator indicating string length\
    \ before allocating that amount\n// of memory to copy into, or more efficient\
    \ if it ends up allocating/copying a\n// lot less content.\n// Example, x == \"\
    ab\\0cd\" -> old_x == \"ab\".\n\n// USING A VECTOR OF CHAR - AUTO, EXCEPTION SAFE,\
    \ HINTS AT BINARY CONTENT, GUARANTEED CONTIGUOUS EVEN IN C++03\nstd::vector<char>\
    \ old_x(x.data(), x.data() + x.size());       // without the NUL\nstd::vector<char>\
    \ old_x(x.c_str(), x.c_str() + x.size() + 1);  // with the NUL\n\n// USING STACK\
    \ WHERE MAXIMUM SIZE OF x IS KNOWN TO BE COMPILE-TIME CONSTANT \"N\"\n// (a bit\
    \ dangerous, as \"known\" things are sometimes wrong and often become wrong)\n\
    char y[N + 1];\nstrcpy(y, x.c_str());\n\n// USING STACK WHERE UNEXPECTEDLY LONG\
    \ x IS TRUNCATED (e.g. Hello\\0->Hel\\0)\nchar y[N + 1];\nstrncpy(y, x.c_str(),\
    \ N);  // copy at most N, zero-padding if shorter\ny[N] = '\\0';             \
    \  // ensure NUL terminated\n\n// USING THE STACK TO HANDLE x OF UNKNOWN (BUT\
    \ SANE) LENGTH\nchar* y = alloca(x.size() + 1);\nstrcpy(y, x.c_str());\n\n// USING\
    \ THE STACK TO HANDLE x OF UNKNOWN LENGTH (NON-STANDARD GCC EXTENSION)\nchar y[x.size()\
    \ + 1];\nstrcpy(y, x.c_str());\n\n// USING new/delete HEAP MEMORY, MANUAL DEALLOC,\
    \ NO INHERENT EXCEPTION SAFETY\nchar* y = new char[x.size() + 1];\nstrcpy(y, x.c_str());\n\
    //     or as a one-liner: char* y = strcpy(new char[x.size() + 1], x.c_str());\n\
    // use y...\ndelete[] y; // make sure no break, return, throw or branching bypasses\
    \ this\n\n// USING new/delete HEAP MEMORY, SMART POINTER DEALLOCATION, EXCEPTION\
    \ SAFE\n// see boost shared_array usage in Johannes Schaub's answer\n\n// USING\
    \ malloc/free HEAP MEMORY, MANUAL DEALLOC, NO INHERENT EXCEPTION SAFETY\nchar*\
    \ y = strdup(x.c_str());\n// use y...\nfree(y);\n\nOther reasons to want a char*\
    \ or const char* generated from a string\nSo, above you've seen how to get a (const)\
    \ char*, and how to make a copy of the text independent of the original string,\
    \ but what can you do with it?  A random smattering of examples...\n\ngive \"\
    C\" code access to the C++ string's text, as in printf(\"x is '%s'\", x.c_str());\n\
    copy x's text to a buffer specified by your function's caller (e.g. strncpy(callers_buffer,\
    \ callers_buffer_size, x.c_str())), or volatile memory used for device I/O (e.g.\
    \ for (const char* p = x.c_str(); *p; ++p) *p_device = *p;)\nappend x's text to\
    \ an character array already containing some ASCIIZ text (e.g. strcat(other_buffer,\
    \ x.c_str())) - be careful not to overrun the buffer (in many situations you may\
    \ need to use strncat)\nreturn a const char* or char* from a function (perhaps\
    \ for historical reasons - client's using your existing API - or for C compatibility\
    \ you don't want to return a std::string, but do want to copy your string's data\
    \ somewhere for the caller)\n\n\nbe careful not to return a pointer that may be\
    \ dereferenced by the caller after a local string variable to which that pointer\
    \ pointed has left scope\nsome projects with shared objects compiled/linked for\
    \ different std::string implementations (e.g. STLport and compiler-native) may\
    \ pass data as ASCIIZ to avoid conflicts\n\n\n"
- - How to convert a std::string to const char* or char*?
  - "\nUse the .c_str() method for const char *.\nYou can use &mystring[0] to get\
    \ a char * pointer, but there are a couple of gotcha's: you won't necessarily\
    \ get a zero terminated string, and you won't be able to change the string's size.\
    \ You especially have to be careful not to add characters past the end of the\
    \ string or you'll get a buffer overrun (and probable crash).\nThere was no guarantee\
    \ that all of the characters would be part of the same contiguous buffer until\
    \ C++11, but in practice all known implementations of std::string worked that\
    \ way anyway; see Does â&s[0]â point to contiguous characters in a std::string?.\n\
    Note that many string member functions will reallocate the internal buffer and\
    \ invalidate any pointers you might have saved. Best to use them immediately and\
    \ then discard.\n"
- - Can I call a constructor from another constructor (do constructor chaining) in
    C++?
  - "\nC++11: Yes!\nC++11 and onwards has this same feature (called delegating constructors).\
    \ \nThe syntax is slightly different from C#:\nclass Foo {\npublic: \n  Foo(char\
    \ x, int y) {}\n  Foo(int y) : Foo('a', y) {}\n};\n\nC++03: No\nUnfortunately,\
    \ there's no way to do this in C++03, but there are two ways of simulating this:\n\
    \nYou can combine two (or more) constructors via default parameters:\nclass Foo\
    \ {\npublic:\n  Foo(char x, int y=0);  // combines two constructors (char) and\
    \ (char, int)\n  // ...\n};\n\nUse an init method to share common code:\nclass\
    \ Foo {\npublic:\n  Foo(char x);\n  Foo(char x, int y);\n  // ...\nprivate:\n\
    \  void init(char x, int y);\n};\n\nFoo::Foo(char x)\n{\n  init(x, int(x) + 7);\n\
    \  // ...\n}\n\nFoo::Foo(char x, int y)\n{\n  init(x, y);\n  // ...\n}\n\nvoid\
    \ Foo::init(char x, int y)\n{\n  // ...\n}\n\n\nSee the C++FAQ entry for reference.\n"
- - Can I call a constructor from another constructor (do constructor chaining) in
    C++?
  - "\nNo, you can't call one constructor from another in C++03 (called a delegating\
    \ constructor).\nThis changed in C++11 (aka C++0x), which added support for the\
    \ following syntax:\n(example taken from Wikipedia)\nclass SomeType\n{\n  int\
    \ number;\n\npublic:\n  SomeType(int newNumber) : number(newNumber) {}\n  SomeType()\
    \ : SomeType(42) {}\n};\n\n"
- - Can I call a constructor from another constructor (do constructor chaining) in
    C++?
  - "\nI believe you can call a constructor from a constructor. It will compile and\
    \ run. I recently saw someone do this and it ran on both Windows and Linux.\n\
    It just doesn't do what you want. The inner constructor will construct a temporary\
    \ local object which gets deleted once the outer constructor returns. They would\
    \ have to be different constructors as well or you would create a recursive call.\n\
    Ref: https://isocpp.org/wiki/faq/ctors#init-methods\n"
- - When should you use a class vs a struct in C++?
  - "\nDifferences between a class and a struct in C++ are that structs have default\
    \ public members and bases and classes have default private members and bases.\
    \ Both classes and structs can have a mixture of public, protected and private\
    \ members, can use inheritance and can have member functions.\nI would recommend\
    \ using structs as plain-old-data structures without any class-like features,\
    \ and using classes as aggregate data structures with private data and member\
    \ functions.\n"
- - When should you use a class vs a struct in C++?
  - "\nAs everyone else notes there are really only two actual language differences:\n\
    \nstruct defaults to public access and class defaults to private access.\nWhen\
    \ inheriting, struct defaults to public inheritance and class defaults to private\
    \ inheritance.  (Ironically, as with so many things in C++, the default is backwards:\
    \ public inheritance is by far the more common choice, but people rarely declare\
    \ structs just to save on typing the \"public\" keyword.\n\nBut the real difference\
    \ in practice is between a class/struct that declares a constructor/destructor\
    \ and one that doesn't.  There are certain guarantees to a \"plain-old-data\"\
    \ POD type, that no longer apply once you take over the class's construction.\
    \  To keep this distinction clear, many people deliberately only use structs for\
    \ POD types, and, if they are going to add any methods at all, use classes.  The\
    \ difference between the two fragments below is otherwise meaningless:\nclass\
    \ X\n{\n  public:\n\n  // ...\n};\n\nstruct X\n{\n  // ...\n};\n\n(Incidentally,\
    \ here's a thread with some good explanations about what \"POD type\" actually\
    \ means: What are POD types in C++?)\n"
- - When should you use a class vs a struct in C++?
  - "\nThere are lots of misconceptions in the existing answers.\nBoth class and struct\
    \ declare a class.\nYes, you may have to rearrange your access modifying keywords\
    \ inside the class definition, depending on which keyword you used to declare\
    \ the class.\nBut, beyond syntax, the only reason to choose one over the other\
    \ is convention/style/preference.\nSome people like to stick with the struct keyword\
    \ for classes without member functions, because the resulting definition \"looks\
    \ like\" a simple structure from C.\nSimilarly, some people like to use the class\
    \ keyword for classes with member functions and private data, because it says\
    \ \"class\" on it and therefore looks like examples from their favourite book\
    \ on object-oriented programming.\nThe reality is that this completely up to you\
    \ and your team, and it'll make literally no difference whatsoever to your program.\n\
    The following two classes are absolutely equivalent in every way except their\
    \ name:\nstruct Foo\n{\n   int x;\n};\n\nclass Bar\n{\npublic:\n   int x;\n};\n\
    \nYou can even switch keywords when redeclaring:\nclass Foo;\nstruct Bar;\n\n\
    (although some compilers will emit a warning when you do this, on the assumption\
    \ that you probably didn't intend to do something so confusing and that you should\
    \ therefore be prompted to double-check your code.)\nand the following expressions\
    \ both evaluate to true:\nstd::is_class<Foo>::value\nstd::is_class<Bar>::value\n\
    \nDo note, though, that you can't switch the keywords when redefining; this is\
    \ only because (per the one-definition rule) duplicate class definitions across\
    \ translation units must \"consist of the same sequence of tokens\". This means\
    \ you can't even exchange const int member; with int const member;, and has nothing\
    \ to do with the semantics of class or struct.\n"
- - Why should C++ programmers minimize use of 'new'?
  - "\nThere are two widely-used memory allocation techniques: automatic allocation\
    \ and dynamic allocation.  Commonly, there is a corresponding region of memory\
    \ for each: the stack and the heap.\nStack\nThe stack always allocates memory\
    \ in a sequential fashion.  It can do so because it requires you to release the\
    \ memory in the reverse order (First-In, Last-Out: FILO).  This is the memory\
    \ allocation technique for local variables in many programming languages.  It\
    \ is very, very fast because it requires minimal bookkeeping and the next address\
    \ to allocate is implicit.\nIn C++, this is called automatic storage because the\
    \ storage is claimed automatically at the end of scope.  As soon as execution\
    \ of current code block (delimited using {}) is completed, memory for all variables\
    \ in that block is automatically collected.  This is also the moment where destructors\
    \ are invoked to clean up resources.\nHeap\nThe heap allows for a more flexible\
    \ memory allocation mode.  Bookkeeping is more complex and allocation is slower.\
    \  Because there is no implicit release point, you must release the memory manually,\
    \ using delete or delete[] (free in C).  However, the absence of an implicit release\
    \ point is the key to the heap's flexibility.\nReasons to use dynamic allocation\n\
    Even if using the heap is slower and potentially leads to memory leaks or memory\
    \ fragmentation, there are perfectly good use cases for dynamic allocation, as\
    \ it's less limited.\nTwo key reasons to use dynamic allocation:\n\nYou don't\
    \ know how much memory you need at compile time.  For instance, when reading a\
    \ text file into a string, you usually don't know what size the file has, so you\
    \ can't decide how much memory to allocate until you run the program.\nYou want\
    \ to allocate memory which will persist after leaving the current block.  For\
    \ instance, you may want to write a function string readfile(string path) that\
    \ returns the contents of a file.  In this case, even if the stack could hold\
    \ the entire file contents, you could not return from a function and keep the\
    \ allocated memory block.\n\nWhy dynamic allocation is often unnecessary\nIn C++\
    \ there's a neat construct called a destructor.  This mechanism allows you to\
    \ manage resources by aligning the lifetime of the resource with the lifetime\
    \ of a variable. This technique is called RAII and is the distinguishing point\
    \ of C++. It \"wraps\" resources into objects.  std::string is a perfect example.\
    \  This snippet:\nint main ( int argc, char* argv[] )\n{\n    std::string program(argv[0]);\n\
    }\n\nactually allocates a variable amount of memory.  The std::string object allocates\
    \ memory using the heap and releases it in its destructor.  In this case, you\
    \ did not need to manually manage any resources and still got the benefits of\
    \ dynamic memory allocation.\nIn particular, it implies that in this snippet:\n\
    int main ( int argc, char* argv[] )\n{\n    std::string * program = new std::string(argv[0]);\
    \  // Bad!\n    delete program;\n}\n\nthere is unneeded dynamic memory allocation.\
    \  The program requires more typing (!) and introduces the risk of forgetting\
    \ to deallocate the memory.  It does this with no apparent benefit.\nWhy you should\
    \ use automatic storage as often as possible\nBasically, the last paragraph sums\
    \ it up.  Using automatic storage as often as possible makes your programs:\n\n\
    faster to type;\nfaster when run;\nless prone to memory/resource leaks.\n\nBonus\
    \ points\nIn the referenced question, there are additional concerns.  In particular,\
    \ the following class:\nclass Line {\npublic:\n    Line();\n    ~Line();\n   \
    \ std::string* mString;\n};\n\nLine::Line() {\n    mString = new std::string(\"\
    foo_bar\");\n}\n\nLine::~Line() {\n    delete mString;\n}\n\nIs actually a lot\
    \ more risky to use than the following one:\nclass Line {\npublic:\n    Line();\n\
    \    std::string mString;\n};\n\nLine::Line() {\n    mString = \"foo_bar\";\n\
    \    // note: there is a cleaner way to write this.\n}\n\nThe reason is that std::string\
    \ properly defines a copy constructor.  Consider the following program:\nint main\
    \ ()\n{\n    Line l1;\n    Line l2 = l1;\n}\n\nUsing the original version, this\
    \ program will likely crash, as it uses delete on the same string twice.  Using\
    \ the modified version, each Line instance will own its own string instance, each\
    \ with its own memory and both will be released at the end of the program.\nOther\
    \ notes\nExtensive use of RAII is considered a best practice in C++ because of\
    \ all the reasons above.  However, there is an additional benefit which is not\
    \ immediately obvious.  Basically, it's better than the sum of its parts.  The\
    \ whole mechanism composes.  It scales.\nIf you use the Line class as a building\
    \ block:\n class Table\n {\n      Line borders[4];\n };\n\nThen\n int main ()\n\
    \ {\n     Table table;\n }\n\nallocates four std::string instances, four Line\
    \ instances, one Table instance and all the string's contents and everything is\
    \ freed automagically.\n"
- - Why should C++ programmers minimize use of 'new'?
  - "\nBecause the stack is fast and foolproof\nIn C++, it takes but a single instruction\
    \ to allocate space -- on the stack -- for every local scope object in a given\
    \ function, and it's impossible to leak any of that memory. That comment intended\
    \ (or should have intended) to say something like \"use the stack and not the\
    \ heap\".\n"
- - Why should C++ programmers minimize use of 'new'?
  - "\nIt's complicated.\nFirst, C++ is not garbage collected. Therefore, for every\
    \ new, there must be a corresponding delete. If you fail to put this delete in,\
    \ then you have a memory leak. Now, for a simple case like this:\nstd::string\
    \ *someString = new std::string(...);\n//Do stuff\ndelete someString;\n\nThis\
    \ is simple. But what happens if \"Do stuff\" throws an exception? Oops: memory\
    \ leak. What happens if \"Do stuff\" issues return early? Oops: memory leak.\n\
    And this is for the simplest case. If you happen to return that string to someone,\
    \ now they have to delete it. And if they pass it as an argument, does the person\
    \ receiving it need to delete it? When should they delete it?\nOr, you can just\
    \ do this:\nstd::string someString(...);\n//Do stuff\n\nNo delete. The object\
    \ was created on the \"stack\", and it will be destroyed once it goes out of scope.\
    \ You can even return the object, thus transfering its contents to the calling\
    \ function. You can pass the object to functions (typically as a reference or\
    \ const-reference: void SomeFunc(std::string &iCanModifyThis, const std::string\
    \ &iCantModifyThis). And so forth.\nAll without new and delete. There's no question\
    \ of who owns the memory or who's responsible for deleting it. If you do:\nstd::string\
    \ someString(...);\nstd::string otherString;\notherString = someString;\n\nIt\
    \ is understood that otherString has a copy of the data of someString. It isn't\
    \ a pointer; it is a separate object. They may happen to have the same contents,\
    \ but you can change one without affecting the other:\nsomeString += \"More text.\"\
    ;\nif(otherString == someString) { /*Will never get here */ }\n\nSee the idea?\n"
- - C++ code for testing the Collatz conjecture faster than hand-written assembly
    - why?
  - "\nIf you think a 64-bit DIV instruction is a good way to divide by two, then\
    \ no wonder the compiler's asm output beat your hand-written code, even with -O0\
    \ (compile fast, no extra optimization, and store/reload to memory after/before\
    \ every C statement so a debugger can modify variables).\nSee Agner Fog's Optimizing\
    \ Assembly guide to learn how to write efficient asm.  He also has instruction\
    \ tables and a microarch guide for specific details for specific CPUs.  See also\
    \ the x86 tag wiki for more perf links.\nSee also this more general question about\
    \ beating the compiler with hand-written asm: Is inline assembly language slower\
    \ than native C++ code?.  TL:DR: yes if you do it wrong (like this question).\n\
    Usually you're fine letting the compiler do its thing, especially if you try to\
    \ write C++ that can compile efficiently.  Also see is assembly faster than compiled\
    \ languages?.  One of the answers links to these neat slides showing how various\
    \ C compilers optimize some really simple functions with cool tricks.\n\neven:\n\
    \    mov rbx, 2\n    xor rdx, rdx\n    div rbx\n\nOn Intel Haswell, div r64 is\
    \ 36 uops, with a latency of 32-96 cycles, and a throughput of one per 21-74 cycles.\
    \  (Plus the 2 uops to set up RBX and zero RDX, but out-of-order execution can\
    \ run those early).  High-uop-count instructions like DIV are microcoded, which\
    \ can also cause front-end bottlenecks. In this case, latency is the most relevant\
    \ factor because it's part of a loop-carried dependency chain.\nshr rax, 1 does\
    \ the same unsigned division: It's 1 uop, with 1c latency, and can run 2 per clock\
    \ cycle.\nFor comparison, 32-bit division is faster, but still horrible vs. shifts.\
    \ idiv r32 is 9 uops, 22-29c latency, and one per 8-11c throughput on Haswell.\n\
    \nAs you can see from looking at gcc's -O0 asm output (Godbolt compiler explorer),\
    \ it only uses shifts instructions. clang -O0 does compile naively like you thought,\
    \ even using 64-bit IDIV twice. (When optimizing, compilers do use both outputs\
    \ of IDIV when the source does a division and modulus with the same operands,\
    \ if they use IDIV at all)\nGCC doesn't have a totally-naive mode; it always transforms\
    \ through GIMPLE, which means some \"optimizations\" can't be disabled.  This\
    \ includes recognizing division-by-constant and using shifts (power of 2) or a\
    \ fixed-point multiplicative inverse (non power of 2) to avoid IDIV (see div_by_13\
    \ in the above godbolt link).\ngcc -Os (optimize for size) does use IDIV for non-power-of-2\
    \ division,\nunfortunately even in cases where the multiplicative inverse code\
    \ is only slightly larger but much faster.\n\nHelping the compiler\n(summary for\
    \ this case: use uint64_t n)\nFirst of all, it's only interesting to look at optimized\
    \ compiler output.  (-O3).  -O0 speed is basically meaningless.\nLook at your\
    \ asm output (on Godbolt, or see How to remove \"noise\" from GCC/clang assembly\
    \ output?).  When the compiler doesn't make optimal code in the first place: Writing\
    \ your C/C++ source in a way that guides the compiler into making better code\
    \ is usually the best approach.  You have to know asm, and know what's efficient,\
    \ but you apply this knowledge indirectly.  Compilers are also a good source of\
    \ ideas: sometimes clang will do something cool, and you can hand-hold gcc into\
    \ doing the same thing: see this answer and what I did with the non-unrolled loop\
    \ in @Veedrac's code below.)\nThis approach is portable, and in 20 years some\
    \ future compiler can compile it to whatever is efficient on future hardware (x86\
    \ or not), maybe using new ISA extension or auto-vectorizing.  Hand-written x86-64\
    \ asm from 15 years ago would usually not be optimally tuned for Skylake.  e.g.\
    \ compare&branch macro-fusion didn't exist back then.  What's optimal now for\
    \ hand-crafted asm for one microarchitecture might not be optimal for other current\
    \ and future CPUs. Comments on @johnfound's answer discuss major differences between\
    \ AMD Bulldozer and Intel Haswell, which have a big effect on this code.  But\
    \ in theory, g++ -O3 -march=bdver3 and g++ -O3 -march=skylake will do the right\
    \ thing.  (Or -march=native.)   Or -mtune=... to just tune, without using instructions\
    \ that other CPUs might not support.\nMy feeling is that guiding the compiler\
    \ to asm that's good for a current CPU you care about shouldn't be a problem for\
    \ future compilers.  They're hopefully better than current compilers at finding\
    \ ways to transform code, and can find a way that works for future CPUs.  Regardless,\
    \ future x86 probably won't be terrible at anything that's good on current x86,\
    \ and the future compiler will avoid any asm-specific pitfalls while implementing\
    \ something like the data movement from your C source, if it doesn't see something\
    \ better.\nHand-written asm is a black-box for the optimizer, so constant-propagation\
    \ doesn't work when inlining makes an input a compile-time constant.  Other optimizations\
    \ are also affected.  Read https://gcc.gnu.org/wiki/DontUseInlineAsm before using\
    \ asm.  (And avoid MSVC-style inline asm: inputs/outputs have to go through memory\
    \ which adds overhead.)\nIn this case: your n has a signed type, and gcc uses\
    \ the SAR/SHR/ADD sequence that gives the correct rounding.  (IDIV and arithmetic-shift\
    \ \"round\" differently for negative inputs, see the SAR insn set ref manual entry).\
    \  (IDK if gcc tried and failed to prove that n can't be negative, or what.  Signed-overflow\
    \ is undefined behaviour, so it should have been able to.)\nYou should have used\
    \ uint64_t n, so it can just SHR.  And so it's portable to systems where long\
    \ is only 32-bit (e.g. x86-64 Windows).\n\nBTW, gcc's optimized asm output looks\
    \ pretty good (using unsigned long n): the inner loop it inlines into main() does\
    \ this:\n # from gcc5.4 -O3  plus my comments\n\n # edx= count=1\n # rax= uint64_t\
    \ n\n\n.L9:                   # do{\n    lea    rcx, [rax+1+rax*2]   # rcx = 3*n\
    \ + 1\n    mov    rdi, rax\n    shr    rdi         # rdi = n>>1;\n    test   al,\
    \ 1       # set flags based on n%2 (aka n&1)\n    mov    rax, rcx\n    cmove \
    \ rax, rdi    # n= (n%2) ? 3*n+1 : n/2;\n    add    edx, 1      # ++count;\n \
    \   cmp    rax, 1\n    jne   .L9          #}while(n!=1)\n\n  cmp/branch to update\
    \ max and maxi, and then do the next n\n\nThe inner loop is branchless, and the\
    \ critical path of the loop-carried dependency chain is:\n\n3-component LEA (3\
    \ cycles)\ncmov (2 cycles on Haswell, 1c on Broadwell or later).\n\nTotal: 5 cycle\
    \ per iteration, latency bottleneck.  Out-of-order execution takes care of everything\
    \ else in parallel with this (in theory: I haven't tested with perf counters to\
    \ see if it really runs at 5c/iter).\nThe FLAGS input of cmov (produced by TEST)\
    \ is faster to produce than the RAX input (from LEA->MOV), so it's not on the\
    \ critical path.\nSimilarly, the MOV->SHR that produces CMOV's RDI input is off\
    \ the critical path, because it's also faster than the LEA.  MOV on IvyBridge\
    \ and later has zero latency (handled at register-rename time).  (It still takes\
    \ a uop, and a slot in the pipeline, so it's not free, just zero latency).  The\
    \ extra MOV in the LEA dep chain is part of the bottleneck on other CPUs.\nThe\
    \ cmp/jne is also not part of the critical path: it's not loop-carried, because\
    \ control dependencies are handled with branch prediction + speculative execution,\
    \ unlike data dependencies on the critical path.\n\nBeating the compiler\nGCC\
    \ did a pretty good job here.  It could save one code byte by using inc edx instead\
    \ of add edx, 1, because nobody cares about P4 and its false-dependencies for\
    \ partial-flag-modifying instructions.\nIt could also save all the MOV instructions,\
    \ and the TEST:  SHR sets CF= the bit shifted out, so we can use cmovc instead\
    \ of test / cmovz.\n ### Hand-optimized version of what gcc does\n.L9:       \
    \                #do{\n    lea     rcx, [rax+1+rax*2] # rcx = 3*n + 1\n    shr\
    \     rax, 1         # n>>=1;    CF = n&1 = n%2\n    cmovc   rax, rcx       #\
    \ n= (n&1) ? 3*n+1 : n/2;\n    inc     edx            # ++count;\n    cmp    \
    \ rax, 1\n    jne     .L9            #}while(n!=1)\n\nSee @johnfound's answer\
    \ for another clever trick: remove the CMP by branching on SHR's flag result as\
    \ well as using it for CMOV:  zero only if n was 1 (or 0) to start with.  (Fun\
    \ fact: SHR with count != 1 on Nehalem or earlier causes a stall if you read the\
    \ flag results.  That's how they made it single-uop.  The shift-by-1 special encoding\
    \ is fine, though.)\nAvoiding MOV doesn't help with the latency at all on Haswell\
    \ (Can x86's MOV really be \"free\"? Why can't I reproduce this at all?).  It\
    \ does help significantly on CPUs like Intel pre-IvB, and AMD Bulldozer-family,\
    \ where MOV is not zero-latency.  The compiler's wasted MOV instructions do affect\
    \ the critical path.  BD's complex-LEA and CMOV are both lower latency (2c and\
    \ 1c respectively), so it's a bigger fraction of the latency.  Also, throughput\
    \ bottlenecks become an issue, because it only has two integer ALU pipes.  See\
    \ @johnfound's answer, where he has timing results from an AMD CPU.\nEven on Haswell,\
    \ this version may help a bit by avoiding some occasional delays where a non-critical\
    \ uop steals an execution port from one on the critical path, delaying execution\
    \ by 1 cycle.  (This is called a resource conflict).  It also saves a register,\
    \ which may help when doing multiple n values in parallel in an interleaved loop\
    \ (see below).\nLEA's latency depends on the addressing mode, on Intel SnB-family\
    \ CPUs.  3c for 3 components ([base+idx+const], which takes two separate adds),\
    \ but only 1c with 2 or fewer components (one add).  Some CPUs (like Core2) do\
    \ even a 3-component LEA in a single cycle, but SnB-family doesn't.  Worse, Intel\
    \ SnB-family standardizes latencies so there are no 2c uops, otherwise 3-component\
    \ LEA would be only 2c like Bulldozer.  (3-component LEA is slower on AMD as well,\
    \ just not by as much).\nSo lea  rcx, [rax + rax*2] / inc rcx is only 2c latency,\
    \ faster than lea  rcx, [rax + rax*2 + 1], on Intel SnB-family CPUs like Haswell.\
    \  Break-even on BD, and worse on Core2.  It does cost an extra uop, which normally\
    \ isn't worth it to save 1c latency, but latency is the major bottleneck here\
    \ and Haswell has a wide enough pipeline to handle the extra uop throughput.\n\
    Neither gcc, icc, nor clang (on godbolt) used SHR's CF output, always using an\
    \ AND or TEST.  Silly compilers. :P  They're great pieces of complex machinery,\
    \ but a clever human can often beat them on small-scale problems.  (Given thousands\
    \ to millions of times longer to think about it, of course!  Compilers don't use\
    \ exhaustive algorithms to search for every possible way to do things, because\
    \ that would take too long when optimizing a lot of inlined code, which is what\
    \ they do best.  They also don't model the pipeline in the target microarchitecture,\
    \ at least not in the same detail as IACA or other static-analysis tools; they\
    \ just use some heuristics.)\n\nSimple loop unrolling won't help; this loop bottlenecks\
    \ on the latency of a loop-carried dependency chain, not on loop overhead / throughput.\
    \  This means it would do well with hyperthreading (or any other kind of SMT),\
    \ since the CPU has lots of time to interleave instructions from two threads.\
    \  This would mean parallelizing the loop in main, but that's fine because each\
    \ thread can just check a range of n values and produce a pair of integers as\
    \ a result.\nInterleaving by hand within a single thread might be viable, too.\
    \  Maybe compute the sequence for a pair of numbers in parallel, since each one\
    \ only takes a couple registers, and they can all update the same max / maxi.\
    \  This creates more instruction-level parallelism.\nThe trick is deciding whether\
    \ to wait until all the n values have reached 1 before getting another pair of\
    \ starting n values, or whether to break out and get a new start point for just\
    \ one that reached the end condition, without touching the registers for the other\
    \ sequence.  Probably it's best to keep each chain working on useful data, otherwise\
    \ you'd have to conditionally increment its counter.\n\nYou could maybe even do\
    \ this with SSE packed-compare stuff to conditionally increment the counter for\
    \ vector elements where n hadn't reached 1 yet.  And then to hide the even longer\
    \ latency of a SIMD conditional-increment implementation, you'd need to keep more\
    \ vectors of n values up in the air.  Maybe only worth with 256b vector (4x uint64_t).\n\
    I think the best strategy to make detection of a 1 \"sticky\" is to mask the vector\
    \ of all-ones that you add to increment the counter.  So after you've seen a 1\
    \ in an element, the increment-vector will have a zero, and +=0 is a no-op.\n\
    Untested idea for manual vectorization\n# starting with YMM0 = [ n_d, n_c, n_b,\
    \ n_a ]  (64-bit elements)\n# ymm4 = _mm256_set1_epi64x(1):  increment vector\n\
    # ymm5 = all-zeros:  count vector\n\n.inner_loop:\n    vpaddq    ymm1, ymm0, xmm0\n\
    \    vpaddq    ymm1, ymm1, xmm0\n    vpaddq    ymm1, ymm1, set1_epi64(1)     #\
    \ ymm1= 3*n + 1.  Maybe could do this more efficiently?\n\n    vprllq    ymm3,\
    \ ymm0, 63                # shift bit 1 to the sign bit\n\n    vpsrlq    ymm0,\
    \ ymm0, 1                 # n /= 2\n\n    # There may be a better way to do this\
    \ blend, avoiding the bypass delay for an FP blend between integer insns, not\
    \ sure.  Probably worth it\n    vpblendvpd ymm0, ymm0, ymm1, ymm3       # variable\
    \ blend controlled by the sign bit of each 64-bit element.  I might have the source\
    \ operands backwards, I always have to look this up.\n\n    # ymm0 = updated n\
    \  in each element.\n\n    vpcmpeqq ymm1, ymm0, set1_epi64(1)\n    vpandn   ymm4,\
    \ ymm1, ymm4         # zero out elements of ymm4 where the compare was true\n\n\
    \    vpaddq   ymm5, ymm5, ymm4         # count++ in elements where n has never\
    \ been == 1\n\n    vptest   ymm4, ymm4\n    jnz  .inner_loop\n    # Fall through\
    \ when all the n values have reached 1 at some point, and our increment vector\
    \ is all-zero\n\n    vextracti128 ymm0, ymm5, 1\n    vpmaxq .... crap this doesn't\
    \ exist\n    # Actually just delay doing a horizontal max until the very very\
    \ end.  But you need some way to record max and maxi.\n\nYou can and should implement\
    \ this with intrinsics, instead of hand-written asm.\n\nAlgorithmic / implementation\
    \ improvement:\nBesides just implementing the same logic with more efficient asm,\
    \ look for ways to simplify the logic, or avoid redundant work.  e.g. memoize\
    \ to detect common endings to sequences.  Or even better, look at 8 trailing bits\
    \ at once (gnasher's answer)\n@EOF points out that tzcnt (or bsf) could be used\
    \ to do multiple n/=2 iterations in one step.  That's probably better than SIMD\
    \ vectorizing, because no SSE or AVX instruction can do that.  It's still compatible\
    \ with doing multiple scalar ns in parallel in different integer registers, though.\n\
    So the loop might look like this:\ngoto loop_entry;  // C++ structured like the\
    \ asm, for illustration only\ndo {\n   n = n*3 + 1;\n  loop_entry:\n   shift =\
    \ _tzcnt_u64(n);\n   n >>= shift;\n   count += shift;\n} while(n != 1);\n\nThis\
    \ may do significantly fewer iterations, but variable-count shifts are slow on\
    \ Intel SnB-family CPUs without BMI2.  3 uops, 2c latency.  (They have an input\
    \ dependency on the FLAGS because count=0 means the flags are unmodified.  They\
    \ handle this as a data dependency, and take multiple uops because a uop can only\
    \ have 2 inputs (pre-HSW/BDW anyway)).  This is the kind that people complaining\
    \ about x86's crazy-CISC design are referring to.  It makes x86 CPUs slower than\
    \ they would be if the ISA was designed from scratch today, even in a mostly-similar\
    \ way.  (i.e. this is part of the \"x86 tax\" that costs speed / power.)  SHRX/SHLX/SARX\
    \ (BMI2) are a big win (1 uop / 1c latency).\nIt also puts tzcnt (3c on Haswell\
    \ and later) on the critical path, so it significantly lengthens the total latency\
    \ of the loop-carried dependency chain.  It does remove any need for a CMOV, or\
    \ for preparing a register holding n>>1, though.  @Veedrac's answer overcomes\
    \ all this by deferring the tzcnt/shift for multiple iterations, which is highly\
    \ effective (see below).\nWe can safely use BSF or TZCNT interchangeably, because\
    \ n can never be zero at that point.  TZCNT's machine-code decodes as BSF on CPUs\
    \ that don't support BMI1.  (Meaningless prefixes are ignored, so REP BSF runs\
    \ as BSF).\nTZCNT performs much better than BSF on AMD CPUs that support it, \
    \ so it can be a good idea to use REP BSF, even if you don't care about setting\
    \ ZF if the input is zero rather than the output.  Some compilers do this when\
    \ you use __builtin_ctzll even with -mno-bmi.  \nThey perform the same on Intel\
    \ CPUs, so just save the byte if that's all that matters.  TZCNT on Intel (pre-Skylake)\
    \ still has a false-dependency on the supposedly write-only output operand, just\
    \ like BSF, to support the undocumented behaviour that BSF with input = 0 leaves\
    \ its destination unmodified.  So you need to work around that unless optimizing\
    \ only for Skylake, so there's nothing to gain from the extra REP byte.  (Intel\
    \ often goes above and beyond what the x86 ISA manual requires, to avoid breaking\
    \ widely-used code that depends on something it shouldn't, or that is retroactively\
    \ disallowed.  e.g. Windows 9x's assumes no speculative prefetching of TLB entries,\
    \ which was safe when the code was written, before Intel updated the TLB management\
    \ rules.)\nAnyway, LZCNT/TZCNT on Haswell have the same false dep as POPCNT: see\
    \ this Q&A.  This is why in gcc's asm output for @Veedrac's code, you see it breaking\
    \ the dep chain with xor-zeroing on the register it's about to use as TZCNT's\
    \ destination, when it doesn't use dst=src.  Since TZCNT/LZCNT/POPCNT never leave\
    \ their destination undefined or unmodified, this false dependency on the output\
    \ on Intel CPUs is purely a performance bug / limitation.  Presumably it's worth\
    \ some transistors / power to have them behave like other uops that go to the\
    \ same execution unit.  The only software-visible upside is in the interaction\
    \ with another microarchitectural limitation: they can micro-fuse a memory operand\
    \ with an indexed addressing mode on Haswell, but on Skylake where Intel removed\
    \ the false dependency for LZCNT/TZCNT they \"un-laminate\" indexed addressing\
    \ modes while POPCNT can still micro-fuse any addr mode.\n\nImprovements to ideas\
    \ / code from other answers:\n@hidefromkgb's answer has a nice observation that\
    \ you're guaranteed to be able to do one right shift after a 3n+1.  You can compute\
    \ this more even more efficiently than just leaving out the checks between steps.\
    \  The asm implementation in that answer is broken, though (it depends on OF,\
    \ which is undefined after SHRD with a count > 1), and slow: ROR rdi,2 is faster\
    \ than SHRD rdi,rdi,2, and using two CMOV instructions on the critical path is\
    \ slower than an extra TEST that can run in parallel.\nI put tidied / improved\
    \ C (which guides the compiler to produce better asm), and tested+working faster\
    \ asm (in comments below the C) up on Godbolt: see the link in @hidefromkgb's\
    \ answer.  (This answer hit the 30k char limit from the large Godbolt URLs, but\
    \ shortlinks can rot and were too long for goo.gl anyway.)\nAlso improved the\
    \ output-printing to convert to a string and make one write() instead of writing\
    \ one char at a time. This minimizes impact on timing the whole program with perf\
    \ stat ./collatz (to record performance counters), and I de-obfuscated some of\
    \ the non-critical asm.\n\n@Veedrac's code\nI got a very small speedup from right-shifting\
    \ as much as we know needs doing, and checking to continue the loop.  From 7.5s\
    \ for limit=1e8 down to 7.275s, on Core2Duo (Merom), with an unroll factor of\
    \ 16.\ncode + comments on Godbolt.  Don't use this version with clang; it does\
    \ something silly with the defer-loop.  Using a tmp counter k and then adding\
    \ it to count later changes what clang does, but that slightly hurts gcc.\nSee\
    \ discussion in comments: Veedrac's code is excellent on CPUs with BMI1 (i.e.\
    \ not Celeron/Pentium)\n"
- - C++ code for testing the Collatz conjecture faster than hand-written assembly
    - why?
  - "\nClaiming that the C++ compiler can produce more optimal code than a competent\
    \ assembly language programmer is a very bad mistake. And especially in this case.\
    \ The human always can make the code better that the compiler can, and this particular\
    \ situation is good illustration of this claim.\nThe timing difference you're\
    \ seeing is because the assembly code in the question is very far from optimal\
    \ in the inner loops.\n(The below code is 32-bit, but can be easily converted\
    \ to 64-bit)\nFor example, the sequence function can be optimized to only 5 instructions:\n\
    \    .seq:\n        inc     esi                 ; counter\n        lea     edx,\
    \ [3*eax+1]      ; edx = 3*n+1\n        shr     eax, 1              ; eax = n/2\n\
    \        cmovc   eax, edx            ; if CF eax = edx\n        jnz     .seq \
    \               ; jmp if n<>1\n\nThe whole code looks like:\ninclude \"%lib%/freshlib.inc\"\
    \n@BinaryType console, compact\noptions.DebugMode = 1\ninclude \"%lib%/freshlib.asm\"\
    \n\nstart:\n        InitializeAll\n        mov ecx, 999999\n        xor edi, edi\
    \        ; max\n        xor ebx, ebx        ; max i\n\n    .main_loop:\n\n   \
    \     xor     esi, esi\n        mov     eax, ecx\n\n    .seq:\n        inc   \
    \  esi                 ; counter\n        lea     edx, [3*eax+1]      ; edx =\
    \ 3*n+1\n        shr     eax, 1              ; eax = n/2\n        cmovc   eax,\
    \ edx            ; if CF eax = edx\n        jnz     .seq                ; jmp\
    \ if n<>1\n\n        cmp     edi, esi\n        cmovb   edi, esi\n        cmovb\
    \   ebx, ecx\n\n        dec     ecx\n        jnz     .main_loop\n\n        OutputValue\
    \ \"Max sequence: \", edi, 10, -1\n        OutputValue \"Max index: \", ebx, 10,\
    \ -1\n\n        FinalizeAll\n        stdcall TerminateAll, 0\n\nIn order to compile\
    \ this code, FreshLib is needed.\nIn my tests, (1Â GHz AMD A4-1200 processor),\
    \ the above code is approximately four times faster than the C++ code from the\
    \ question (when compiled with -O0: 430Â ms vs. 1900Â ms), and more than two times\
    \ faster (430Â ms vs. 830Â ms) when the C++ code is compiled with -O3.\nThe output\
    \ of both programs is the same: max sequence = 525 on i = 837799.\n"
- - C++ code for testing the Collatz conjecture faster than hand-written assembly
    - why?
  - "\nFor more performance: A simple change is observing that after n = 3n+1, n will\
    \ be even, so you can divide by 2 immediately. And n won't be 1, so you don't\
    \ need to test for it. So you could save a few if statements and write: \nwhile\
    \ (n % 2 == 0) n /= 2;\nif (n > 1) for (;;) {\n    n = (3*n + 1) / 2;\n    if\
    \ (n % 2 == 0) {\n        do n /= 2; while (n % 2 == 0);\n        if (n == 1)\
    \ break;\n    }\n}\n\nHere's a big win: If you look at the lowest 8 bits of n,\
    \ all the steps until you divided by 2 eight times are completely determined by\
    \ those eight bits. For example, if the last eight bits are 0x01, that is in binary\
    \ your number is ???? 0000 0001 then the next steps are:\n3n+1 -> ???? 0000 0100\n\
    / 2  -> ???? ?000 0010\n/ 2  -> ???? ??00 0001\n3n+1 -> ???? ??00 0100\n/ 2  ->\
    \ ???? ???0 0010\n/ 2  -> ???? ???? 0001\n3n+1 -> ???? ???? 0100\n/ 2  -> ????\
    \ ???? ?010\n/ 2  -> ???? ???? ??01\n3n+1 -> ???? ???? ??00\n/ 2  -> ???? ????\
    \ ???0\n/ 2  -> ???? ???? ????\n\nSo all these steps can be predicted, and 256k\
    \ + 1 is replaced with 81k + 1. Something similar will happen for all combinations.\
    \ So you can make a loop with a big switch statement: \nk = n / 256;\nm = n %\
    \ 256;\n\nswitch (m) {\n    case 0: n = 1 * k + 0; break;\n    case 1: n = 81\
    \ * k + 1; break; \n    case 2: n = 81 * k + 1; break; \n    ...\n    case 155:\
    \ n = 729 * k + 425; break;\n    ...\n}\n\nRun the loop until n â¤ 128, because\
    \ at that point n could become 1 with fewer than eight divisions by 2, and doing\
    \ eight or more steps at a time would make you miss the point where you reach\
    \ 1 for the first time. Then continue the \"normal\" loop - or have a table prepared\
    \ that tells you how many more steps are need to reach 1. \nPS. I strongly suspect\
    \ Peter Cordes' suggestion would make it even faster. There will be no conditional\
    \ branches at all except one, and that one will be predicted correctly except\
    \ when the loop actually ends. So the code would be something like\nstatic const\
    \ unsigned int multipliers [256] = { ... }\nstatic const unsigned int adders [256]\
    \ = { ... }\n\nwhile (n > 128) {\n    size_t lastBits = n % 256;\n    n = (n >>\
    \ 8) * multipliers [lastBits] + adders [lastBits];\n}\n\nIn practice, you would\
    \ measure whether processing the last 9, 10, 11, 12 bits of n at a time would\
    \ be faster. For each bit, the number of entries in the table would double, and\
    \ I excect a slowdown when the tables don't fit into L1 cache anymore. \nPPS.\
    \ If you need the number of operations: In each iteration we do exactly eight\
    \ divisions by two, and a variable number of (3n + 1) operations, so an obvious\
    \ method to count the operations would be another array. But we can actually calculate\
    \ the number of steps (based on number of iterations of the loop).\nWe could redefine\
    \ the problem slightly: Replace n with (3n + 1) / 2 if odd, and replace n with\
    \ n / 2 if even. Then every iteration will do exactly 8 steps, but you could consider\
    \ that cheating :-) So assume there were r operations n <- 3n+1 and s operations\
    \ n <- n/2. The result will be quite exactly n' = n * 3^r / 2^s, because n <-\
    \ 3n+1 means n <- 3n * (1 + 1/3n). Taking the logarithm we find r = (s + log2\
    \ (n' / n)) / log2 (3). \nIf we do the loop until n â¤ 1,000,000 and have a precomputed\
    \ table how many iterations are needed from any start point n â¤ 1,000,000 then\
    \ calculating r as above, rounded to the nearest integer, will give the right\
    \ result unless s is truly large. \n"
- - How do you declare an interface in C++?
  - "\nTo expand on the answer by bradtgmurray,  you may want to make one exception\
    \ to the pure virtual method list of your interface by adding a virtual destructor.\
    \ This allows you to pass pointer ownership to another party without exposing\
    \ the concrete derived class. The destructor doesn't have to do anything, because\
    \ the interface doesn't have any concrete members. It might seem contradictory\
    \ to define a function as both virtual and inline, but trust me - it isn't.\n\
    class IDemo\n{\n    public:\n        virtual ~IDemo() {}\n        virtual void\
    \ OverrideMe() = 0;\n};\n\nclass Parent\n{\n    public:\n        virtual ~Parent();\n\
    };\n\nclass Child : public Parent, public IDemo\n{\n    public:\n        virtual\
    \ void OverrideMe()\n        {\n            //do stuff\n        }\n};\n\nYou don't\
    \ have to include a body for the virtual destructor - it turns out some compilers\
    \ have trouble optimizing an empty destructor and you're better off using the\
    \ default. \n"
- - How do you declare an interface in C++?
  - "\nMake a class with pure virtual methods. Use the interface by creating another\
    \ class that overrides those virtual methods.\nA pure virtual method is a class\
    \ method that is defined as virtual and assigned to 0.\nclass IDemo\n{\n    public:\n\
    \        virtual ~IDemo() {}\n        virtual void OverrideMe() = 0;\n};\n\nclass\
    \ Child : public IDemo\n{\n    public:\n        virtual void OverrideMe()\n  \
    \      {\n            //do stuff\n        }\n};\n\n"
- - How do you declare an interface in C++?
  - "\nThe whole reason you have a special Interface type-category in addition to\
    \ abstract base classes in C#/Java is because C#/Java do not support multiple\
    \ inheritance.  \nC++ supports multiple inheritance, and so a special type isn't\
    \ needed.  An abstract base class with no non-abstract (pure virtual) methods\
    \ is functionally equivalent to a C#/Java interface.\n"
- - What are C++ functors and their uses?
  - "\nA functor is pretty much just a class which defines the operator(). That lets\
    \ you create objects which \"look like\" a function:\n// this is a functor\nstruct\
    \ add_x {\n  add_x(int x) : x(x) {}\n  int operator()(int y) const { return x\
    \ + y; }\n\nprivate:\n  int x;\n};\n\n// Now you can use it like this:\nadd_x\
    \ add42(42); // create an instance of the functor class\nint i = add42(8); //\
    \ and \"call\" it\nassert(i == 50); // and it added 42 to its argument\n\nstd::vector<int>\
    \ in; // assume this contains a bunch of values)\nstd::vector<int> out(in.size());\n\
    // Pass a functor to std::transform, which calls the functor on every element\
    \ \n// in the input sequence, and stores the result to the output sequence\nstd::transform(in.begin(),\
    \ in.end(), out.begin(), add_x(1)); \nassert(out[i] == in[i] + 1); // for all\
    \ i\n\nThere are a couple of nice things about functors. One is that unlike regular\
    \ functions, they can contain state. The above example creates a function which\
    \ adds 42 to whatever you give it. But that value 42 is not hardcoded, it was\
    \ specified as a constructor argument when we created our functor instance. I\
    \ could create another adder, which added 27, just by calling the constructor\
    \ with a different value. This makes them nicely customizable.\nAs the last lines\
    \ show, you often pass functors as arguments to other functions such as std::transform\
    \ or the other standard library algorithms. You could do the same with a regular\
    \ function pointer except, as I said above, functors can be \"customized\" because\
    \ they contain state, making them more flexible (If I wanted to use a function\
    \ pointer, I'd have to write a function which added exactly 1 to its argument.\
    \ The functor is general, and adds whatever you initialized it with), and they\
    \ are also potentially more efficient. In the above example, the compiler knows\
    \ exactly which function std::transform should call. It should call add_x::operator().\
    \ That means it can inline that function call. And that makes it just as efficient\
    \ as if I had manually called the function on each value of the vector.\nIf I\
    \ had passed a function pointer instead, the compiler couldn't immediately see\
    \ which function it points to, so unless it performs some fairly complex global\
    \ optimizations, it'd have to dereference the pointer at runtime, and then make\
    \ the call.\n"
- - What are C++ functors and their uses?
  - "\nLittle addition. You can use boost::function, to create functors from functions\
    \ and methods, like this:\nclass Foo\n{\npublic:\n    void operator () (int i)\
    \ { printf(\"Foo %d\", i); }\n};\nvoid Bar(int i) { printf(\"Bar %d\", i); }\n\
    Foo foo;\nboost::function<void (int)> f(foo);//wrap functor\nf(1);//prints \"\
    Foo 1\"\nboost::function<void (int)> b(&Bar);//wrap normal function\nb(1);//prints\
    \ \"Bar 1\"\n\nand you can use boost::bind to add state to this functor\nboost::function<void\
    \ ()> f1 = boost::bind(foo, 2);\nf1();//no more argument, function argument stored\
    \ in f1\n//and this print \"Foo 2\" (:\n//and normal function\nboost::function<void\
    \ ()> b1 = boost::bind(&Bar, 2);\nb1();// print \"Bar 2\"\n\nand most useful,\
    \ with boost::bind and boost::function you can create functor from class method,\
    \ actually this is a delegate:\nclass SomeClass\n{\n    std::string state_;\n\
    public:\n    SomeClass(const char* s) : state_(s) {}\n\n    void method( std::string\
    \ param )\n    {\n        std::cout << state_ << param << std::endl;\n    }\n\
    };\nSomeClass *inst = new SomeClass(\"Hi, i am \");\nboost::function< void (std::string)\
    \ > callback;\ncallback = boost::bind(&SomeClass::method, inst, _1);//create delegate\n\
    //_1 is a placeholder it holds plase for parameter\ncallback(\"useless\");//prints\
    \ \"Hi, i am useless\"\n\nYou can create list or vector of functors\nstd::list<\
    \ boost::function<void (EventArg e)> > events;\n//add some events\n....\n//call\
    \ them\nstd::for_each(\n        events.begin(), events.end(), \n        boost::bind(\
    \ boost::apply<void>(), _1, e));\n\nThere is one problem with all this stuff,\
    \ compiler error messages is not human readable :)\n"
- - What are C++ functors and their uses?
  - "\nA Functor is a object which acts like a function.\nBasically, a class which\
    \ defines operator().\nclass MyFunctor\n{\n   public:\n     int operator()(int\
    \ x) { return x * 2;}\n}\n\nMyFunctor doubler;\nint x = doubler(5);\n\nThe real\
    \ advantage is that a functor can hold state. \nclass Matcher\n{\n   int target;\n\
    \   public:\n     Matcher(int m) : target(m) {}\n     bool operator()(int x) {\
    \ return x == target;}\n}\n\nMatcher Is5(5);\n\nif (Is5(n))    // same as if (n\
    \ == 5)\n{ ....}\n\n"
- - What is the difference between 'typedef' and 'using' in C++11?
  - "\nThey are equivalent, from the standard (emphasis mine) (7.1.3.2):\n\nA typedef-name\
    \ can also be introduced by an alias-declaration. The\n  identifier following\
    \ the using keyword becomes a typedef-name and the\n  optional attribute-specifier-seq\
    \ following the identifier appertains\n  to that typedef-name. It has the same\
    \ semantics as if it were\n  introduced by the typedef specifier. In particular,\
    \ it\n  does not define a new type and it shall not appear in the type-id.\n\n"
- - What is the difference between 'typedef' and 'using' in C++11?
  - "\nThe using syntax has an advantage when used within templates. If you need the\
    \ type abstraction, but also need to keep template parameter to be possible to\
    \ be specified in future. You should write something like this.\ntemplate <typename\
    \ T> struct whatever {};\n\ntemplate <typename T> struct rebind\n{\n  typedef\
    \ whatever<T> type; // to make it possible to substitue the whatever in future.\n\
    };\n\nrebind<int>::type variable;\n\ntemplate <typename U> struct bar { typename\
    \ rebind<U>::type _var_member; }\n\nBut using syntax simplifies this use case.\n\
    template <typename T> using my_type = whatever<T>;\n\nmy_type<int> variable;\n\
    template <typename U> struct baz { my_type<U> _var_member; }\n\n"
- - What is the difference between 'typedef' and 'using' in C++11?
  - "\nThey are largely the same, except that:\n\nThe alias declaration is compatible\
    \ with templates, whereas the C\n  style typedef is not.\n\n"
- - Difference between 'struct' and 'typedef struct' in C++?
  - "\nIn C++, there is only a subtle difference.  It's a holdover from C, in which\
    \ it makes a difference.\nThe C language standard (C89 Â§3.1.2.3, C99 Â§6.2.3, and\
    \ C11 Â§6.2.3) mandates separate namespaces for different categories of identifiers,\
    \ including tag identifiers (for struct/union/enum) and ordinary identifiers (for\
    \ typedef and other identifiers). \nIf you just said:\nstruct Foo { ... };\nFoo\
    \ x;\n\nyou would get a compiler error, because Foo is only defined in the tag\
    \ namespace. \nYou'd have to declare it as:\nstruct Foo x;\n\nAny time you want\
    \ to refer to a Foo, you'd always have to call it a struct Foo.  This gets annoying\
    \ fast, so you can add a typedef:\nstruct Foo { ... };\ntypedef struct Foo Foo;\n\
    \nNow struct Foo (in the tag namespace) and just plain Foo (in the ordinary identifier\
    \ namespace) both refer to the same thing, and you can freely declare objects\
    \ of type Foo without the struct keyword.\n\nThe construct:\ntypedef struct Foo\
    \ { ... } Foo;\n\nis just an abbreviation for the declaration and typedef.\n\n\
    Finally,\ntypedef struct { ... } Foo;\n\ndeclares an anonymous structure and creates\
    \ a typedef for it.  Thus, with this construct, it doesn't have a name in the\
    \ tag namespace, only a name in the typedef namespace.  This means it also cannot\
    \ be forward-declared.  If you want to make a forward declaration, you have to\
    \ give it a name in the tag namespace.\n\nIn C++, all struct/union/enum/class\
    \ declarations act like they are implicitly typedef'ed, as long as the name is\
    \ not hidden by another declaration with the same name.  See Michael Burr's answer\
    \ for the full details.\n"
- - Difference between 'struct' and 'typedef struct' in C++?
  - "\nIn this DDJ article, Dan Saks explains one small area where bugs can creep\
    \ through if you do not typedef your structs (and classes!):\n\nIf you want, you\
    \ can imagine that C++\n  generates a typedef for every tag\n  name, such as\n\
    typedef class string string;\n\nUnfortunately, this is not entirely\n  accurate.\
    \ I wish it were that simple,\n  but it's not. C++ can't generate such\n  typedefs\
    \ for structs, unions, or enums\n  without introducing incompatibilities\n  with\
    \ C.\nFor example, suppose a C program\n  declares both a function and a struct\n\
    \  named status:\nint status(); struct status;\n\nAgain, this may be bad practice,\
    \ but\n  it is C. In this program, status (by\n  itself) refers to the function;\
    \ struct\n  status refers to the type.\nIf C++ did automatically generate\n  typedefs\
    \ for tags, then when you\n  compiled this program as C++, the\n  compiler would\
    \ generate:\ntypedef struct status status;\n\nUnfortunately, this type name would\n\
    \  conflict with the function name, and\n  the program would not compile. That's\n\
    \  why C++ can't simply generate a\n  typedef for each tag.\nIn C++, tags act\
    \ just like typedef\n  names, except that a program can\n  declare an object,\
    \ function, or\n  enumerator with the same name and the\n  same scope as a tag.\
    \ In that case, the\n  object, function, or enumerator name\n  hides the tag name.\
    \ The program can\n  refer to the tag name only by using\n  the keyword class,\
    \ struct, union, or\n  enum (as appropriate) in front of the\n  tag name. A type\
    \ name consisting of\n  one of these keywords followed by a\n  tag is an elaborated-type-specifier.\n\
    \  For instance, struct status and enum\n  month are elaborated-type-specifiers.\
    \ \nThus, a C program that contains both:\nint status(); struct status;\n\nbehaves\
    \ the same when compiled as C++.\n  The name status alone refers to the\n  function.\
    \ The program can refer to the\n  type only by using the\n  elaborated-type-specifier\
    \ struct\n  status.\nSo how does this allow bugs to creep\n  into programs? Consider\
    \ the program in\n  Listing 1. This program defines a\n  class foo with a default\
    \ constructor,\n  and a conversion operator that\n  converts a foo object to char\
    \ const *.\n  The expression\np = foo();\n\nin main should construct a foo object\n\
    \  and apply the conversion operator. The\n  subsequent output statement\ncout\
    \ << p << '\\n';\n\nshould display class foo, but it\n  doesn't. It displays function\
    \ foo.\nThis surprising result occurs because\n  the program includes header lib.h\n\
    \  shown in Listing 2. This header\n  defines a function also named foo. The\n\
    \  function name foo hides the class name\n  foo, so the reference to foo in main\n\
    \  refers to the function, not the class.\n  main can refer to the class only\
    \ by\n  using an elaborated-type-specifier, as\n  in\np = class foo();\n\nThe\
    \ way to avoid such confusion\n  throughout the program is to add the\n  following\
    \ typedef for the class name\n  foo:\ntypedef class foo foo;\n\nimmediately before\
    \ or after the class\n  definition. This typedef causes a\n  conflict between\
    \ the type name foo and\n  the function name foo (from the\n  library) that will\
    \ trigger a\n  compile-time error.\nI know of no one who actually writes\n  these\
    \ typedefs as a matter of course.\n  It requires a lot of discipline. Since\n\
    \  the incidence of errors such as the\n  one in Listing 1 is probably pretty\n\
    \  small, you many never run afoul of\n  this problem. But if an error in your\n\
    \  software might cause bodily injury,\n  then you should write the typedefs no\n\
    \  matter how unlikely the error. \nI can't imagine why anyone would ever\n  want\
    \ to hide a class name with a\n  function or object name in the same\n  scope\
    \ as the class. The hiding rules\n  in C were a mistake, and they should\n  not\
    \ have been extended to classes in\n  C++. Indeed, you can correct the\n  mistake,\
    \ but it requires extra\n  programming discipline and effort that\n  should not\
    \ be necessary.\n\n"
- - Difference between 'struct' and 'typedef struct' in C++?
  - "\nOne more important difference: typedefs cannot be forward declared. So for\
    \ the typedef option you must #include the file containing the typedef, meaning\
    \ everything that #includes your .h also includes that file whether it directly\
    \ needs it or not, and so on. It can definitely impact your build times on larger\
    \ projects.\nWithout the typedef, in some cases you can just add a forward declaration\
    \ of struct Foo; at the top of your .h file, and only #include the struct definition\
    \ in your .cpp file.\n"
- - What is the difference between g++ and gcc?
  - "\ngcc and g++ are compiler-drivers of the GNU Compiler Collection (which was\
    \ once upon a time just the GNU C Compiler).\nEven though they automatically determine\
    \ which backends (cc1 cc1plus ...) to call depending on the file-type, unless\
    \ overridden with -x language, they have some differences.\nThe probably most\
    \ important difference in their defaults is which libraries they link against\
    \ automatically.\nAccording to GCC's online documentation link options and how\
    \ g++ is invoked, g++ is equivalent to gcc -xc++ -lstdc++ -shared-libgcc (the\
    \ 1st is a compiler option, the 2nd two are linker options). This can be checked\
    \ by running both with the -v option (it displays the backend toolchain commands\
    \ being run).\n"
- - What is the difference between g++ and gcc?
  - "\nGCC: GNU Compiler Collection\n\nReferrers to all the different languages that\
    \ are supported by the GNU compiler.\n\ngcc: GNU CÂ Â Â Â Â  Compiler\ng++: GNU C++\
    \ Compiler\nThe main differences:\n\ngcc will compile: *.c/*.cpp files as C and\
    \ C++ respectively.\ng++ will compile: *.c/*.cpp files but they will all be treated\
    \ as C++ files.\nAlso if you use g++ to link the object files it automatically\
    \ links in the std C++ libraries (gcc does not do this).\ngcc compiling C files\
    \ has fewer predefined macros.\ngcc compiling *.cpp and g++ compiling *.c/*.cpp\
    \ files has a few extra macros.\n\nExtra Macros when compiling *.cpp files:\n\
    #define __GXX_WEAK__ 1\n#define __cplusplus 1\n#define __DEPRECATED 1\n#define\
    \ __GNUG__ 4\n#define __EXCEPTIONS 1\n#define __private_extern__ extern\n\n"
- - What is the difference between g++ and gcc?
  - "\nFor c++ you should use g++.\nIt's the same compiler (e.g. the GNU compiler\
    \ collection). GCC or G++ just choose a different front-end with different default\
    \ options.\nIn a nutshell: if you use g++ the frontend will tell the linker that\
    \ you may want to link with the C++ standard libraries. The gcc frontend won't\
    \ do that (also it could link with them if you pass the right command line options).\
    \ \n"
- - Why is my program slow when looping over exactly 8192 elements?
  - "\nThe difference is caused by the same super-alignment issue from the following\
    \ related questions:\n\nWhy is transposing a matrix of 512x512 much slower than\
    \ transposing a matrix of 513x513?\nMatrix multiplication: Small difference in\
    \ matrix size, large difference in timings\n\nBut that's only because there's\
    \ one other problem with the code.\nStarting from the original loop:\nfor(i=1;i<SIZE-1;i++)\
    \ \n    for(j=1;j<SIZE-1;j++) {\n        res[j][i]=0;\n        for(k=-1;k<2;k++)\
    \ \n            for(l=-1;l<2;l++) \n                res[j][i] += img[j+l][i+k];\n\
    \        res[j][i] /= 9;\n}\n\nFirst notice that the two inner loops are trivial.\
    \ They can be unrolled as follows:\nfor(i=1;i<SIZE-1;i++) {\n    for(j=1;j<SIZE-1;j++)\
    \ {\n        res[j][i]=0;\n        res[j][i] += img[j-1][i-1];\n        res[j][i]\
    \ += img[j  ][i-1];\n        res[j][i] += img[j+1][i-1];\n        res[j][i] +=\
    \ img[j-1][i  ];\n        res[j][i] += img[j  ][i  ];\n        res[j][i] += img[j+1][i\
    \  ];\n        res[j][i] += img[j-1][i+1];\n        res[j][i] += img[j  ][i+1];\n\
    \        res[j][i] += img[j+1][i+1];\n        res[j][i] /= 9;\n    }\n}\n\nSo\
    \ that leaves the two outer-loops that we're interested in.\nNow we can see the\
    \ problem is the same in this question: Why does the order of the loops affect\
    \ performance when iterating over a 2D array?\nYou are iterating the matrix column-wise\
    \ instead of row-wise.\n\nTo solve this problem, you should interchange the two\
    \ loops.\nfor(j=1;j<SIZE-1;j++) {\n    for(i=1;i<SIZE-1;i++) {\n        res[j][i]=0;\n\
    \        res[j][i] += img[j-1][i-1];\n        res[j][i] += img[j  ][i-1];\n  \
    \      res[j][i] += img[j+1][i-1];\n        res[j][i] += img[j-1][i  ];\n    \
    \    res[j][i] += img[j  ][i  ];\n        res[j][i] += img[j+1][i  ];\n      \
    \  res[j][i] += img[j-1][i+1];\n        res[j][i] += img[j  ][i+1];\n        res[j][i]\
    \ += img[j+1][i+1];\n        res[j][i] /= 9;\n    }\n}\n\nThis eliminates all\
    \ the non-sequential access completely so you no longer get random slow-downs\
    \ on large powers-of-two.\n\nCore i7 920 @ 3.5 GHz\nOriginal code:\n8191: 1.499\
    \ seconds\n8192: 2.122 seconds\n8193: 1.582 seconds\n\nInterchanged Outer-Loops:\n\
    8191: 0.376 seconds\n8192: 0.357 seconds\n8193: 0.351 seconds\n\n"
- - Why is my program slow when looping over exactly 8192 elements?
  - "\nThe following tests have been done with Visual C++ compiler as it is used by\
    \ the default Qt Creator install (I guess with no optimization flag). When using\
    \ GCC, there is no big difference between Mystical's version and my \"optimized\"\
    \ code. So the conclusion is that compiler optimizations take care off micro optimization\
    \ better than humans (me at last). I leave the rest of my answer for reference.\n\
    \nIt's not efficient to process images this way. It's better to use single dimension\
    \ arrays. Processing all pixels is the done in one loop. Random access to points\
    \ could be done using:\npointer + (x + y*width)*(sizeOfOnePixel)\n\nIn this particular\
    \ case, it's better to compute and cache the sum of three pixels groups horizontally\
    \ because they are used three times each.\nI've done some tests and I think it's\
    \ worth sharing. Each result is an average of five tests.\nOriginal code by user1615209:\n\
    8193: 4392 ms\n8192: 9570 ms\n\nMystical's version:\n8193: 2393 ms\n8192: 2190\
    \ ms\n\nTwo pass using a 1D array: first pass for horizontal sums, second for\
    \ vertical sum and average.\nTwo pass addressing with three pointers and only\
    \ increments like this:\nimgPointer1 = &avg1[0][0];\nimgPointer2 = &avg1[0][SIZE];\n\
    imgPointer3 = &avg1[0][SIZE+SIZE];\n\nfor(i=SIZE;i<totalSize-SIZE;i++){\n    resPointer[i]=(*(imgPointer1++)+*(imgPointer2++)+*(imgPointer3++))/9;\n\
    }\n\n8193: 938 ms\n8192: 974 ms\n\nTwo pass using a 1D array and addressing like\
    \ this:\nfor(i=SIZE;i<totalSize-SIZE;i++){\n    resPointer[i]=(hsumPointer[i-SIZE]+hsumPointer[i]+hsumPointer[i+SIZE])/9;\n\
    }\n\n8193: 932 ms\n8192: 925 ms\n\nOne pass caching horizontal sums just one row\
    \ ahead so they stay in cache:\n// Horizontal sums for the first two lines\nfor(i=1;i<SIZE*2;i++){\n\
    \    hsumPointer[i]=imgPointer[i-1]+imgPointer[i]+imgPointer[i+1];\n}\n// Rest\
    \ of the computation\nfor(;i<totalSize;i++){\n    // Compute horizontal sum for\
    \ next line\n    hsumPointer[i]=imgPointer[i-1]+imgPointer[i]+imgPointer[i+1];\n\
    \    // Final result\n    resPointer[i-SIZE]=(hsumPointer[i-SIZE-SIZE]+hsumPointer[i-SIZE]+hsumPointer[i])/9;\n\
    }\n\n8193: 599 ms\n8192: 652 ms\n\nConclusion:\n\nNo benefits of using several\
    \ pointers and just increments (I thought it would have been faster)\nCaching\
    \ horizontal sums is better than computing them several time.\nTwo pass is not\
    \ three times faster, two times only.\nIt's possible to achieve 3.6 times faster\
    \ using both a single pass and caching an intermediary result\n\nI'm sure it's\
    \ possible to do much better.\nNOTE\nPlease, note that I wrote this answer to\
    \ target general performance issues rather than the cache problem explained in\
    \ Mystical's excellent answer. At the beginning it was just pseudo code. I was\
    \ asked to do tests in the comments... Here is a completely refactored version\
    \ with tests.\n"
- - What's the best way to trim std::string?
  - "\nEDIT Since c++17, some parts of the standard library were removed. Fortunately,\
    \ starting with c++11, we have lambdas which are a superior solution.\n#include\
    \ <algorithm> \n#include <cctype>\n#include <locale>\n\n// trim from start (in\
    \ place)\nstatic inline void ltrim(std::string &s) {\n    s.erase(s.begin(), std::find_if(s.begin(),\
    \ s.end(), [](int ch) {\n        return !std::isspace(ch);\n    }));\n}\n\n//\
    \ trim from end (in place)\nstatic inline void rtrim(std::string &s) {\n    s.erase(std::find_if(s.rbegin(),\
    \ s.rend(), [](int ch) {\n        return !std::isspace(ch);\n    }).base(), s.end());\n\
    }\n\n// trim from both ends (in place)\nstatic inline void trim(std::string &s)\
    \ {\n    ltrim(s);\n    rtrim(s);\n}\n\n// trim from start (copying)\nstatic inline\
    \ std::string ltrim_copy(std::string s) {\n    ltrim(s);\n    return s;\n}\n\n\
    // trim from end (copying)\nstatic inline std::string rtrim_copy(std::string s)\
    \ {\n    rtrim(s);\n    return s;\n}\n\n// trim from both ends (copying)\nstatic\
    \ inline std::string trim_copy(std::string s) {\n    trim(s);\n    return s;\n\
    }\n\nThanks to https://stackoverflow.com/a/44973498/524503 for bringing up the\
    \ modern solution.\nOriginal answer:\nI tend to use one of these 3 for my trimming\
    \ needs:\n#include <algorithm> \n#include <functional> \n#include <cctype>\n#include\
    \ <locale>\n\n// trim from start\nstatic inline std::string &ltrim(std::string\
    \ &s) {\n    s.erase(s.begin(), std::find_if(s.begin(), s.end(),\n           \
    \ std::not1(std::ptr_fun<int, int>(std::isspace))));\n    return s;\n}\n\n// trim\
    \ from end\nstatic inline std::string &rtrim(std::string &s) {\n    s.erase(std::find_if(s.rbegin(),\
    \ s.rend(),\n            std::not1(std::ptr_fun<int, int>(std::isspace))).base(),\
    \ s.end());\n    return s;\n}\n\n// trim from both ends\nstatic inline std::string\
    \ &trim(std::string &s) {\n    return ltrim(rtrim(s));\n}\n\nThey are fairly self\
    \ explanatory and work very well.\nEDIT: BTW, I have std::ptr_fun in there to\
    \ help disambiguate std::isspace because there is actually a second definition\
    \ which supports locales. This could have been a cast just the same, but I tend\
    \ to like this better.\nEDIT: To address some comments about accepting a parameter\
    \ by reference, modifying and returning it. I Agree. An implementation that I\
    \ would likely prefer would be two sets of functions, one for in place and one\
    \ which makes a copy. A better set of examples would be:\n#include <algorithm>\
    \ \n#include <functional> \n#include <cctype>\n#include <locale>\n\n// trim from\
    \ start (in place)\nstatic inline void ltrim(std::string &s) {\n    s.erase(s.begin(),\
    \ std::find_if(s.begin(), s.end(),\n            std::not1(std::ptr_fun<int, int>(std::isspace))));\n\
    }\n\n// trim from end (in place)\nstatic inline void rtrim(std::string &s) {\n\
    \    s.erase(std::find_if(s.rbegin(), s.rend(),\n            std::not1(std::ptr_fun<int,\
    \ int>(std::isspace))).base(), s.end());\n}\n\n// trim from both ends (in place)\n\
    static inline void trim(std::string &s) {\n    ltrim(s);\n    rtrim(s);\n}\n\n\
    // trim from start (copying)\nstatic inline std::string ltrim_copy(std::string\
    \ s) {\n    ltrim(s);\n    return s;\n}\n\n// trim from end (copying)\nstatic\
    \ inline std::string rtrim_copy(std::string s) {\n    rtrim(s);\n    return s;\n\
    }\n\n// trim from both ends (copying)\nstatic inline std::string trim_copy(std::string\
    \ s) {\n    trim(s);\n    return s;\n}\n\nI am keeping the original answer above\
    \ though for context and in the interest of keeping the high voted answer still\
    \ available.\n"
- - What's the best way to trim std::string?
  - "\nUsing Boost's string algorithms would be easiest:\n#include <boost/algorithm/string.hpp>\n\
    \nstd::string str(\"hello world! \");\nboost::trim_right(str);\n\nstr is now \"\
    hello world!\". There's also trim_left and trim, which trims both sides.\n\nIf\
    \ you add _copy suffix to any of above function names e.g. trim_copy, the function\
    \ will return a trimmed copy of the string instead of modifying it through a reference.\n\
    If you add _if suffix to any of above function names e.g. trim_copy_if, you can\
    \ trim all characters satisfying your custom predicate, as opposed to just whitespaces.\n"
- - What's the best way to trim std::string?
  - "\nUse the following code to right trim (trailing) spaces and tab characters from\
    \ std::strings (ideone):\n// trim trailing spaces\nsize_t endpos = str.find_last_not_of(\"\
    \ \\t\");\nsize_t startpos = str.find_first_not_of(\" \\t\");\nif( std::string::npos\
    \ != endpos )\n{\n    str = str.substr( 0, endpos+1 );\n    str = str.substr(\
    \ startpos );\n}\nelse {\n    str.erase(std::remove(std::begin(str), std::end(str),\
    \ ' '), std::end(str));\n}\n\nAnd just to balance things out, I'll include the\
    \ left trim code too (ideone):\n// trim leading spaces\nsize_t startpos = str.find_first_not_of(\"\
    \ \\t\");\nif( string::npos != startpos )\n{\n    str = str.substr( startpos );\n\
    }\n\n"
- - Why use apparently meaningless do-while and if-else statements in macros?
  - "\nThe do ... while and if ... else are there to make it so that a\nsemicolon\
    \ after your macro always means the same thing.  Let's say you\nhad something\
    \ like your second macro.\n#define BAR(X) f(x); g(x)\n\nNow if you were to use\
    \ BAR(X); in an if ... else statement, where the bodies of the if statement were\
    \ not wrapped in curly brackets, you'd get a bad surprise.\nif (corge)\n  BAR(corge);\n\
    else\n  gralt();\n\nThe above code would expand into\nif (corge)\n  f(corge);\
    \ g(corge);\nelse\n  gralt();\n\nwhich is syntactically incorrect, as the else\
    \ is no longer associated with the if.  It doesn't help to wrap things in curly\
    \ braces within the macro, because a semicolon after the braces is syntactically\
    \ incorrect.\nif (corge)\n  {f(corge); g(corge);};\nelse\n  gralt();\n\nThere\
    \ are two ways of fixing the problem.  The first is to use a comma to sequence\
    \ statements within the macro without robbing it of its ability to act like an\
    \ expression.\n#define BAR(X) f(X), g(X)\n\nThe above version of bar BAR expands\
    \ the above code into what follows, which is syntactically correct.\nif (corge)\n\
    \  f(corge), g(corge);\nelse\n  gralt();\n\nThis doesn't work if instead of f(X)\
    \ you have a more complicated body of code that needs to go in its own block,\
    \ say for example to declare local variables.  In the most general case the solution\
    \ is to use something like do ... while to cause the macro to be a single statement\
    \ that takes a semicolon without confusion.\n#define BAR(X) do { \\\n  int i =\
    \ f(X); \\\n  if (i > 4) g(i); \\\n} while (0)\n\nYou don't have to use do ...\
    \ while, you could cook up something with if ... else as well, although when if\
    \ ... else expands inside of an if ... else it leads to a \"dangling else\", which\
    \ could make an existing dangling else problem even harder to find, as in the\
    \ following code.\nif (corge)\n  if (1) { f(corge); g(corge); } else;\nelse\n\
    \  gralt();\n\nThe point is to use up the semicolon in contexts where a dangling\
    \ semicolon is erroneous.  Of course, it could (and probably should) be argued\
    \ at this point that it would be better to declare BAR as an actual function,\
    \ not a macro.\nIn summary, the do ... while is there to work around the shortcomings\
    \ of the C preprocessor.  When those C style guides tell you to lay off the C\
    \ preprocessor, this is the kind of thing they're worried about.\n"
- - Why use apparently meaningless do-while and if-else statements in macros?
  - "\nMacros are copy/pasted pieces of text the pre-processor will put in the genuine\
    \ code; the macro's author hopes the replacement will produce valid code.\nThere\
    \ are three good \"tips\" to succeed in that:\nHelp the macro behave like genuine\
    \ code\nNormal code is usually ended by a semi-colon. Should the user view code\
    \ not needing one...\ndoSomething(1) ;\nDO_SOMETHING_ELSE(2)  // <== Hey? What's\
    \ this?\ndoSomethingElseAgain(3) ;\n\nThis means the user expects the compiler\
    \ to produce an error if the semi-colon is absent.\nBut the real real good reason\
    \ is that at some time, the macro's author will perhaps need to replace the macro\
    \ with a genuine function (perhaps inlined). So the macro should really behave\
    \ like one.\nSo we should have a macro needing semi-colon.\nProduce a valid code\n\
    As shown in jfm3's answer, sometimes the macro contains more than one instruction.\
    \ And if the macro is used inside a if statement, this will be problematic:\n\
    if(bIsOk)\n   MY_MACRO(42) ;\n\nThis macro could be expanded as:\n#define MY_MACRO(x)\
    \ f(x) ; g(x)\n\nif(bIsOk)\n   f(42) ; g(42) ; // was MY_MACRO(42) ;\n\nThe g\
    \ function will be executed regardless of the value of bIsOk.\nThis means that\
    \ we must have to add a scope to the macro:\n#define MY_MACRO(x) { f(x) ; g(x)\
    \ ; }\n\nif(bIsOk)\n   { f(42) ; g(42) ; } ; // was MY_MACRO(42) ;\n\nProduce\
    \ a valid code 2\nIf the macro is something like:\n#define MY_MACRO(x) int i =\
    \ x + 1 ; f(i) ;\n\nWe could have another problem in the following code:\nvoid\
    \ doSomething()\n{\n    int i = 25 ;\n    MY_MACRO(32) ;\n}\n\nBecause it would\
    \ expand as:\nvoid doSomething()\n{\n    int i = 25 ;\n    int i = 32 + 1 ; f(i)\
    \ ; ; // was MY_MACRO(32) ;\n}\n\nThis code won't compile, of course. So, again,\
    \ the solution is using a scope:\n#define MY_MACRO(x) { int i = x + 1 ; f(i) ;\
    \ }\n\nvoid doSomething()\n{\n    int i = 25 ;\n    { int i = 32 + 1 ; f(i) ;\
    \ } ; // was MY_MACRO(32) ;\n}\n\nThe code behaves correctly again.\nCombining\
    \ semi-colon + scope effects?\nThere is one C/C++ idiom that produces this effect:\
    \ The do/while loop:\ndo\n{\n    // code\n}\nwhile(false) ;\n\nThe do/while can\
    \ create a scope, thus encapsulating the macro's code, and needs a semi-colon\
    \ in the end, thus expanding into code needing one.\nThe bonus?\nThe C++ compiler\
    \ will optimize away the do/while loop, as the fact its post-condition is false\
    \ is known at compile time. This means that a macro like:\n#define MY_MACRO(x)\
    \                                  \\\ndo                                    \
    \               \\\n{                                                    \\\n\
    \    const int i = x + 1 ;                            \\\n    f(i) ; g(i) ;  \
    \                                  \\\n}                                     \
    \               \\\nwhile(false)\n\nvoid doSomething(bool bIsOk)\n{\n   int i\
    \ = 25 ;\n\n   if(bIsOk)\n      MY_MACRO(42) ;\n\n   // Etc.\n}\n\nwill expand\
    \ correctly as\nvoid doSomething(bool bIsOk)\n{\n   int i = 25 ;\n\n   if(bIsOk)\n\
    \      do\n      {\n         const int i = 42 + 1 ; // was MY_MACRO(42) ;\n  \
    \       f(i) ; g(i) ;\n      }\n      while(false) ;\n\n   // Etc.\n}\n\nand is\
    \ then compiled and optimized away as\nvoid doSomething(bool bIsOk)\n{\n   int\
    \ i = 25 ;\n\n   if(bIsOk)\n   {\n      f(43) ; g(43) ;\n   }\n\n   // Etc.\n\
    }\n\n"
- - Why use apparently meaningless do-while and if-else statements in macros?
  - "\n@jfm3 - You have a nice answer to the question.  You might also want to add\
    \ that the macro idiom also prevents the possibly more dangerous (because there's\
    \ no error) unintended behavior with simple 'if' statements:\n#define FOO(x) \
    \ f(x); g(x)\n\nif (test) FOO( baz);\n\nexpands to:\nif (test) f(baz); g(baz);\n\
    \nwhich is syntactically correct so there's no compiler error, but has the probably\
    \ unintended consequence that g() will always be called.\n"
- - std::wstring VS std::string
  - "\nstring? wstring?\nstd::string is a basic_string templated on a char, and std::wstring\
    \ on a wchar_t.\nchar vs. wchar_t\nchar is supposed to hold a character, usually\
    \ an 8-bit character.\nwchar_t is supposed to hold a wide character, and then,\
    \ things get tricky:\nOn Linux, a wchar_t is 4 bytes, while on Windows, it's 2\
    \ bytes.\nWhat about Unicode, then?\nThe problem is that neither char nor wchar_t\
    \ is directly tied to unicode.\nOn Linux?\nLet's take a Linux OS: My Ubuntu system\
    \ is already unicode aware. When I work with a char string, it is natively encoded\
    \ in UTF-8 (i.e. Unicode string of chars). The following code:\n#include <cstring>\n\
    #include <iostream>\n\nint main(int argc, char* argv[])\n{\n   const char text[]\
    \ = \"olÃ©\" ;\n\n\n   std::cout << \"sizeof(char)    : \" << sizeof(char) << std::endl\
    \ ;\n   std::cout << \"text            : \" << text << std::endl ;\n   std::cout\
    \ << \"sizeof(text)    : \" << sizeof(text) << std::endl ;\n   std::cout << \"\
    strlen(text)    : \" << strlen(text) << std::endl ;\n\n   std::cout << \"text(ordinals)\
    \  :\" ;\n\n   for(size_t i = 0, iMax = strlen(text); i < iMax; ++i)\n   {\n \
    \     std::cout << \" \" << static_cast<unsigned int>(\n                     \
    \         static_cast<unsigned char>(text[i])\n                          );\n\
    \   }\n\n   std::cout << std::endl << std::endl ;\n\n   // - - - \n\n   const\
    \ wchar_t wtext[] = L\"olÃ©\" ;\n\n   std::cout << \"sizeof(wchar_t) : \" << sizeof(wchar_t)\
    \ << std::endl ;\n   //std::cout << \"wtext           : \" << wtext << std::endl\
    \ ; <- error\n   std::cout << \"wtext           : UNABLE TO CONVERT NATIVELY.\"\
    \ << std::endl ;\n   std::wcout << L\"wtext           : \" << wtext << std::endl;\n\
    \n   std::cout << \"sizeof(wtext)   : \" << sizeof(wtext) << std::endl ;\n   std::cout\
    \ << \"wcslen(wtext)   : \" << wcslen(wtext) << std::endl ;\n\n   std::cout <<\
    \ \"wtext(ordinals) :\" ;\n\n   for(size_t i = 0, iMax = wcslen(wtext); i < iMax;\
    \ ++i)\n   {\n      std::cout << \" \" << static_cast<unsigned int>(\n       \
    \                       static_cast<unsigned short>(wtext[i])\n              \
    \                );\n   }\n\n   std::cout << std::endl << std::endl ;\n\n   return\
    \ 0;\n}\n\noutputs the following text:\nsizeof(char)    : 1\ntext            :\
    \ olÃ©\nsizeof(text)    : 5\nstrlen(text)    : 4\ntext(ordinals)  : 111 108 195\
    \ 169\n\nsizeof(wchar_t) : 4\nwtext           : UNABLE TO CONVERT NATIVELY.\n\
    wtext           : olï¿½\nsizeof(wtext)   : 16\nwcslen(wtext)   : 3\nwtext(ordinals)\
    \ : 111 108 233\n\nYou'll see the \"olÃ©\" text in char is really constructed by\
    \ four chars: 110, 108, 195 and 169 (not counting the trailing zero). (I'll let\
    \ you study the wchar_t code as an exercise)\nSo, when working with a char on\
    \ Linux, you should usually end up using Unicode without even knowing it. And\
    \ as std::string works with char, so std::string is already unicode-ready.\nNote\
    \ that std::string, like the C string API, will consider the \"olÃ©\" string to\
    \ have 4 characters, not three. So you should be cautious when truncating/playing\
    \ with unicode chars because some combination of chars is forbidden in UTF-8.\n\
    On Windows?\nOn Windows, this is a bit different. Win32 had to support a lot of\
    \ application working with char and on different charsets/codepages produced in\
    \ all the world, before the advent of Unicode.\nSo their solution was an interesting\
    \ one: If an application works with char, then the char strings are encoded/printed/shown\
    \ on GUI labels using the local charset/codepage on the machine. For example,\
    \ \"olÃ©\" would be \"olÃ©\" in a French-localized Windows, but would be something\
    \ different on an cyrillic-localized Windows (\"olÐ¹\" if you use Windows-1251).\
    \ Thus, \"historical apps\" will usually still work the same old way.\nFor Unicode\
    \ based applications, Windows uses wchar_t, which is 2-bytes wide, and is encoded\
    \ in UTF-16, which is Unicode encoded on 2-bytes characters (or at the very least,\
    \ the mostly compatible UCS-2, which is almost the same thing IIRC).\nApplications\
    \ using char are said \"multibyte\" (because each glyph is composed of one or\
    \ more chars), while applications using wchar_t are said \"widechar\" (because\
    \ each glyph is composed of one or two wchar_t. See MultiByteToWideChar and WideCharToMultiByte\
    \ Win32 conversion API for more info.\nThus, if you work on Windows, you badly\
    \ want to use wchar_t (unless you use a framework hiding that, like GTK+ or QT...).\
    \ The fact is that behind the scenes, Windows works with wchar_t strings, so even\
    \ historical applications will have their char strings converted in wchar_t when\
    \ using API like SetWindowText() (low level API function to set the label on a\
    \ Win32 GUI).\nMemory issues?\nUTF-32 is 4 bytes per characters, so there is no\
    \ much to add, if only that a UTF-8 text and UTF-16 text will always use less\
    \ or the same amount of memory than an UTF-32 text (and usually less).\nIf there\
    \ is a memory issue, then you should know than for most western languages, UTF-8\
    \ text will use less memory than the same UTF-16 one.\nStill, for other languages\
    \ (chinese, japanese, etc.), the memory used will be either the same, or slightly\
    \ larger for UTF-8 than for UTF-16.\nAll in all, UTF-16 will mostly use 2 and\
    \ occassionally 4 bytes per characters (unless you're dealing with some kind of\
    \ esoteric language glyphs (Klingon? Elvish?), while UTF-8 will spend from 1 to\
    \ 4 bytes.\nSee http://en.wikipedia.org/wiki/UTF-8#Compared_to_UTF-16 for more\
    \ info.\nConclusion\n\nWhen I should use std::wstring over std::string?\nOn Linux?\
    \ Almost never (Â§).\nOn Windows? Almost always (Â§).\nOn cross-platform code? Depends\
    \ on your toolkit...\n(Â§) : unless you use a toolkit/framework saying otherwise\n\
    Can std::string hold all the ASCII character set including special characters?\n\
    Notice: A std::string is suitable for holding a 'binary' buffer, where a std::wstring\
    \ is not!\nOn Linux? Yes.\nOn Windows? Only special characters available for the\
    \ current locale of the Windows user.\nEdit (After a comment from Johann Gerell):\n\
    a std::string will be enough to handle all char-based strings (each char being\
    \ a number from 0 to 255). But:\n\nASCII is supposed to go from 0 to 127. Higher\
    \ chars are NOT ASCII.\na char from 0 to 127 will be held correctly\na char from\
    \ 128 to 255 will have a signification depending on your encoding (unicode, non-unicode,\
    \ etc.), but it will be able to hold all Unicode glyphs as long as they are encoded\
    \ in UTF-8.\n\nIs std::wstring supported by almost all popular C++ compilers?\n\
    Mostly, with the exception of GCC based compilers that are ported to Windows.\n\
    It works on my g++ 4.3.2 (under Linux), and I used Unicode API on Win32 since\
    \ Visual C++ 6.\nWhat is exactly a wide character?\nOn C/C++, it's a character\
    \ type written wchar_t which is larger than the simple char character type. It\
    \ is supposed to be used to put inside characters whose indices (like Unicode\
    \ glyphs) are larger than 255 (or 127, depending...).\n\n"
- - std::wstring VS std::string
  - "\nI recommend avoiding std::wstring on Windows or elsewhere, except when required\
    \ by the interface, or anywhere near Windows API calls and respective encoding\
    \ conversions as a syntactic sugar. \nMy view is summarized in http://utf8everywhere.org\
    \ of which I am a co-author. \nUnless your application is API-call-centric, e.g.\
    \ mainly UI application, the suggestion is to store Unicode strings in std::string\
    \ and encoded in UTF-8, performing conversion near API calls. The benefits outlined\
    \ in the article outweigh the apparent annoyance of conversion, especially in\
    \ complex applications. This is doubly so for multi-platform and library development.\
    \ \nAnd now, answering your questions:\n\nA few weak reasons. It exists for historical\
    \ reasons, where widechars were believed to be the proper way of supporting Unicode.\
    \ It is now used to interface APIs that prefer UTF-16 strings. I use them only\
    \ in the direct vicinity of such API calls.\nThis has nothing to do with std::string.\
    \ It can hold whatever encoding you put in it. The only question is how You treat\
    \ its content. My recommendation is UTF-8, so it will be able to hold all Unicode\
    \ characters correctly. It's a common practice on Linux, but I think Windows programs\
    \ should do it also.\nNo. \nWide character is a confusing name. In the early days\
    \ of Unicode, there was a belief that a character can be encoded in two bytes,\
    \ hence the name. Today, it stands for \"any part of the character that is two\
    \ bytes long\". UTF-16 is seen as a sequence of such byte pairs (aka Wide characters).\
    \ A character in UTF-16 takes either one or two pairs.\n\n"
- - std::wstring VS std::string
  - "\nSo, every reader here now should have a clear understanding about the facts,\
    \ the situation. If not, then you must read paercebal's outstandingly comprehensive\
    \ answer [btw: thanks!].\nMy pragmatical conclusion is shockingly simple: all\
    \ that C++ (and STL) \"character encoding\" stuff is substantially broken and\
    \ useless. Blame it on Microsoft or not, that will not help anyway.\nMy solution,\
    \ after in-depth investigation, much frustration and the consequential experiences\
    \ is the following:\n\naccept, that you have to be responsible on your own for\
    \ the encoding and conversion stuff (and you will see that much of it is rather\
    \ trivial)\nuse std::string for any UTF-8 encoded strings (just a typedef std::string\
    \ UTF8String)\naccept that such an UTF8String object is just a dumb, but cheap\
    \ container. Do never ever access and/or manipulate characters in it directly\
    \ (no search, replace, and so on). You could, but you really just really, really\
    \ do not want to waste your time writing text manipulation algorithms for multi-byte\
    \ strings! Even if other people already did such stupid things, don't do that!\
    \ Let it be! (Well, there are scenarios where it makes sense... just use the ICU\
    \ library for those).\nuse std::wstring for UCS-2 encoded strings (typedef std::wstring\
    \ UCS2String) - this is a compromise, and a concession to the mess that the WIN32\
    \ API introduced). UCS-2 is sufficient for most of us (more on that later...).\n\
    use UCS2String instances whenever a character-by-character access is required\
    \ (read, manipulate, and so on). Any character-based processing should be done\
    \ in a NON-multibyte-representation. It is simple, fast, easy.\nadd two utility\
    \ functions to convert back & forth between UTF-8 and UCS-2:\nUCS2String ConvertToUCS2(\
    \ const UTF8String &str );\nUTF8String ConvertToUTF8( const UCS2String &str );\n\
    \n\nThe conversions are straightforward, google should help here ...\nThat's it.\
    \ Use UTF8String wherever memory is precious and for all UTF-8 I/O. Use UCS2String\
    \ wherever the string must be parsed and/or manipulated. You can convert between\
    \ those two representations any time.\nAlternatives & Improvements\n\nconversions\
    \ from & to single-byte character encodings (e.g. ISO-8859-1) can be realized\
    \ with help of plain translation tables, e.g. const wchar_t tt_iso88951[256] =\
    \ {0,1,2,...}; and appropriate code for conversion to & from UCS2.\nif UCS-2 is\
    \ not sufficient, than switch to UCS-4 (typedef std::basic_string<uint32_t> UCS2String)\n\
    \nICU or other unicode libraries?\nFor advanced stuff.\n"
- - What does T&& (double ampersand) mean in C++11?
  - "\nIt declares an rvalue reference (standards proposal doc).\nHere's an introduction\
    \ to rvalue references.  \nHere's a fantastic in-depth look at rvalue references\
    \ by one of Microsoft's standard library developers. (But see the Caution in the\
    \ comments following this answer before reading this article.)\nThe biggest difference\
    \ between a C++03 reference (now called an lvalue reference in C++11) is that\
    \ it can bind to an rvalue like a temporary without having to be const.  Thus,\
    \ this syntax is now legal:\nT&& r = T();\n\nrvalue references primarily provide\
    \ for the following:\nMove semantics.  A move constructor and move assignment\
    \ operator can now be defined that takes an rvalue reference instead of the usual\
    \ const-lvalue reference.  A move functions like a copy, except it is not obliged\
    \ to keep the source unchanged; in fact, it usually modifies the source such that\
    \ it no longer owns the moved resources.  This is great for eliminating extraneous\
    \ copies, especially in standard library implementations.\nFor example, a copy\
    \ constructor might look like this:\nfoo(foo const& other)\n{\n    this->length\
    \ = other.length;\n    this->ptr = new int[other.length];\n    copy(other.ptr,\
    \ other.ptr + other.length, this->ptr);\n}\n\nIf this constructor was passed a\
    \ temporary, the copy would be unnecessary because we know the temporary will\
    \ just be destroyed; why not make use of the resources the temporary already allocated?\
    \  In C++03, there's no way to prevent the copy as we cannot determine we were\
    \ passed a temporary.  In C++11, we can overload a move constructor:\nfoo(foo&&\
    \ other)\n{\n   this->length = other.length;\n   this->ptr = other.ptr;\n   other.length\
    \ = 0;\n   other.ptr = nullptr;\n}\n\nNotice the big difference here: the move\
    \ constructor actually modifies its argument.  This would effectively \"move\"\
    \ the temporary into the object being constructed, thereby eliminating the unnecessary\
    \ copy.\nThe move constructor would be used for temporaries and for non-const\
    \ lvalue references that are explicitly converted to rvalue references using the\
    \ std::move function (it just performs the conversion).  The following code both\
    \ invoke the move constructor for f1 and f2:\nfoo f1((foo())); // Move a temporary\
    \ into f1; temporary becomes \"empty\"\nfoo f2 = std::move(f1); // Move f1 into\
    \ f2; f1 is now \"empty\"\n\nPerfect forwarding.  rvalue references allow us to\
    \ properly forward arguments for templated functions.  Take for example this factory\
    \ function:\ntemplate <typename T, typename A1>\nstd::unique_ptr<T> factory(A1&\
    \ a1)\n{\n    return std::unique_ptr<T>(new T(a1));\n}\n\nIf we called factory<foo>(5),\
    \ the argument will be deduced to be int&, which will not bind to a literal 5,\
    \ even if foo's constructor takes an int.  Well, we could instead use A1 const&,\
    \ but what if foo takes the constructor argument by non-const reference?  To make\
    \ a truly generic factory function, we would have to overload factory on A1& and\
    \ on A1 const&.  That might be fine if factory takes 1 parameter type, but each\
    \ additional parameter type would multiply the necessary overload set by 2.  That's\
    \ very quickly unmaintainable.\nrvalue references fix this problem by allowing\
    \ the standard library to define a std::forward function that can properly forward\
    \ lvalue/rvalue references.  For more information about how std::forward works,\
    \ see this excellent answer.\nThis enables us to define the factory function like\
    \ this:\ntemplate <typename T, typename A1>\nstd::unique_ptr<T> factory(A1&& a1)\n\
    {\n    return std::unique_ptr<T>(new T(std::forward<A1>(a1)));\n}\n\nNow the argument's\
    \ rvalue/lvalue-ness is preserved when passed to T's constructor.  That means\
    \ that if factory is called with an rvalue, T's constructor is called with an\
    \ rvalue.  If factory is called with an lvalue, T's constructor is called with\
    \ an lvalue.  The improved factory function works because of one special rule:\n\
    \nWhen the function parameter type is of\n  the form T&& where T is a template\n\
    \  parameter, and the function argument\n  is an lvalue of type A, the type A&\
    \ is\n  used for template argument deduction.\n\nThus, we can use factory like\
    \ so:\nauto p1 = factory<foo>(foo()); // calls foo(foo&&)\nauto p2 = factory<foo>(*p1);\
    \   // calls foo(foo const&)\n\nImportant rvalue reference properties:\n\nFor\
    \ overload resolution, lvalues prefer binding to lvalue references and rvalues\
    \ prefer binding to rvalue references.  Hence why temporaries prefer invoking\
    \ a move constructor / move assignment operator over a copy constructor / assignment\
    \ operator.\nrvalue references will implicitly bind to rvalues and to temporaries\
    \ that are the result of an implicit conversion.  i.e. float f = 0f; int&& i =\
    \ f; is well formed because float is implicitly convertible to int; the reference\
    \ would be to a temporary that is the result of the conversion.\nNamed rvalue\
    \ references are lvalues.  Unnamed rvalue references are rvalues.  This is important\
    \ to understand why the std::move call is necessary in: foo&& r = foo(); foo f\
    \ = std::move(r);\n\n"
- - What does T&& (double ampersand) mean in C++11?
  - "\nIt denotes an rvalue reference. Rvalue references will only bind to temporary\
    \ objects, unless explicitly generated otherwise. They are used to make objects\
    \ much more efficient under certain circumstances, and to provide a facility known\
    \ as perfect forwarding, which greatly simplifies template code.\nIn C++03, you\
    \ can't distinguish between a copy of a non-mutable lvalue and an rvalue.\nstd::string\
    \ s;\nstd::string another(s);           // calls std::string(const std::string&);\n\
    std::string more(std::string(s)); // calls std::string(const std::string&);\n\n\
    In C++0x, this is not the case.\nstd::string s;\nstd::string another(s);     \
    \      // calls std::string(const std::string&);\nstd::string more(std::string(s));\
    \ // calls std::string(std::string&&);\n\nConsider the implementation behind these\
    \ constructors. In the first case, the string has to perform a copy to retain\
    \ value semantics, which involves a new heap allocation. However, in the second\
    \ case, we know in advance that the object which was passed in to our constructor\
    \ is immediately due for destruction, and it doesn't have to remain untouched.\
    \ We can effectively just swap the internal pointers and not perform any copying\
    \ at all in this scenario, which is substantially more efficient. Move semantics\
    \ benefit any class which has expensive or prohibited copying of internally referenced\
    \ resources. Consider the case of std::unique_ptr- now that our class can distinguish\
    \ between temporaries and non-temporaries, we can make the move semantics work\
    \ correctly so that the unique_ptr cannot be copied but can be moved, which means\
    \ that std::unique_ptr can be legally stored in Standard containers, sorted, etc,\
    \ whereas C++03's std::auto_ptr cannot.\nNow we consider the other use of rvalue\
    \ references- perfect forwarding. Consider the question of binding a reference\
    \ to a reference.\nstd::string s;\nstd::string& ref = s;\n(std::string&)& anotherref\
    \ = ref; // usually expressed via template\n\nCan't recall what C++03 says about\
    \ this, but in C++0x, the resultant type when dealing with rvalue references is\
    \ critical. An rvalue reference to a type T, where T is a reference type, becomes\
    \ a reference of type T.\n(std::string&)&& ref // ref is std::string&\n(const\
    \ std::string&)&& ref // ref is const std::string&\n(std::string&&)&& ref // ref\
    \ is std::string&&\n(const std::string&&)&& ref // ref is const std::string&&\n\
    \nConsider the simplest template function- min and max. In C++03 you have to overload\
    \ for all four combinations of const and non-const manually. In C++0x it's just\
    \ one overload. Combined with variadic templates, this enables perfect forwarding.\n\
    template<typename A, typename B> auto min(A&& aref, B&& bref) {\n    // for example,\
    \ if you pass a const std::string& as first argument,\n    // then A becomes const\
    \ std::string& and by extension, aref becomes\n    // const std::string&, completely\
    \ maintaining it's type information.\n    if (std::forward<A>(aref) < std::forward<B>(bref))\n\
    \        return std::forward<A>(aref);\n    else\n        return std::forward<B>(bref);\n\
    }\n\nI left off the return type deduction, because I can't recall how it's done\
    \ offhand, but that min can accept any combination of lvalues, rvalues, const\
    \ lvalues.\n"
- - What does T&& (double ampersand) mean in C++11?
  - "\nThe term for T&& when used with type deduction (such as for perfect forwarding)\
    \ is known colloquially as a forwarding reference.  The term \"universal reference\"\
    \ was coined by Scott Meyers in this article, but was later changed.\nThat is\
    \ because it may be either r-value or l-value.\nExamples are:\n// template\ntemplate<class\
    \ T> foo(T&& t) { ... }\n\n// auto\nauto&& t = ...;\n\n// typedef\ntypedef ...\
    \ T;\nT&& t = ...;\n\n// decltype\ndecltype(...)&& t = ...;\n\nMore discussion\
    \ can be found in the answer for: Syntax for universal references\n"
- - How to convert std::string to lower case?
  - "\nFrom this:\n#include <algorithm>\n#include <string> \n\nstd::string data =\
    \ \"Abc\"; \nstd::transform(data.begin(), data.end(), data.begin(), ::tolower);\n\
    \nYou're really not going to get away with iterating through each character. \
    \ There's no way to know whether the character is lowercase or uppercase otherwise.\n\
    If you really hate tolower(), here's a non-portable alternative that I don't recommend\
    \ you use:\nchar easytolower(char in) {\n  if(in <= 'Z' && in >= 'A')\n    return\
    \ in - ('Z' - 'z');\n  return in;\n}\n\nstd::transform(data.begin(), data.end(),\
    \ data.begin(), easytolower);\n\nBe aware that ::tolower() can only do a per-single-byte-character\
    \ substitution, which is ill-fitting for many scripts, especially if using a multi-byte-encoding\
    \ like UTF-8.\n"
- - How to convert std::string to lower case?
  - "\nThere is a Boost string algorithm for this:\n#include <boost/algorithm/string.hpp>\
    \    \n\nstd::string str = \"HELLO, WORLD!\";\nboost::algorithm::to_lower(str);\
    \ // modifies str\n\nOr, for non-in-place:\n#include <boost/algorithm/string.hpp>\
    \    \n\nconst std::string str = \"HELLO, WORLD!\";\nconst std::string lower_str\
    \ = boost::algorithm::to_lower_copy(str);\n\n"
- - How to convert std::string to lower case?
  - "\ntl;dr\nUse the ICU library.\n\nFirst you have to answer a question: What is\
    \ the encoding of your std::string? Is it ISO-8859-1? Or perhaps ISO-8859-8? Or\
    \ Windows Codepage 1252? Does whatever you're using to convert upper-to-lowercase\
    \ know that? (Or does it fail miserably for characters over 0x7f?)\nIf you are\
    \ using UTF-8 (the only sane choice among the 8-bit encodings) with std::string\
    \ as container, you are already deceiving yourself into believing that you are\
    \ still in control of things, because you are storing a multibyte character sequence\
    \ in a container that is not aware of the multibyte concept. Even something as\
    \ simple as .substr() is a ticking timebomb. (Because splitting a multibyte sequence\
    \ will result in an invalid (sub-) string.)\nAnd as soon as you try something\
    \ like std::toupper( 'Ã' ), in any encoding, you are in deep trouble. (Because\
    \ it's simply not possible to do this \"right\" with the standard library, which\
    \ can only deliver one result character, not the \"SS\" needed here.) [1] Another\
    \ example would be std::tolower( 'I' ), which should yield different results depending\
    \ on the locale. In Germany, 'i' would be correct; in Turkey, 'Ä±' (LATIN SMALL\
    \ LETTER DOTLESS I) is the expected result.\nThen there is the point that the\
    \ standard library is depending on which locales are supported on the machine\
    \ your software is running on... and what do you do if it isn't?\nSo what you\
    \ are really looking for is a string class that is capable of dealing with all\
    \ this correctly, and that is not std::string.\n(C++11 note: std::u16string and\
    \ std::u32string are better, but still not perfect.)\nWhile Boost looks nice,\
    \ API wise, Boost.Locale is basically a wrapper around ICU. If Boost is compiled\
    \ with ICU support... if it isn't, Boost.Locale is limited to the locale support\
    \ compiled for the standard library.\nAnd believe me, getting Boost to compile\
    \ with ICU can be a real pain sometimes. (There are no pre-compiled binaries for\
    \ Windows, so you'd have to supply them together with your application, and that\
    \ opens a whole new can of worms...)\nSo personally I would recommend getting\
    \ full Unicode support straight from the horse's mouth and using the ICU library\
    \ directly:\n#include <unicode/unistr.h>\n#include <unicode/ustream.h>\n#include\
    \ <unicode/locid.h>\n\n#include <iostream>\n\nint main()\n{\n    char const *\
    \ someString = \"Eidenges\\xe4\\xdf\";\n    icu::UnicodeString someUString( someString,\
    \ \"ISO-8859-1\" );\n    // Setting the locale explicitly here for completeness.\n\
    \    // Usually you would use the user-specified system locale.\n    std::cout\
    \ << someUString.toLower( \"de_DE\" ) << \"\\n\";\n    std::cout << someUString.toUpper(\
    \ \"de_DE\" ) << \"\\n\";\n    return 0;\n}\n\nCompile (with G++ in this example):\n\
    g++ -Wall example.cpp -licuuc -licuio\n\nThis gives:\neidengesÃ¤Ã\nEIDENGESÃSS\n\
    \n\n[1] In 2017, the Council for German Orthography ruled that \"áº\" U+1E9E LATIN\
    \ CAPITAL LETTER SHARP S could be used officially, as an option beside the traditional\
    \ \"SS\" conversion to avoid ambiguity e.g. in passports (where names are capitalized).\
    \ My beautiful go-to example, made obsolete by committee decision... \n"
- - What does the C++ standard state the size of int, long type to be?
  - "\nThe C++ standard does not specify the size of integral types in bytes, but\
    \ it specifies minimum ranges they must be able to hold. You can infer minimum\
    \ size in bits from the required range. You can infer minimum size in bytes from\
    \ that and the value of the CHAR_BIT macro that defines the number of bits in\
    \ a byte (in all but the most obscure platforms it's 8, and it can't be less than\
    \ 8).\nOne additional constraint for char is that its size is always 1 byte, or\
    \ CHAR_BIT bits (hence the name).\nMinimum ranges required by the standard (page\
    \ 22) are:\nand Data Type Ranges on MSDN:\n\nsigned char: -127 to 127 (note, not\
    \ -128 to 127; this accommodates 1's-complement and sign-and-magnitude platforms)\n\
    unsigned char: 0 to 255\n\"plain\" char: same range as signed char or unsigned\
    \ char, implementation-defined\nsigned short: -32767 to 32767\nunsigned short:\
    \ 0 to 65535\nsigned int: -32767 to 32767\nunsigned int: 0 to 65535\nsigned long:\
    \ -2147483647 to 2147483647\nunsigned long: 0 to 4294967295\nsigned long long:\
    \ -9223372036854775807 to 9223372036854775807\nunsigned long long: 0 to 18446744073709551615\n\
    \nA C++ (or C) implementation can define the size of a type in bytes sizeof(type)\
    \ to any value, as long as\n\nthe expression sizeof(type) * CHAR_BIT evaluates\
    \ to a number of bits high enough to contain required ranges, and\nthe ordering\
    \ of type is still valid (e.g. sizeof(int) <= sizeof(long)).\n\nThe actual implementation-specific\
    \ ranges can be found in <limits.h> header in C, or <climits> in C++ (or even\
    \ better, templated std::numeric_limits in <limits> header).\nFor example, this\
    \ is how you will find maximum range for int:\nC:\n#include <limits.h>\nconst\
    \ int min_int = INT_MIN;\nconst int max_int = INT_MAX;\n\nC++:\n#include <limits>\n\
    const int min_int = std::numeric_limits<int>::min();\nconst int max_int = std::numeric_limits<int>::max();\n\
    \n"
- - What does the C++ standard state the size of int, long type to be?
  - "\nFor 32-bit systems, the 'de facto' standard is ILP32 â that is, int, long and\
    \ pointer are all 32-bit quantities.\nFor 64-bit systems, the primary Unix 'de\
    \ facto' standard is LP64 â long and pointer are 64-bit (but int is 32-bit). \
    \ The Windows 64-bit standard is LLP64 â long long and pointer are 64-bit (but\
    \ long and int are both 32-bit).\nAt one time, some Unix systems used an ILP64\
    \ organization.\nNone of these de facto standards is legislated by the C standard\
    \ (ISO/IEC 9899:1999), but all are permitted by it.\nAnd, by definition, sizeof(char)\
    \ is 1, notwithstanding the test in the Perl configure script.\nNote that there\
    \ were machines (Crays) where CHAR_BIT was much larger than 8.  That meant, IIRC,\
    \ that sizeof(int) was also 1, because both char and int were 32-bit.\n"
- - What does the C++ standard state the size of int, long type to be?
  - "\nIn practice there's no such thing. Often you can expect std::size_t to  represent\
    \ the unsigned native integer size on current architecture. i.e. 16-bit, 32-bit\
    \ or 64-bit but it isn't always the case as pointed out in the comments to this\
    \ answer.\nAs far as all the other built-in types go, it really depends on the\
    \ compiler. Here's two excerpts taken from the current working draft of the latest\
    \ C++ standard:\n\nThere are five standard signed integer types : signed char,\
    \ short int, int, long int, and long long int. In this list, each type provides\
    \ at least as much storage as those preceding it in the list.\nFor each of the\
    \ standard signed integer types, there exists a corresponding (but different)\
    \ standard unsigned integer type: unsigned char, unsigned short int, unsigned\
    \ int, unsigned long int, and unsigned long long int, each of which occupies the\
    \ same amount of storage and has the same alignment requirements.\n\nIf you want\
    \ to you can statically (compile-time) assert the sizeof these fundamental types.\
    \ It will alert people to think about porting your code if the sizeof assumptions\
    \ change. \n"
- - What is a âcache-friendlyâ code?
  - "\nPreliminaries\nOn modern computers, only the lowest level memory structures\
    \ (the registers) can move data around in single clock cycles. However, registers\
    \ are very expensive and most computer cores have less than a few dozen registers\
    \ (few hundred to maybe a thousand bytes total).  At the other end of the memory\
    \ spectrum (DRAM), the memory is very cheap (i.e. literally millions of times\
    \ cheaper) but takes hundreds of cycles after a request to receive the data. \
    \ To bridge this gap between super fast and expensive and super slow and cheap\
    \ are the cache memories, named L1, L2, L3 in decreasing speed and cost. The idea\
    \ is that most of the executing code will be hitting a small set of variables\
    \ often, and the rest (a much larger set of variables) infrequently. If the processor\
    \ can't find the data in L1 cache, then it looks in L2 cache. If not there, then\
    \ L3 cache, and if not there, main memory. Each of these \"misses\" is expensive\
    \ in time.\n(The analogy is cache memory is to system memory, as system memory\
    \ is to hard disk storage. Hard disk storage is super cheap, but very slow).\n\
    Caching is one of the main methods to reduce the impact of latency. To paraphrase\
    \ Herb Sutter (cfr. links below): increasing bandwidth is easy, but we can't buy\
    \ our way out of latency.\nData is always retrieved through the memory hierarchy\
    \ (smallest == fastest to slowest). A cache hit/miss usually refers to a hit/miss\
    \ in the highest level of cache in the CPU -- by highest level I mean the largest\
    \ == slowest. The cache hit rate is crucial for performance, since every cache\
    \ miss results in fetching data from RAM (or worse ...) which takes a lot of time\
    \ (hundreds of cycles for RAM, tens of millions of cycles for HDD). In comparison,\
    \ reading data from the (highest level) cache typically takes only a handful of\
    \ cycles.\nIn modern computer architectures, the performance bottleneck is leaving\
    \ the CPU die (e.g. accessing RAM or higher). This will only get worse over time.\
    \ The increase in processor frequency is currently no longer relevant to increase\
    \ performance. The problem is memory access. Hardware design efforts in CPUs therefore\
    \ currently focus heavily on optimizing caches, prefetching, pipelines and concurrency.\
    \ For instance, modern CPUs spend around 85% of die on caches and up to 99% for\
    \ storing/moving data!\nThere is quite a lot to be said on the subject. Here are\
    \ a few great references about caches, memory hierarchies and proper programming:\n\
    \nAgner Fog's page. In his excellent documents, you can find detailed examples\
    \ covering languages ranging from assembly to C++. \nIf you are into videos, I\
    \ strongly recommend to have a look at  Herb Sutter's talk on machine architecture\
    \ (youtube) (specifically check 12:00 and onwards!).\nSlides about memory optimization\
    \ by Christer Ericson (director of technology @ Sony)\nLWN.net's article \"What\
    \ every programmer should know about memory\"\n\nMain concepts for cache-friendly\
    \ code\nA very important aspect of cache-friendly code is all about the principle\
    \ of locality, the goal of which is to place related data close in memory to allow\
    \ efficient caching. In terms of the CPU cache, it's important to be aware of\
    \ cache lines to understand how this works: How do cache lines work? \nThe following\
    \ particular aspects are of high importance to optimize caching:\n\nTemporal locality:\
    \ when a given memory location was accessed, it is likely that the same location\
    \ is accessed again in the near future. Ideally, this information will still be\
    \ cached at that point.\nSpatial locality: this refers to placing related data\
    \ close to eachother. Caching happens on many levels, not just in the CPU. For\
    \ example, when you read from RAM, typically a larger chunk of memory is fetched\
    \ than what was specifically asked for because very often the program will require\
    \ that data soon. HDD caches follow the same line of thought. Specifically for\
    \ CPU caches, the notion of cache lines is important.\n\nUse appropriate c++ containers\n\
    A simple example of cache-friendly versus cache-unfriendly is c++'s std::vector\
    \ versus std::list. Elements of a std::vector are stored in contiguous memory,\
    \ and as such accessing them is much more cache-friendly than accessing elements\
    \ in a std::list, which stores its content all over the place. This is due to\
    \ spatial locality.\nA very nice illustration of this is given by Bjarne Stroustrup\
    \ in this youtube clip (thanks to @Mohammad Ali Baydoun for the link!).\nDon't\
    \ neglect the cache in data structure and algorithm design\nWhenever possible,\
    \ try to adapt your data structures and order of computations in a way that allows\
    \ maximum use of the cache. An common technique in this regard is cache blocking\
    \ (Archive.org version), which is of extreme importance in high-performance computing\
    \ (cfr. for example ATLAS).\nKnow and exploit the implicit structure of data\n\
    Another simple example, which many people in the field sometimes forget is column-major\
    \ (ex. fortran,matlab) vs. row-major ordering (ex. c,c++) for storing two dimensional\
    \ arrays. For example, consider the following matrix:\n1 2\n3 4\n\nIn row-major\
    \ ordering, this is stored in memory as 1 2 3 4; in column-major ordering this\
    \ would be stored as 1 3 2 4. It is easy to see that implementations which do\
    \ not exploit this ordering will quickly run into (easily avoidable!) cache issues.\
    \ Unfortunately, I see stuff like this very often in my domain (machine learning).\
    \ @MatteoItalia showed this example in more detail in his answer.\nWhen fetching\
    \ a certain element of a matrix from memory, elements near it will be fetched\
    \ as well and stored in a cache line. If the ordering is exploited, this will\
    \ result in fewer memory accesses (because the next few values which are needed\
    \ for subsequent computations are already in a cache line). \nFor simplicity,\
    \ assume the cache comprises a single cache line which can contain 2 matrix elements\
    \ and that when a given element is fetched from memory, the next one is too. Say\
    \ we want to take the sum over all elements in the example 2x2 matrix above (lets\
    \ call it M):\nExploiting the ordering (e.g. changing column index first in c++):\
    \ \nM[0][0] (memory) + M[0][1] (cached) + M[1][0] (memory) + M[1][1] (cached)\n\
    = 1 + 2 + 3 + 4\n--> 2 cache hits, 2 memory accesses\n\nNot exploiting the ordering\
    \ (e.g. changing row index first in c++):\nM[0][0] (memory) + M[1][0] (memory)\
    \ + M[0][1] (memory) + M[1][1] (memory)\n= 1 + 3 + 2 + 4\n--> 0 cache hits, 4\
    \ memory accesses\n\nIn this simple example, exploiting the ordering approximately\
    \ doubles execution speed (since memory access requires much more cycles than\
    \ computing the sums). In practice the performance difference can be much larger.\n\
    Avoid unpredictable branches\nModern architectures feature pipelines and compilers\
    \ are becoming very good at reordering code to minimize delays due to memory access.\
    \ When your critical code contains (unpredictable) branches, it is hard or impossible\
    \ to prefetch data. This will indirectly lead to more cache misses.\nThis is explained\
    \ very well here (thanks to @0x90 for the link): Why is it faster to process a\
    \ sorted array than an unsorted array?\nAvoid virtual functions\nIn the context\
    \ of c++, virtual methods represent a controversial issue with regard to cache\
    \ misses (a general consensus exists that they should be avoided when possible\
    \ in terms of performance). Virtual functions can induce cache misses during look\
    \ up, but this only happens if the specific function is not called often (otherwise\
    \ it would likely be cached), so this is regarded as a non-issue by some. For\
    \ reference about this issue, check out: What is the performance cost of having\
    \ a virtual method in a C++ class? \nCommon problems\nA common problem in modern\
    \ architectures with multiprocessor caches is called false sharing. This occurs\
    \ when each individual processor is attempting to use data in another memory region\
    \ and attempts to store it in the same cache line. This causes the cache line\
    \ -- which contains data another processor can use -- to be overwritten again\
    \ and again. Effectively, different threads make each other wait by inducing cache\
    \ misses in this situation.\nSee also (thanks to @Matt for the link): How and\
    \ when to align to cache line size?\nAn extreme symptom of poor caching in RAM\
    \ memory (which is probably not what you mean in this context) is so-called thrashing.\
    \ This occurs when the process continuously generates page faults (e.g. accesses\
    \ memory which is not in the current page) which require disk access.\n"
- - What is a âcache-friendlyâ code?
  - "\nIn addition to @Marc Claesen's answer, I think that an instructive classic\
    \ example of cache-unfriendly code is code that scans a C bidimensional array\
    \ (e.g. a bitmap image) column-wise instead of row-wise.\nElements that are adjacent\
    \ in a row are also adjacent in memory, thus accessing them in sequence means\
    \ accessing them in ascending memory order; this is cache-friendly, since the\
    \ cache tends to prefetch contiguous blocks of memory.\nInstead, accessing such\
    \ elements column-wise is cache-unfriendly, since elements on the same column\
    \ are distant in memory from each other (in particular, their distance is equal\
    \ to the size of the row), so when you use this access pattern you are jumping\
    \ around in memory, potentially wasting the effort of the cache of retrieving\
    \ the elements nearby in memory.\nAnd all that it takes to ruin the performance\
    \ is to go from\n// Cache-friendly version - processes pixels which are adjacent\
    \ in memory\nfor(unsigned int y=0; y<height; ++y)\n{\n    for(unsigned int x=0;\
    \ x<width; ++x)\n    {\n        ... image[y][x] ...\n    }\n}\n\nto\n// Cache-unfriendly\
    \ version - jumps around in memory for no good reason\nfor(unsigned int x=0; x<width;\
    \ ++x)\n{\n    for(unsigned int y=0; y<height; ++y)\n    {\n        ... image[y][x]\
    \ ...\n    }\n}\n\nThis effect can be quite dramatic (several order of magnitudes\
    \ in speed) in systems with small caches and/or working with big arrays (e.g.\
    \ 10+ megapixels 24 bpp images on current machines); for this reason, if you have\
    \ to do many vertical scans, often it's better to rotate the image of 90 degrees\
    \ first and perform the various analysis later, limiting the cache-unfriendly\
    \ code just to the rotation.\n"
- - What is a âcache-friendlyâ code?
  - "\nOptimizing cache usage largely comes down to two factors.\nLocality of Reference\n\
    The first factor (to which others have already alluded) is locality of reference.\
    \ Locality of reference really has two dimensions though: space and time.\n\n\
    Spatial\n\nThe spatial dimension also comes down to two things: first, we want\
    \ to pack our information densely, so more information will fit in that limited\
    \ memory. This means (for example) that you need a major improvement in computational\
    \ complexity to justify data structures based on small nodes joined by pointers.\n\
    Second, we want information that will be processed together also located together.\
    \ A typical cache works in \"lines\", which means when you access some information,\
    \ other information at nearby addresses will be loaded into the cache with the\
    \ part we touched. For example, when I touch one byte, the cache might load 128\
    \ or 256 bytes near that one. To take advantage of that, you generally want the\
    \ data arranged to maximize the likelihood that you'll also use that other data\
    \ that was loaded at the same time.\nFor just a really trivial example, this can\
    \ mean that a linear search can be much more competitive with a binary search\
    \ than you'd expect. Once you've loaded one item from a cache line, using the\
    \ rest of the data in that cache line is almost free. A binary search becomes\
    \ noticeably faster only when the data is large enough that the binary search\
    \ reduces the number of cache lines you access.\n\nTime\n\nThe time dimension\
    \ means that when you do some operations on some data, you want (as much as possible)\
    \ to do all the operations on that data at once.\nSince you've tagged this as\
    \ C++, I'll point to a classic example of a relatively cache-unfriendly design:\
    \ std::valarray. valarray overloads most arithmetic operators, so I can (for example)\
    \ say a = b + c + d; (where a, b, c and d are all valarrays) to do element-wise\
    \ addition of those arrays.\nThe problem with this is that it walks through one\
    \ pair of inputs, puts results in a temporary, walks through another pair of inputs,\
    \ and so on. With a lot of data, the result from one computation may disappear\
    \ from the cache before it's used in the next computation, so we end up reading\
    \ (and writing) the data repeatedly before we get our final result. If each element\
    \ of the final result will be something like (a[n] + b[n]) * (c[n] + d[n]);, we'd\
    \ generally prefer to read each a[n], b[n], c[n] and d[n] once, do the computation,\
    \ write the result, increment n and repeat 'til we're done.2\nLine Sharing\nThe\
    \ second major factor is avoiding line sharing. To understand this, we probably\
    \ need to back up and look a little at how caches are organized. The simplest\
    \ form of cache is direct mapped. This means one address in main memory can only\
    \ be stored in one specific spot in the cache. If we're using two data items that\
    \ map to the same spot in the cache, it works badly -- each time we use one data\
    \ item, the other has to be flushed from the cache to make room for the other.\
    \ The rest of the cache might be empty, but those items won't use other parts\
    \ of the cache.\nTo prevent this, most caches are what are called \"set associative\"\
    . For example, in a 4-way set-associative cache, any item from main memory can\
    \ be stored at any of 4 different places in the cache. So, when the cache is going\
    \ to load an item, it looks for the least recently used3 item among those four,\
    \ flushes it to main memory, and loads the new item in its place.\nThe problem\
    \ is probably fairly obvious: for a direct-mapped cache, two operands that happen\
    \ to map to the same cache location can lead to bad behavior. An N-way set-associative\
    \ cache increases the number from 2 to N+1. Organizing a cache into more \"ways\"\
    \ takes extra circuitry and generally runs slower, so (for example) an 8192-way\
    \ set associative cache is rarely a good solution either.\nUltimately, this factor\
    \ is more difficult to control in portable code though. Your control over where\
    \ your data is placed is usually fairly limited. Worse, the exact mapping from\
    \ address to cache varies between otherwise similar processors. In some cases,\
    \ however, it can be worth doing things like allocating a large buffer, and then\
    \ using only parts of what you allocated to ensure against data sharing the same\
    \ cache lines (even though you'll probably need to detect the exact processor\
    \ and act accordingly to do this).\n\nFalse Sharing\n\nThere's another, related\
    \ item called \"false sharing\". This arises in a multiprocessor or multicore\
    \ system, where two (or more) processors/cores have data that's separate, but\
    \ falls in the same cache line. This forces the two processors/cores to coordinate\
    \ their access to the data, even though each has its own, separate data item.\
    \ Especially if the two modify the data in alternation, this can lead to a massive\
    \ slowdown as the data has to be constantly shuttled between the processors. This\
    \ can't easily be cured by organizing the cache into more \"ways\" or anything\
    \ like that either. The primary way to prevent it is to ensure that two threads\
    \ rarely (preferably never) modify data that could possibly be in the same cache\
    \ line (with the same caveats about difficulty of controlling the addresses at\
    \ which data is allocated).\n\n\nThose who know C++ well might wonder if this\
    \ is open to optimization via something like expression templates. I'm pretty\
    \ sure the answer is that yes, it could be done and if it was, it would probably\
    \ be a pretty substantial win. I'm not aware of anybody having done so, however,\
    \ and given how little valarray gets used, I'd be at least a little surprised\
    \ to see anybody do so either.\nIn case anybody wonders how valarray (designed\
    \ specifically for performance) could be this badly wrong, it comes down to one\
    \ thing: it was really designed for machines like the older Crays, that used fast\
    \ main memory and no cache. For them, this really was a nearly ideal design.\n\
    Yes, I'm simplifying: most caches don't really measure the least recently used\
    \ item precisely, but they use some heuristic that's intended to be close to that\
    \ without having to keep a full time-stamp for each access.\n\n"
- - What is object slicing?
  - "\n\"Slicing\" is where you assign an object of a derived class to an instance\
    \ of a base class, thereby losing part of the information - some of it is \"sliced\"\
    \ away.\nFor example, \nclass A {\n   int foo;\n};\n\nclass B : public A {\n \
    \  int bar;\n};\n\nSo an object of type B has two data members, foo and bar.\n\
    Then if you were to write this:\nB b;\n\nA a = b;\n\nThen the information in b\
    \ about member bar is lost in a.\n"
- - What is object slicing?
  - "\nMost answers here fail to explain what the actual problem with slicing is.\
    \ They only explain the benign cases of slicing, not the treacherous ones. Assume,\
    \ like the other answers, that you're dealing with two classes A and B, where\
    \ B derives (publicly) from A.\nIn this situation, C++ lets you pass an instance\
    \ of B to  A's assignment operator (and also to the copy constructor). This works\
    \ because an instance of B can be converted to a const A&, which is what assignment\
    \ operators and copy-constructors expect their arguments to be.\nThe benign case\n\
    B b;\nA a = b;\n\nNothing bad happens there - you asked for an instance of A which\
    \ is a copy of B, and that's exactly what you get. Sure, a won't contain some\
    \ of b's members, but how should it? It's an A, after all, not a B, so it hasn't\
    \ even heard about these members, let alone would be able to store them.\nThe\
    \ treacherous case\nB b1;\nB b2;\nA& a_ref = b2;\na_ref = b1;\n//b2 now contains\
    \ a mixture of b1 and b2!\n\nYou might think that b2 will be a copy of b1 afterwards.\
    \ But, alas, it's not! If you inspect it, you'll discover that b2 is a Frankensteinian\
    \ creature, made from some chunks of b1 (the chunks that B inherits from A), and\
    \ some chunks of b2 (the chunks that only B contains). Ouch!\nWhat happened? Well,\
    \ C++ by default doesn't treat assignment operators as virtual. Thus, the line\
    \ a_ref = b1 will call the assignment operator of A, not that of B. This is because\
    \ for non-virtual functions, the declared type (which is A&) determines which\
    \ function is called, as opposed to the actual type (which would be B, since a_ref\
    \ references an instance of B). Now, A's assignment operator obviously knows only\
    \ about the members declared in A, so it will copy only those, leaving the members\
    \ added in B unchanged.\nA solution\nAssigning only to parts of an object usually\
    \ makes little sense, yet C++ unfortunately provides no built-in way to forbid\
    \ this. You can, however, roll your own. The first step is making the assignment\
    \ operator virtual. This will guarantee that it's always the actual type's assignment\
    \ operator which is called, not the declared type's. The second step is to use\
    \ dynamic_cast to verify that the assigned object has a compatible type. The third\
    \ step is to do the actual assignment in a (protected!) member assign(), since\
    \ B's assign() will probably want to use A's assign() to copy A's members.\nclass\
    \ A {\npublic:\n  virtual A& operator= (const A& a) {\n    assign(a);\n    return\
    \ *this;\n  }\n\nprotected:\n  void assign(const A& a) {\n    // copy members\
    \ of A from a to this\n  }\n};\n\nclass B : public A {\npublic:\n  virtual B&\
    \ operator= (const A& a) {\n    if (const B* b = dynamic_cast<const B*>(&a))\n\
    \      assign(*b);\n    else\n      throw bad_assignment();\n    return *this;\n\
    \  }\n\nprotected:\n  void assign(const B& b) {\n    A::assign(b); // Let A's\
    \ assign() copy members of A from b to this\n    // copy members of B from b to\
    \ this\n  }\n};\n\nNote that, for pure convenience, B's operator= covariantly\
    \ overrides the return type, since it knows that it's returning an instance of\
    \ B.\n"
- - What is object slicing?
  - "\nIf You have a base class A and a derived class B, then You can do the following.\n\
    void wantAnA(A myA)\n{\n   // work with myA\n}\n\nB derived;\n// work with the\
    \ object \"derived\"\nwantAnA(derived);\n\nNow the method wantAnA needs a copy\
    \ of derived. However, the object derived cannot be copied completely, as the\
    \ class B could invent additional member variables which are not in its base class\
    \ A.\nTherefore, to call wantAnA, the compiler will \"slice off\" all additional\
    \ members of the derived class. The result might be an object you did not want\
    \ to create, because\n\nit may be incomplete,\nit behaves like an A-object (all\
    \ special behaviour of the class B is lost).\n\n"
- - Meaning of 'const' last in a function declaration of a class?
  - "\nWhen you add the const keyword to a method the this pointer will essentially\
    \ become a pointer to const object, and you cannot therefore change any member\
    \ data. (Unless you use mutable, more on that later).\nThe const keyword is part\
    \ of the functions signature which means that you can implement two similar methods,\
    \ one which is called when the object is const, and one that isn't.\n#include\
    \ <iostream>\n\nclass MyClass\n{\nprivate:\n    int counter;\npublic:\n    void\
    \ Foo()\n    { \n        std::cout << \"Foo\" << std::endl;    \n    }\n\n   \
    \ void Foo() const\n    {\n        std::cout << \"Foo const\" << std::endl;\n\
    \    }\n\n};\n\nint main()\n{\n    MyClass cc;\n    const MyClass& ccc = cc;\n\
    \    cc.Foo();\n    ccc.Foo();\n}\n\nThis will output\nFoo\nFoo const\n\nIn the\
    \ non-const method you can change the instance members, which you cannot do in\
    \ the const version. If you change the method declaration in the above example\
    \ to the code below you will get some errors.\n    void Foo()\n    {\n       \
    \ counter++; //this works\n        std::cout << \"Foo\" << std::endl;    \n  \
    \  }\n\n    void Foo() const\n    {\n        counter++; //this will not compile\n\
    \        std::cout << \"Foo const\" << std::endl;\n    }\n\nThis is not completely\
    \ true, because you can mark a member as mutable and a const method can then change\
    \ it. It's mostly used for internal counters and stuff. The solution for that\
    \ would be the below code.\n#include <iostream>\n\nclass MyClass\n{\nprivate:\n\
    \    mutable int counter;\npublic:\n\n    MyClass() : counter(0) {}\n\n    void\
    \ Foo()\n    {\n        counter++;\n        std::cout << \"Foo\" << std::endl;\
    \    \n    }\n\n    void Foo() const\n    {\n        counter++;\n        std::cout\
    \ << \"Foo const\" << std::endl;\n    }\n\n    int GetInvocations() const\n  \
    \  {\n        return counter;\n    }\n};\n\nint main(void)\n{\n    MyClass cc;\n\
    \    const MyClass& ccc = cc;\n    cc.Foo();\n    ccc.Foo();\n    std::cout <<\
    \ \"The MyClass instance has been invoked \" << ccc.GetInvocations() << \" times\"\
    \ << endl;\n}\n\nwhich would output\nFoo\nFoo const\nThe MyClass instance has\
    \ been invoked 2 times\n\n"
- - Meaning of 'const' last in a function declaration of a class?
  - "\nThe const means that the method promises not to alter any members of the class.\
    \ You'd be able to execute the object's members that are so marked, even if the\
    \ object itself were marked const:\nconst foobar fb;\nfb.foo();\n\nwould be legal.\n\
    See How many and which are the uses of âconstâ in C++? for more information.\n"
- - Meaning of 'const' last in a function declaration of a class?
  - "\nThe const qualifier means that the methods can be called on any value of foobar.\
    \ The difference comes when you consider calling a non-const method on a const\
    \ object. Consider if your foobar type had the following extra method declaration:\n\
    class foobar {\n  ...\n  const char* bar();\n}\n\nThe method bar() is non-const\
    \ and can only be accessed from non-const values.\nvoid func1(const foobar& fb1,\
    \ foobar& fb2) {\n  const char* v1 = fb1.bar();  // won't compile\n  const char*\
    \ v2 = fb2.bar();  // works\n}\n\nThe idea behind const though is to mark methods\
    \ which will not alter the internal state of the class. This is a powerful concept\
    \ but is not actually enforceable in C++.  It's more of a promise than a guarantee.\
    \ And one that is often broken and easily broken.  \nfoobar& fbNonConst = const_cast<foobar&>(fb1);\n\
    \n"
- - Can code that is valid in both C and C++ produce different behavior when compiled
    in each language?
  - "\nThe following, valid in C and C++, is going to (most likely) result in different\
    \ values in i in C and C++:\nint i = sizeof('a');\n\nSee Size of character ('a')\
    \ in C/C++ for an explanation of the difference.\nAnother one from this article:\n\
    #include <stdio.h>\n\nint  sz = 80;\n\nint main(void)\n{\n    struct sz { char\
    \ c; };\n\n    int val = sizeof(sz);      // sizeof(int) in C,\n             \
    \                  // sizeof(struct sz) in C++\n    printf(\"%d\\n\", val);\n\
    \    return 0;\n}\n\n"
- - Can code that is valid in both C and C++ produce different behavior when compiled
    in each language?
  - "\nHere is an example that takes advantage of the difference between function\
    \ calls and object declarations in C and C++, as well as the fact that C90 allows\
    \ the calling of undeclared functions:\n#include <stdio.h>\n\nstruct f { int x;\
    \ };\n\nint main() {\n    f();\n}\n\nint f() {\n    return printf(\"hello\");\n\
    }\n\nIn C++ this will print nothing because a temporary f is created and destroyed,\
    \ but in C90 it will print hello because functions can be called without having\
    \ been declared.\nIn case you were wondering about the name f being used twice,\
    \ the C and C++ standards explicitly allows this, and to make an object you have\
    \ to say struct f to disambiguate if you want the structure, or leave off struct\
    \ if you want the function.\n"
- - Can code that is valid in both C and C++ produce different behavior when compiled
    in each language?
  - "\nFor C++ vs. C90, there's at least one way to get different behavior that's\
    \ not implementation defined. C90 doesn't have single-line comments. With a little\
    \ care, we can use that to create an expression with entirely different results\
    \ in C90 and in C++.\nint a = 10 //* comment */ 2 \n        + 3;\n\nIn C++, everything\
    \ from the // to the end of the line is a comment, so this works out as:\nint\
    \ a = 10 + 3;\n\nSince C90 doesn't have single-line comments, only the /* comment\
    \ */ is a comment. The first / and the 2 are both parts of the initialization,\
    \ so it comes out to:\nint a = 10 / 2 + 3;\n\nSo, a correct C++ compiler will\
    \ give 13, but a strictly correct C90 compiler 8. Of course, I just picked arbitrary\
    \ numbers here -- you can use other numbers as you see fit.\n"
- - C++ Singleton design pattern
  - "\nIn 2008 I provided a C++98 implementation of the Singleton design pattern that\
    \ is lazy-evaluated, guaranteed-destruction, not-technically-thread-safe:\nCan\
    \ any one provide me a sample of Singleton in c++? \nHere is an updated C++11\
    \ implementation of the Singleton design pattern that is lazy-evaluated, correctly-destroyed,\
    \ and thread-safe.\nclass S\n{\n    public:\n        static S& getInstance()\n\
    \        {\n            static S    instance; // Guaranteed to be destroyed.\n\
    \                                  // Instantiated on first use.\n           \
    \ return instance;\n        }\n    private:\n        S() {}                  \
    \  // Constructor? (the {} brackets) are needed here.\n\n        // C++ 03\n \
    \       // ========\n        // Don't forget to declare these two. You want to\
    \ make sure they\n        // are unacceptable otherwise you may accidentally get\
    \ copies of\n        // your singleton appearing.\n        S(S const&);      \
    \        // Don't Implement\n        void operator=(S const&); // Don't implement\n\
    \n        // C++ 11\n        // =======\n        // We can use the better technique\
    \ of deleting the methods\n        // we don't want.\n    public:\n        S(S\
    \ const&)               = delete;\n        void operator=(S const&)  = delete;\n\
    \n        // Note: Scott Meyers mentions in his Effective Modern\n        // \
    \      C++ book, that deleted functions should generally\n        //       be\
    \ public as it results in better error messages\n        //       due to the compilers\
    \ behavior to check accessibility\n        //       before deleted status\n};\n\
    \nSee this article about when to use a singleton: (not often)\nSingleton: How\
    \ should it be used\nSee this two article about initialization order and how to\
    \ cope:\nStatic variables initialisation order\nFinding C++ static initialization\
    \ order problems \nSee this article describing lifetimes:\nWhat is the lifetime\
    \ of a static variable in a C++ function? \nSee this article that discusses some\
    \ threading implications to singletons:\nSingleton instance declared as static\
    \ variable of GetInstance method, is it thread-safe?\nSee this article that explains\
    \ why double checked locking will not work on C++:\nWhat are all the common undefined\
    \ behaviours that a C++ programmer should know about?\nDr Dobbs: C++ and The Perils\
    \ of Double-Checked Locking: Part I\n"
- - C++ Singleton design pattern
  - "\nBeing a Singleton, you usually do not want it to be destructed.\nIt will get\
    \ torn down and deallocated when the program terminates, which is the normal,\
    \ desired behavior for a singleton.  If you want to be able to explicitly clean\
    \ it, it's fairly easy to add a static method to the class that allows you to\
    \ restore it to a clean state, and have it reallocate next time it's used, but\
    \ that's outside of the scope of a \"classic\" singleton.\n"
- - C++ Singleton design pattern
  - "\nYou could avoid memory allocation. There are many variants, all having problems\
    \ in case of multithreading environment.\nI prefer this kind of implementation\
    \ (actually, it is not correctly said I prefer, because I avoid singletons as\
    \ much as possible):\nclass Singleton\n{\nprivate:\n   Singleton();\n\npublic:\n\
    \   static Singleton& instance()\n   {\n      static Singleton INSTANCE;\n   \
    \   return INSTANCE;\n   }\n};\n\nIt has no dynamic memory allocation.\n"
- - What should main() return in C and C++?
  - "\nThe return value for main should indicate how the program exited. Normal exit\
    \ is generally represented by a 0 return value from main. Abnormal exit is usually\
    \ signalled by a non-zero return, but there is no standard for how non-zero codes\
    \ are interpreted. Also as noted by others, void main() is explicitly prohibited\
    \ by the C++ standard and shouldn't be used. The valid C++ main signatures are:\n\
    int main()\n\nand\nint main(int argc, char* argv[])\n\nwhich is equivalent to\n\
    int main(int argc, char** argv)\n\nIt's also worth noting that in C++, int main()\
    \ can be left without a return value at which point it defaults to returning 0.\
    \ This is also true with a C99 program. Whether return 0 should be omitted or\
    \ not is open to debate. The range of valid C program main signatures is much\
    \ greater.  \nAlso, efficiency is not an issue with the main function. It can\
    \ only be entered and left once (marking the program's start and termination)\
    \ according to the C++ standard. For C, the case is different and re-entering\
    \ main() is allowed, but should be avoided. \n"
- - What should main() return in C and C++?
  - "\nThe accepted answer appears to be targetted for C++, so I thought I'd add an\
    \ answer that pertains to C, and this differs in a few ways.\nISO/IEC 9899:1989\
    \ (C90):\nmain() should be declared as either:\nint main(void)\nint main(int argc,\
    \ char **argv)\n\nOr equivalent. For example, int main(int argc, char *argv[])\
    \ is equivalent to the second one. Further, the int return type can be omitted\
    \ as it is a default.\nIf an implementation permits it, main() can be declared\
    \ in other ways, but this makes the program implementation defined, and no longer\
    \ strictly conforming.\nThe standard defines 3 values for returning that are strictly\
    \ conforming (that is, does not rely on implementation defined behaviour): 0 and\
    \ EXIT_SUCCESS for a successful termination, and EXIT_FAILURE for an unsuccessful\
    \ termination. Any other values are non-standard and implementation defined. main()\
    \ must have an explicit return statement at the end to avoid undefined behaviour.\n\
    Finally, there is nothing wrong from a standards point of view with calling main()\
    \ from a program.\nISO/IEC 9899:1999 (C99):\nFor C99, everything is the same as\
    \ above except:\n\nThe int return type may not be omitted.\nYou may omit the return\
    \ statement from main(). If you do, and main() finished, there is an implicit\
    \ return 0.\n\n"
- - What should main() return in C and C++?
  - "\nStandard C â Hosted Environment\nFor a hosted environment (that's the normal\
    \ one), the C11 standard (ISO/IEC 9899:2011) says:\n\n5.1.2.2.1 Program startup\n\
    The function called at program startup is named main. The implementation declares\
    \ no\n  prototype for this function. It shall be defined with a return type of\
    \ int and with no\n  parameters:\nint main(void) { /* ... */ }\n\nor with two\
    \ parameters (referred to here as argc and argv, though any names may be\n  used,\
    \ as they are local to the function in which they are declared):\nint main(int\
    \ argc, char *argv[]) { /* ... */ }\n\nor equivalent;10) or in some other implementation-defined\
    \ manner.\nIf they are declared, the parameters to the main function shall obey\
    \ the following\n  constraints:\n\nThe value of argc shall be nonnegative.\nargv[argc]\
    \ shall be a null pointer.\nIf the value of argc is greater than zero, the array\
    \ members argv[0] through\n  argv[argc-1] inclusive shall contain pointers to\
    \ strings, which are given\n  implementation-defined values by the host environment\
    \ prior to program startup. The\n  intent is to supply to the program information\
    \ determined prior to program startup\n  from elsewhere in the hosted environment.\
    \ If the host environment is not capable of\n  supplying strings with letters\
    \ in both uppercase and lowercase, the implementation\n  shall ensure that the\
    \ strings are received in lowercase.\nIf the value of argc is greater than zero,\
    \ the string pointed to by argv[0]\n  represents the program name; argv[0][0]\
    \ shall be the null character if the\n  program name is not available from the\
    \ host environment. If the value of argc is\n  greater than one, the strings pointed\
    \ to by argv[1] through argv[argc-1]\n  represent the program parameters.\nThe\
    \ parameters argc and argv and the strings pointed to by the argv array shall\n\
    \  be modifiable by the program, and retain their last-stored values between program\n\
    \  startup and program termination.\n\n10) Thus, int can be replaced by a typedef\
    \ name defined as int, or the type of argv can be written as\n  char **argv, and\
    \ so on.\n\nProgram termination in C99 or C11\nThe value returned from main()\
    \ is transmitted to the 'environment' in an implementation-defined way.\n\n5.1.2.2.3\
    \ Program termination\n1 If the return type of the main function is a type compatible\
    \ with int, a return from the\n  initial call to the main function is equivalent\
    \ to calling the exit function with the value\n  returned by the main function\
    \ as its argument;11) reaching the } that terminates the\n  main function returns\
    \ a value of 0. If the return type is not compatible with int, the\n  termination\
    \ status returned to the host environment is unspecified.\n11) In accordance with\
    \ 6.2.4, the lifetimes of objects with automatic storage duration declared in\
    \ main\n  will have ended in the former case, even where they would not have in\
    \ the latter.\n\nNote that 0 is mandated as 'success'. You can use EXIT_FAILURE\
    \ and EXIT_SUCCESS from <stdlib.h> if you prefer, but 0 is well established, and\
    \ so is 1. See also Exit codes greater than 255 â possible?.\nIn C89 (and hence\
    \ in Microsoft C), there is no statement about what happens if the main() function\
    \ returns but does not specify a return value; it therefore leads to undefined\
    \ behaviour.\n\n7.22.4.4 The exit function\nÂ¶5 Finally, control is returned to\
    \ the host environment. If the value of status is zero or EXIT_SUCCESS, an implementation-defined\
    \ form of the status successful termination is returned. If the value of status\
    \ is EXIT_FAILURE, an implementation-defined form of the status unsuccessful termination\
    \ is returned. Otherwise the status returned is implementation-defined.\n\nStandard\
    \ C++ â Hosted Environment\nThe C++11 standard (ISO/IEC 14882:2011) says:\n\n\
    3.6.1 Main function [basic.start.main]\nÂ¶1 A program shall contain a global function\
    \ called main, which is the designated start of the program. [...]\nÂ¶2 An implementation\
    \ shall not predefine the main function. This function shall not be overloaded.\
    \ It shall\n  have a return type of type int, but otherwise its type is implementation\
    \ defined.\n  All implementations\n  shall allow both of the following definitions\
    \ of main:\nint main() { /* ... */ }\n\nand\nint main(int argc, char* argv[])\
    \ { /* ... */ }\n\nIn the latter form argc shall be the number of arguments passed\
    \ to the program from the environment\n  in which the program is run. If argc\
    \ is nonzero these arguments shall be supplied in argv[0]\n  through argv[argc-1]\
    \ as pointers to the initial characters of null-terminated multibyte strings (NTMBSs)\
    \ (17.5.2.1.4.2) and argv[0] shall be the pointer to the initial character of\
    \ a NTMBS that represents the\n  name used to invoke the program or \"\". The\
    \ value of argc shall be non-negative. The value of argv[argc]\n  shall be 0.\
    \ [ Note: It is recommended that any further (optional) parameters be added after\
    \ argv. âend\n  note ]\nÂ¶3 The function main shall not be used within a program.\
    \ The linkage (3.5) of main is implementation-defined. [...]\nÂ¶5 A return statement\
    \ in main has the effect of leaving the main function (destroying any objects\
    \ with automatic\n  storage duration) and calling std::exit with the return value\
    \ as the argument. If control reaches the end\n  of main without encountering\
    \ a return statement, the effect is that of executing\nreturn 0;\n\n\nThe C++\
    \ standard explicitly says \"It [the main function] shall have a return type of\
    \ type int, but otherwise its type is implementation defined\", and requires the\
    \ same two signatures as the C standard to be supported as options. So a 'void\
    \ main()' is directly not allowed by the C++ standard, though there's nothing\
    \ it can do to stop a non-standard implementation allowing alternatives.  Note\
    \ that C++ forbids the user from calling main (but the C standard does not).\n\
    There's a paragraph of Â§18.5 Start and termination in the C++11 standard that\
    \ is identical to the paragraph from Â§7.22.4.4 The exit function in the C11 standard\
    \ (quoted above), apart from a footnote (which simply documents that EXIT_SUCCESS\
    \ and EXIT_FAILURE are defined in <cstdlib>).\nStandard C â Common Extension\n\
    Classically, Unix systems support a third variant:\nint main(int argc, char **argv,\
    \ char **envp) { ... }\n\nThe third argument is a null-terminated list of pointers\
    \ to strings, each of which is an environment variable which has a name, an equals\
    \ sign, and a value (possibly empty).  If you do not use this, you can still get\
    \ at the environment via 'extern char **environ;'.  For a long time, that did\
    \ not have a header that declared it, but the POSIX 2008 standard now requires\
    \ it to be declared in <unistd.h>.\nThis is recognized by the C standard as a\
    \ common extension, documented in Annex J:\n\nJ.5.1 Environment arguments\nÂ¶1\
    \ In a hosted environment, the main function receives a third argument, char *envp[],\n\
    \  that points to a null-terminated array of pointers to char, each of which points\
    \ to a string\n  that provides information about the environment for this execution\
    \ of the program (5.1.2.2.1).\n\nMicrosoft C\nThe Microsoft VS 2010 compiler is\
    \ interesting. The web site says:\n\nThe declaration syntax for main is\n int\
    \ main();\n\nor, optionally,\nint main(int argc, char *argv[], char *envp[]);\n\
    \nAlternatively, the main and wmain functions can be declared as returning void\
    \ (no return value). If you declare main or wmain as returning void, you cannot\
    \ return an exit code to the parent process or operating system by using a return\
    \ statement. To return an exit code when main or wmain is declared as void, you\
    \ must use the exit function.\n\nIt is not clear to me what happens (what exit\
    \ code is returned to the parent or OS) when a program with void main() does exit\
    \ â and the MS web site is silent too.\nInterestingly, MS does not prescribe the\
    \ two-argument version of main() that the C and C++ standards require. It only\
    \ prescribes a three argument form where the third argument is char **envp, a\
    \ pointer to a list of environment variables.\nThe Microsoft page also lists some\
    \ other alternatives â wmain() which takes wide character strings, and some more.\n\
    The Microsoft VisualÂ StudioÂ 2005 version of this page does not list void main()\
    \ as an alternative.  The versions from Microsoft VisualÂ StudioÂ 2008 onwards do.\n\
    Standard C â Freestanding Environment\nAs noted early on, the requirements above\
    \ apply to hosted environments. If you are working with a freestanding environment\
    \ (which is the alternative to a hosted environment), then the standard has much\
    \ less to say. For a freestanding environment, the function called at program\
    \ startup need not be called main and there are no constraints on its return type.\
    \  The standard says:\n\n5.1.2 Execution environments\nTwo execution environments\
    \ are defined: freestanding and hosted. In both cases,\n  program startup occurs\
    \ when a designated C function is called by the execution\n  environment. All\
    \ objects with static storage duration shall be initialized (set to their initial\
    \ values) before program startup. The manner and timing of such initialization\
    \ are otherwise unspecified. Program termination returns control to the execution\
    \ environment.\n5.1.2.1 Freestanding environment\nIn a freestanding environment\
    \ (in which C program execution may take place without any benefit of an operating\
    \ system), the name and type of the function called at program startup are implementation-defined.\
    \ Any library facilities available to a freestanding program, other than the minimal\
    \ set required by clause 4, are implementation-defined.\nThe effect of program\
    \ termination in a freestanding environment is implementation-defined.\n\nThe\
    \ cross-reference to clause 4 Conformance refers to this:\n\nÂ¶5 A strictly conforming\
    \ program shall use only those features of the language and library specified\
    \ in this International Standard.3) It shall not produce output dependent on any\
    \ unspecified, undefined, or implementation-defined behavior, and shall not exceed\
    \ any minimum implementation limit.\nÂ¶6 The two forms of conforming implementation\
    \ are hosted and freestanding. A conforming hosted implementation shall accept\
    \ any strictly conforming program. A conforming freestanding implementation shall\
    \ accept any strictly conforming program in which the use of the features specified\
    \ in the library clause (clause 7) is confined to the contents of the standard\
    \ headers <float.h>, <iso646.h>, <limits.h>, <stdalign.h>,\n  <stdarg.h>, <stdbool.h>,\
    \ <stddef.h>, <stdint.h>, and\n  <stdnoreturn.h>. A conforming implementation\
    \ may have extensions (including\n  additional library functions), provided they\
    \ do not alter the behavior of any strictly conforming program.4)\nÂ¶7 A conforming\
    \ program is one that is acceptable to a conforming implementation.5)\n3) A strictly\
    \ conforming program can use conditional features (see 6.10.8.3) provided the\
    \ use is guarded by an appropriate conditional inclusion preprocessing directive\
    \ using the related macro. For example:\n#ifdef __STDC_IEC_559__ /* FE_UPWARD\
    \ defined */\n    /* ... */\n    fesetround(FE_UPWARD);\n    /* ... */\n#endif\n\
    \n4) This implies that a conforming implementation reserves no identifiers other\
    \ than those explicitly reserved in this International Standard.\n5) Strictly\
    \ conforming programs are intended to be maximally portable among conforming implementations.\
    \ Conforming programs may depend upon non-portable features of a conforming implementation.\n\
    \nIt is noticeable that the only header required of a freestanding environment\
    \ that actually defines any functions is <stdarg.h> (and even those may be â and\
    \ often are â just macros).\nStandard C++ â Freestanding Environment\nJust as\
    \ the C standard recognizes both hosted and freestanding environment, so too does\
    \ the C++ standard. (Quotes from ISO/IEC 14882:2011.)\n\n1.4 Implementation compliance\
    \ [intro.compliance]\nÂ¶7 Two kinds of implementations are defined: a hosted implementation\
    \ and a freestanding implementation. For a hosted implementation, this International\
    \ Standard defines the set of available libraries. A freestanding\n  implementation\
    \ is one in which execution may take place without the benefit of an operating\
    \ system, and has an implementation-defined set of libraries that includes certain\
    \ language-support libraries (17.6.1.3).\nÂ¶8 A conforming implementation may have\
    \ extensions (including additional library functions), provided they do not alter\
    \ the behavior of any well-formed program. Implementations are required to diagnose\
    \ programs that\n  use such extensions that are ill-formed according to this International\
    \ Standard. Having done so, however, they can compile and execute such programs.\n\
    Â¶9 Each implementation shall include documentation that identifies all conditionally-supported\
    \ constructs that it does not support and defines all locale-specific characteristics.3\n\
    3) This documentation also defines implementation-defined behavior; see 1.9.\n\
    17.6.1.3 Freestanding implementations [compliance]\nTwo kinds of implementations\
    \ are defined: hosted and freestanding (1.4). For a hosted implementation, this\
    \ International Standard describes the set of available headers.\nA freestanding\
    \ implementation has an implementation-defined set of headers. This set shall\
    \ include at least the headers shown in Table 16.\nThe supplied version of the\
    \ header <cstdlib> shall declare at least the functions abort, atexit, at_quick_exit,\
    \ exit, and quick_exit (18.5). The other headers listed in this table shall meet\
    \ the same requirements as for a hosted implementation.\nTable 16 â C++ headers\
    \ for freestanding implementations\nSubclause                           Header(s)\n\
    \                                    <ciso646>\n18.2  Types                  \
    \       <cstddef>\n18.3  Implementation properties     <cfloat> <limits> <climits>\n\
    18.4  Integer types                 <cstdint>\n18.5  Start and termination   \
    \      <cstdlib>\n18.6  Dynamic memory management     <new>\n18.7  Type identification\
    \           <typeinfo>\n18.8  Exception handling            <exception>\n18.9\
    \  Initializer lists             <initializer_list>\n18.10 Other runtime support\
    \         <cstdalign> <cstdarg> <cstdbool>\n20.9  Type traits                \
    \   <type_traits>\n29    Atomics                       <atomic>\n\n\nWhat about\
    \ using int main() in C?\nThe standard Â§5.1.2.2.1 of the C11 standard shows the\
    \ preferred notation âÂ int main(void) â but there are also two examples in the\
    \ standard which show int main(): Â§6.5.3.4 Â¶8 and Â§6.7.6.3 Â¶20.  Now, it is important\
    \ to note that examples are not 'normative'; they are only illustrative.  If there\
    \ are bugs in the examples, they do not directly affect the main text of the standard.\
    \  That said, they are strongly indicative of expected behaviour, so if the standard\
    \ includes int main() in an example, it suggests that int main() is not forbidden,\
    \ even if it is not the preferred notation.\n\n6.5.3.4 The sizeof and _Alignof\
    \ operators\nâ¦\nÂ¶8 EXAMPLE 3 In this example, the size of a variable length array\
    \ is computed and returned from a function:\n#include <stddef.h>\n\nsize_t fsize3(int\
    \ n)\n{\n    char b[n+3]; // variable length array\n    return sizeof b; // execution\
    \ time sizeof\n}\nint main()\n{\n    size_t size;\n    size = fsize3(10); // fsize3\
    \ returns 13\n    return 0;\n}\n\n\n"
- - What are the rules for calling the superclass constructor?
  - "\nBase class constructors are automatically called for you if they have no argument.\
    \  If you want to call a superclass constructor with an argument, you must use\
    \ the subclass's constructor initialization list.  Unlike Java, C++ supports multiple\
    \ inheritance (for better or worse), so the base class must be referred to by\
    \ name, rather than \"super()\".\nclass SuperClass\n{\n    public:\n\n       \
    \ SuperClass(int foo)\n        {\n            // do something with foo\n     \
    \   }\n};\n\nclass SubClass : public SuperClass\n{\n    public:\n\n        SubClass(int\
    \ foo, int bar)\n        : SuperClass(foo)    // Call the superclass constructor\
    \ in the subclass' initialization list.\n        {\n            // do something\
    \ with bar\n        }\n};\n\nMore info on the constructor's initialization list\
    \ here and here.\n"
- - What are the rules for calling the superclass constructor?
  - "\nIn C++, the no-argument constructors for all superclasses and member variables\
    \ are called for you, before entering your constructor. If you want to pass them\
    \ arguments, there is a separate syntax for this called \"constructor chaining\"\
    , which looks like this:\nclass Sub : public Base\n{\n  Sub(int x, int y)\n  :\
    \ Base(x), member(y)\n  {\n  }\n  Type member;\n};\n\nIf anything run at this\
    \ point throws, the bases/members which had previously completed construction\
    \ have their destructors called and the exception is rethrown to to the caller.\
    \ If you want to catch exceptions during chaining, you must use a function try\
    \ block:\nclass Sub : public Base\n{\n  Sub(int x, int y)\n  try : Base(x), member(y)\n\
    \  {\n    // function body goes here\n  } catch(const ExceptionType &e) {\n  \
    \  throw kaboom();\n  }\n  Type member;\n};\n\nIn this form, note that the try\
    \ block is the body of the function, rather than being inside the body of the\
    \ function; this allows it to catch exceptions thrown by implicit or explicit\
    \ member and base class initializations, as well as during the body of the function.\
    \ However, if a function catch block does not throw a different exception, the\
    \ runtime will rethrow the original error; exceptions during initialization cannot\
    \ be ignored.\n"
- - What are the rules for calling the superclass constructor?
  - "\nIn C++ there is a concept of constructor's initialization list, which is where\
    \ you can and should call the base class' constructor and where you should also\
    \ initialize the data members.  The initialization list comes after the constructor\
    \ signature following a colon, and before the body of the constructor.  Let's\
    \ say we have a class A:\n\nclass A : public B\n{\npublic:\n  A(int a, int b,\
    \ int c);\nprivate:\n  int b_, c_;\n};\n\nThen, assuming B has a constructor which\
    \ takes an int, A's constructor may look like this:\n\nA::A(int a, int b, int\
    \ c) \n  : B(a), b_(b), c_(c) // initialization list\n{\n  // do something\n}\n\
    \nAs you can see, the constructor of the base class is called in the initialization\
    \ list.  Initializing the data members in the initialization list, by the way,\
    \ is preferable to assigning the values for b_, and c_ inside the body of the\
    \ constructor, because you are saving the extra cost of assignment.\nKeep in mind,\
    \ that data members are always initialized in the order in which they are declared\
    \ in the class definition, regardless of their order in the initialization list.\
    \  To avoid strange bugs, which may arise if your data members depend on each\
    \ other, you should always make sure that the order of the members is the same\
    \ in the initialization list and the class definition.  For the same reason the\
    \ base class constructor must be the first item in the initialization list.  If\
    \ you omit it altogether, then the default constructor for the base class will\
    \ be called automatically.  In that case, if the base class does not have a default\
    \ constructor, you will get a compiler error.\n"
- - Why isn't sizeof for a struct equal to the sum of sizeof of each member?
  - "\nThis is because of padding added to satisfy alignment constraints. Data structure\
    \ alignment impacts both performance and correctness of programs:\n\nMis-aligned\
    \ access might be a hard error (often SIGBUS).\nMis-aligned access might be a\
    \ soft error.\n\n\nEither corrected in hardware, for a modest performance-degradation.\n\
    Or corrected by emulation in software, for a severe performance-degradation.\n\
    In addition, atomicity and other concurrency-guarantees might be broken, leading\
    \ to subtle errors.\n\n\nHere's an example using typical settings for an x86 processor\
    \ (all used 32 and 64 bit modes):\nstruct X\n{\n    short s; /* 2 bytes */\n \
    \            /* 2 padding bytes */\n    int   i; /* 4 bytes */\n    char  c; /*\
    \ 1 byte */\n             /* 3 padding bytes */\n};\n\nstruct Y\n{\n    int  \
    \ i; /* 4 bytes */\n    char  c; /* 1 byte */\n             /* 1 padding byte\
    \ */\n    short s; /* 2 bytes */\n};\n\nstruct Z\n{\n    int   i; /* 4 bytes */\n\
    \    short s; /* 2 bytes */\n    char  c; /* 1 byte */\n             /* 1 padding\
    \ byte */\n};\n\nconst int sizeX = sizeof(struct X); /* = 12 */\nconst int sizeY\
    \ = sizeof(struct Y); /* = 8 */\nconst int sizeZ = sizeof(struct Z); /* = 8 */\n\
    \nOne can minimize the size of structures by sorting members by alignment (sorting\
    \ by size suffices for that in basic types) (like structure Z in the example above).\n\
    IMPORTANT NOTE: Both the C and C++ standards state that structure alignment is\
    \ implementation-defined.  Therefore each compiler may choose to align data differently,\
    \ resulting in different and incompatible data layouts.  For this reason, when\
    \ dealing with libraries that will be used by different compilers, it is important\
    \ to understand how the compilers align data.  Some compilers have command-line\
    \ settings and/or special #pragma statements to change the structure alignment\
    \ settings.\n"
- - Why isn't sizeof for a struct equal to the sum of sizeof of each member?
  - "\nPacking and byte alignment, as described in the C FAQ here:\n\nIt's for alignment.\
    \ Many processors can't access 2- and 4-byte\n  quantities (e.g. ints and long\
    \ ints) if they're crammed in\n  every-which-way.\nSuppose you have this structure:\n\
    struct {\n    char a[3];\n    short int b;\n    long int c;\n    char d[3];\n\
    };\n\nNow, you might think that it ought to be possible to pack this\n  structure\
    \ into memory like this:\n+-------+-------+-------+-------+\n|           a   \
    \        |   b   |\n+-------+-------+-------+-------+\n|   b   |           c \
    \          |\n+-------+-------+-------+-------+\n|   c   |           d       \
    \    |\n+-------+-------+-------+-------+\n\nBut it's much, much easier on the\
    \ processor if the compiler arranges\n  it like this:\n+-------+-------+-------+\n\
    |           a           |\n+-------+-------+-------+\n|       b       |\n+-------+-------+-------+-------+\n\
    |               c               |\n+-------+-------+-------+-------+\n|      \
    \     d           |\n+-------+-------+-------+\n\nIn the packed version, notice\
    \ how it's at least a little bit hard for\n  you and me to see how the b and c\
    \ fields wrap around? In a nutshell,\n  it's hard for the processor, too. Therefore,\
    \ most compilers will pad\n  the structure (as if with extra, invisible fields)\
    \ like this:\n+-------+-------+-------+-------+\n|           a           | pad1\
    \  |\n+-------+-------+-------+-------+\n|       b       |     pad2      |\n+-------+-------+-------+-------+\n\
    |               c               |\n+-------+-------+-------+-------+\n|      \
    \     d           | pad3  |\n+-------+-------+-------+-------+\n\n\n"
- - Why isn't sizeof for a struct equal to the sum of sizeof of each member?
  - "\nIf you want the structure to have a certain size with GCC for example use __attribute__((packed)).\n\
    On Windows you can set the alignment to one byte when using the cl.exe compier\
    \ with the /Zp option.\nUsually it is easier for the CPU to access data that is\
    \ a multiple of 4 (or 8), depending platform and also on the compiler.\nSo it\
    \ is a matter of alignment basically.\nYou need to have good reasons to change\
    \ it.\n"
- - How to concatenate a std::string and an int?
  - "\nIn alphabetical order:\nstd::string name = \"John\";\nint age = 21;\nstd::string\
    \ result;\n\n// 1. with Boost\nresult = name + boost::lexical_cast<std::string>(age);\n\
    \n// 2. with C++11\nresult = name + std::to_string(age);\n\n// 3. with FastFormat.Format\n\
    fastformat::fmt(result, \"{0}{1}\", name, age);\n\n// 4. with FastFormat.Write\n\
    fastformat::write(result, name, age);\n\n// 5. with the {fmt} library\nresult\
    \ = fmt::format(\"{}{}\", name, age);\n\n// 6. with IOStreams\nstd::stringstream\
    \ sstm;\nsstm << name << age;\nresult = sstm.str();\n\n// 7. with itoa\nchar numstr[21];\
    \ // enough to hold all numbers up to 64-bits\nresult = name + itoa(age, numstr,\
    \ 10);\n\n// 8. with sprintf\nchar numstr[21]; // enough to hold all numbers up\
    \ to 64-bits\nsprintf(numstr, \"%d\", age);\nresult = name + numstr;\n\n// 9.\
    \ with STLSoft's integer_to_string\nchar numstr[21]; // enough to hold all numbers\
    \ up to 64-bits\nresult = name + stlsoft::integer_to_string(numstr, 21, age);\n\
    \n// 10. with STLSoft's winstl::int_to_string()\nresult = name + winstl::int_to_string(age);\n\
    \n// 11. With Poco NumberFormatter\nresult = name + Poco::NumberFormatter().format(age);\n\
    \n\nis safe, but slow; requires Boost (header-only); most/all platforms\nis safe,\
    \ requires C++11 (to_string() is already included in #include <string>)\nis safe,\
    \ and fast; requires FastFormat, which must be compiled; most/all platforms\n\
    is safe, and fast; requires FastFormat, which must be compiled; most/all platforms\n\
    is safe, and fast; requires the {fmt} library, which can either be compiled or\
    \ used in a header-only mode; most/all platforms\nsafe, slow, and verbose; requires\
    \ #include <sstream> (from standard C++)\nis brittle (you must supply a large\
    \ enough buffer), fast, and verbose; itoa() is a non-standard extension, and not\
    \ guaranteed to be available for all platforms\nis brittle (you must supply a\
    \ large enough buffer), fast, and verbose; requires nothing (is standard C++);\
    \ all platforms\nis brittle (you must supply a large enough buffer), probably\
    \ the fastest-possible conversion, verbose; requires STLSoft (header-only); most/all\
    \ platforms\nsafe-ish (you don't use more than one int_to_string() call in a single\
    \ statement), fast; requires STLSoft (header-only); Windows-only\nis safe, but\
    \ slow; requires Poco C++ ; most/all platforms\n\n"
- - How to concatenate a std::string and an int?
  - "\nIn C++11, you can use std::to_string, e.g.:\nauto result = name + std::to_string(\
    \ age );\n\n"
- - How to concatenate a std::string and an int?
  - "\nIf you have Boost, you can convert the integer to a string using boost::lexical_cast<std::string>(age).\n\
    Another way is to use stringstreams:\nstd::stringstream ss;\nss << age;\nstd::cout\
    \ << name << ss.str() << std::endl;\n\nA third approach would be to use sprintf\
    \ or snprintf from the C library.\nchar buffer[128];\nsnprintf(buffer, sizeof(buffer),\
    \ \"%s%d\", name.c_str(), age);\nstd::cout << buffer << std::endl;\n\nOther posters\
    \ suggested using itoa. This is NOT a standard function, so your code will not\
    \ be portable if you use it. There are compilers that don't support it.\n"
- - push_back vs emplace_back
  - "\nIn addition to what visitor said :\nThe function void emplace_back(Type&& _Val)\
    \ provided by MSCV10 is non conforming and redundant, because as you noted it\
    \ is strictly equivalent to push_back(Type&& _Val).\nBut the real C++0x form of\
    \ emplace_back is really useful: void emplace_back(Args&&...);\nInstead of taking\
    \ a value_type it takes a variadic list of arguments, so that means that you can\
    \ now perfectly forward the arguments and construct directly an object into a\
    \ container without a temporary at all. \nThat's useful because no matter how\
    \ much cleverness RVO and move semantic bring to the table there is still complicated\
    \ cases where a push_back is likely to make unnecessary copies (or move). For\
    \ example, with the traditional insert() function of a std::map, you have to create\
    \ a temporary, which will then be copied into a std::pair<Key, Value>, which will\
    \ then be copied into the map : \nstd::map<int, Complicated> m;\nint anInt = 4;\n\
    double aDouble = 5.0;\nstd::string aString = \"C++\";\n\n// cross your finger\
    \ so that the optimizer is really good\nm.insert(std::make_pair(4, Complicated(anInt,\
    \ aDouble, aString))); \n\n// should be easier for the optimizer\nm.emplace(4,\
    \ anInt, aDouble, aString);\n\nSo why didn't they implement the right version\
    \ of emplace_back in MSVC? Actually, it bugged me too a while ago, so I asked\
    \ the same question on the Visual C++ blog. Here is the answer from Stephan T\
    \ Lavavej, the official maintainer of the Visual C++ standard library implementation\
    \ at Microsoft.\n\nQ: Are beta 2 emplace functions just some kind of placeholder\
    \ right now?\nA: As you may know, variadic templates\n  aren't implemented in\
    \ VC10. We\n  simulate them with preprocessor\n  machinery for things like\n \
    \ make_shared<T>(), tuple, and the new\n  things in <functional>. This\n  preprocessor\
    \ machinery is relatively\n  difficult to use and maintain. Also,\n  it significantly\
    \ affects compilation\n  speed, as we have to repeatedly\n  include subheaders.\
    \ Due to a\n  combination of our time constraints\n  and compilation speed concerns,\
    \ we\n  haven't simulated variadic templates\n  in our emplace functions.\nWhen\
    \ variadic templates are\n  implemented in the compiler, you can\n  expect that\
    \ we'll take advantage of\n  them in the libraries, including in\n  our emplace\
    \ functions. We take\n  conformance very seriously, but\n  unfortunately, we can't\
    \ do everything\n  all at once.\n\nIt's an understandable decision. Everyone who\
    \ tried just once to emulate variadic template with preprocessor horrible tricks\
    \ knows how disgusting this stuff gets. \n"
- - push_back vs emplace_back
  - "\nemplace_back shouldn't take an argument of type vector::value_type, but instead\
    \ variadic arguments that are forwarded to the constructor of the appended item.\n\
    template <class... Args> void emplace_back(Args&&... args); \n\nIt is possible\
    \ to pass a value_type which will be forwarded to the copy constructor.\nBecause\
    \ it forwards the arguments, this means that if you don't have rvalue, this still\
    \ means that the container will store a \"copied\" copy, not a moved copy.\n std::vector<std::string>\
    \ vec;\n vec.emplace_back(std::string(\"Hello\")); // moves\n std::string s;\n\
    \ vec.emplace_back(s); //copies\n\nBut the above should be identical to what push_back\
    \ does. It is probably rather meant for use cases like:\n std::vector<std::pair<std::string,\
    \ std::string> > vec;\n vec.emplace_back(std::string(\"Hello\"), std::string(\"\
    world\")); \n // should end up invoking this constructor:\n //template<class U,\
    \ class V> pair(U&& x, V&& y);\n //without making any copies of the strings\n\n"
- - push_back vs emplace_back
  - "\nOptimization for emplace_back can be demonstrated in next example.\nFor emplace_back\
    \ constructor A (int x_arg) will be called. And for \npush_back A (int x_arg)\
    \ is called first and move A (A &&rhs) is called afterwards.\nOf course, the constructor\
    \ has to be marked as explicit, but for current example is good to remove explicitness.\n\
    #include <iostream>\n#include <vector>\nclass A\n{\npublic:\n  A (int x_arg) :\
    \ x (x_arg) { std::cout << \"A (x_arg)\\n\"; }\n  A () { x = 0; std::cout << \"\
    A ()\\n\"; }\n  A (const A &rhs) noexcept { x = rhs.x; std::cout << \"A (A &)\\\
    n\"; }\n  A (A &&rhs) noexcept { x = rhs.x; std::cout << \"A (A &&)\\n\"; }\n\n\
    private:\n  int x;\n};\n\nint main ()\n{\n  {\n    std::vector<A> a;\n    std::cout\
    \ << \"call emplace_back:\\n\";\n    a.emplace_back (0);\n  }\n  {\n    std::vector<A>\
    \ a;\n    std::cout << \"call push_back:\\n\";\n    a.push_back (1);\n  }\n  return\
    \ 0;\n}\n\noutput:\ncall emplace_back:\nA (x_arg)\n\ncall push_back:\nA (x_arg)\n\
    A (A &&)\n\n"
- - How do I achieve the theoretical maximum of 4 FLOPs per cycle?
  - "\nI've done this exact task before. But it was mainly to measure power consumption\
    \ and CPU temperatures. The following code (which is fairly long) achieves close\
    \ to optimal on my Core i7 2600K.\nThe key thing to note here is the massive amount\
    \ of manual loop-unrolling as well as interleaving of multiplies and adds...\n\
    The full project can be found on my GitHub: https://github.com/Mysticial/Flops\n\
    Warning:\nIf you decide to compile and run this, pay attention to your CPU temperatures!!!Make\
    \ sure you don't overheat it. And make sure CPU-throttling doesn't affect your\
    \ results!\nFurthermore, I take no responsibility for whatever damage that may\
    \ result from running this code.\nNotes:\n\nThis code is optimized for x64. x86\
    \ doesn't have enough registers for this to compile well.\nThis code has been\
    \ tested to work well on Visual Studio 2010/2012 and GCC 4.6.ICC 11 (Intel Compiler\
    \ 11) surprisingly has trouble compiling it well.\nThese are for pre-FMA processors.\
    \ In order to achieve peak FLOPS on Intel Haswell and AMD Bulldozer processors\
    \ (and later), FMA (Fused Multiply Add) instructions will be needed. These are\
    \ beyond the scope of this benchmark.\n\n\n#include <emmintrin.h>\n#include <omp.h>\n\
    #include <iostream>\nusing namespace std;\n\ntypedef unsigned long long uint64;\n\
    \ndouble test_dp_mac_SSE(double x,double y,uint64 iterations){\n    register __m128d\
    \ r0,r1,r2,r3,r4,r5,r6,r7,r8,r9,rA,rB,rC,rD,rE,rF;\n\n    //  Generate starting\
    \ data.\n    r0 = _mm_set1_pd(x);\n    r1 = _mm_set1_pd(y);\n\n    r8 = _mm_set1_pd(-0.0);\n\
    \n    r2 = _mm_xor_pd(r0,r8);\n    r3 = _mm_or_pd(r0,r8);\n    r4 = _mm_andnot_pd(r8,r0);\n\
    \    r5 = _mm_mul_pd(r1,_mm_set1_pd(0.37796447300922722721));\n    r6 = _mm_mul_pd(r1,_mm_set1_pd(0.24253562503633297352));\n\
    \    r7 = _mm_mul_pd(r1,_mm_set1_pd(4.1231056256176605498));\n    r8 = _mm_add_pd(r0,_mm_set1_pd(0.37796447300922722721));\n\
    \    r9 = _mm_add_pd(r1,_mm_set1_pd(0.24253562503633297352));\n    rA = _mm_sub_pd(r0,_mm_set1_pd(4.1231056256176605498));\n\
    \    rB = _mm_sub_pd(r1,_mm_set1_pd(4.1231056256176605498));\n\n    rC = _mm_set1_pd(1.4142135623730950488);\n\
    \    rD = _mm_set1_pd(1.7320508075688772935);\n    rE = _mm_set1_pd(0.57735026918962576451);\n\
    \    rF = _mm_set1_pd(0.70710678118654752440);\n\n    uint64 iMASK = 0x800fffffffffffffull;\n\
    \    __m128d MASK = _mm_set1_pd(*(double*)&iMASK);\n    __m128d vONE = _mm_set1_pd(1.0);\n\
    \n    uint64 c = 0;\n    while (c < iterations){\n        size_t i = 0;\n    \
    \    while (i < 1000){\n            //  Here's the meat - the part that really\
    \ matters.\n\n            r0 = _mm_mul_pd(r0,rC);\n            r1 = _mm_add_pd(r1,rD);\n\
    \            r2 = _mm_mul_pd(r2,rE);\n            r3 = _mm_sub_pd(r3,rF);\n  \
    \          r4 = _mm_mul_pd(r4,rC);\n            r5 = _mm_add_pd(r5,rD);\n    \
    \        r6 = _mm_mul_pd(r6,rE);\n            r7 = _mm_sub_pd(r7,rF);\n      \
    \      r8 = _mm_mul_pd(r8,rC);\n            r9 = _mm_add_pd(r9,rD);\n        \
    \    rA = _mm_mul_pd(rA,rE);\n            rB = _mm_sub_pd(rB,rF);\n\n        \
    \    r0 = _mm_add_pd(r0,rF);\n            r1 = _mm_mul_pd(r1,rE);\n          \
    \  r2 = _mm_sub_pd(r2,rD);\n            r3 = _mm_mul_pd(r3,rC);\n            r4\
    \ = _mm_add_pd(r4,rF);\n            r5 = _mm_mul_pd(r5,rE);\n            r6 =\
    \ _mm_sub_pd(r6,rD);\n            r7 = _mm_mul_pd(r7,rC);\n            r8 = _mm_add_pd(r8,rF);\n\
    \            r9 = _mm_mul_pd(r9,rE);\n            rA = _mm_sub_pd(rA,rD);\n  \
    \          rB = _mm_mul_pd(rB,rC);\n\n            r0 = _mm_mul_pd(r0,rC);\n  \
    \          r1 = _mm_add_pd(r1,rD);\n            r2 = _mm_mul_pd(r2,rE);\n    \
    \        r3 = _mm_sub_pd(r3,rF);\n            r4 = _mm_mul_pd(r4,rC);\n      \
    \      r5 = _mm_add_pd(r5,rD);\n            r6 = _mm_mul_pd(r6,rE);\n        \
    \    r7 = _mm_sub_pd(r7,rF);\n            r8 = _mm_mul_pd(r8,rC);\n          \
    \  r9 = _mm_add_pd(r9,rD);\n            rA = _mm_mul_pd(rA,rE);\n            rB\
    \ = _mm_sub_pd(rB,rF);\n\n            r0 = _mm_add_pd(r0,rF);\n            r1\
    \ = _mm_mul_pd(r1,rE);\n            r2 = _mm_sub_pd(r2,rD);\n            r3 =\
    \ _mm_mul_pd(r3,rC);\n            r4 = _mm_add_pd(r4,rF);\n            r5 = _mm_mul_pd(r5,rE);\n\
    \            r6 = _mm_sub_pd(r6,rD);\n            r7 = _mm_mul_pd(r7,rC);\n  \
    \          r8 = _mm_add_pd(r8,rF);\n            r9 = _mm_mul_pd(r9,rE);\n    \
    \        rA = _mm_sub_pd(rA,rD);\n            rB = _mm_mul_pd(rB,rC);\n\n    \
    \        i++;\n        }\n\n        //  Need to renormalize to prevent denormal/overflow.\n\
    \        r0 = _mm_and_pd(r0,MASK);\n        r1 = _mm_and_pd(r1,MASK);\n      \
    \  r2 = _mm_and_pd(r2,MASK);\n        r3 = _mm_and_pd(r3,MASK);\n        r4 =\
    \ _mm_and_pd(r4,MASK);\n        r5 = _mm_and_pd(r5,MASK);\n        r6 = _mm_and_pd(r6,MASK);\n\
    \        r7 = _mm_and_pd(r7,MASK);\n        r8 = _mm_and_pd(r8,MASK);\n      \
    \  r9 = _mm_and_pd(r9,MASK);\n        rA = _mm_and_pd(rA,MASK);\n        rB =\
    \ _mm_and_pd(rB,MASK);\n        r0 = _mm_or_pd(r0,vONE);\n        r1 = _mm_or_pd(r1,vONE);\n\
    \        r2 = _mm_or_pd(r2,vONE);\n        r3 = _mm_or_pd(r3,vONE);\n        r4\
    \ = _mm_or_pd(r4,vONE);\n        r5 = _mm_or_pd(r5,vONE);\n        r6 = _mm_or_pd(r6,vONE);\n\
    \        r7 = _mm_or_pd(r7,vONE);\n        r8 = _mm_or_pd(r8,vONE);\n        r9\
    \ = _mm_or_pd(r9,vONE);\n        rA = _mm_or_pd(rA,vONE);\n        rB = _mm_or_pd(rB,vONE);\n\
    \n        c++;\n    }\n\n    r0 = _mm_add_pd(r0,r1);\n    r2 = _mm_add_pd(r2,r3);\n\
    \    r4 = _mm_add_pd(r4,r5);\n    r6 = _mm_add_pd(r6,r7);\n    r8 = _mm_add_pd(r8,r9);\n\
    \    rA = _mm_add_pd(rA,rB);\n\n    r0 = _mm_add_pd(r0,r2);\n    r4 = _mm_add_pd(r4,r6);\n\
    \    r8 = _mm_add_pd(r8,rA);\n\n    r0 = _mm_add_pd(r0,r4);\n    r0 = _mm_add_pd(r0,r8);\n\
    \n\n    //  Prevent Dead Code Elimination\n    double out = 0;\n    __m128d temp\
    \ = r0;\n    out += ((double*)&temp)[0];\n    out += ((double*)&temp)[1];\n\n\
    \    return out;\n}\n\nvoid test_dp_mac_SSE(int tds,uint64 iterations){\n\n  \
    \  double *sum = (double*)malloc(tds * sizeof(double));\n    double start = omp_get_wtime();\n\
    \n#pragma omp parallel num_threads(tds)\n    {\n        double ret = test_dp_mac_SSE(1.1,2.1,iterations);\n\
    \        sum[omp_get_thread_num()] = ret;\n    }\n\n    double secs = omp_get_wtime()\
    \ - start;\n    uint64 ops = 48 * 1000 * iterations * tds * 2;\n    cout << \"\
    Seconds = \" << secs << endl;\n    cout << \"FP Ops  = \" << ops << endl;\n  \
    \  cout << \"FLOPs   = \" << ops / secs << endl;\n\n    double out = 0;\n    int\
    \ c = 0;\n    while (c < tds){\n        out += sum[c++];\n    }\n\n    cout <<\
    \ \"sum = \" << out << endl;\n    cout << endl;\n\n    free(sum);\n}\n\nint main(){\n\
    \    //  (threads, iterations)\n    test_dp_mac_SSE(8,10000000);\n\n    system(\"\
    pause\");\n}\n\nOutput (1 thread, 10000000 iterations) - Compiled with Visual\
    \ Studio 2010 SP1 - x64 Release:\nSeconds = 55.5104\nFP Ops  = 960000000000\n\
    FLOPs   = 1.7294e+010\nsum = 2.22652\n\nThe machine is a Core i7 2600K @ 4.4 GHz.\
    \ Theoretical SSE peak is 4 flops * 4.4 GHz = 17.6 GFlops. This code achieves\
    \ 17.3 GFlops - not bad.\nOutput (8 threads, 10000000 iterations) - Compiled with\
    \ Visual Studio 2010 SP1 - x64 Release:\nSeconds = 117.202\nFP Ops  = 7680000000000\n\
    FLOPs   = 6.55279e+010\nsum = 17.8122\n\nTheoretical SSE peak is 4 flops * 4 cores\
    \ * 4.4 GHz = 70.4 GFlops. Actual is 65.5 GFlops.\n\nLet's take this one step\
    \ further. AVX...\n#include <immintrin.h>\n#include <omp.h>\n#include <iostream>\n\
    using namespace std;\n\ntypedef unsigned long long uint64;\n\ndouble test_dp_mac_AVX(double\
    \ x,double y,uint64 iterations){\n    register __m256d r0,r1,r2,r3,r4,r5,r6,r7,r8,r9,rA,rB,rC,rD,rE,rF;\n\
    \n    //  Generate starting data.\n    r0 = _mm256_set1_pd(x);\n    r1 = _mm256_set1_pd(y);\n\
    \n    r8 = _mm256_set1_pd(-0.0);\n\n    r2 = _mm256_xor_pd(r0,r8);\n    r3 = _mm256_or_pd(r0,r8);\n\
    \    r4 = _mm256_andnot_pd(r8,r0);\n    r5 = _mm256_mul_pd(r1,_mm256_set1_pd(0.37796447300922722721));\n\
    \    r6 = _mm256_mul_pd(r1,_mm256_set1_pd(0.24253562503633297352));\n    r7 =\
    \ _mm256_mul_pd(r1,_mm256_set1_pd(4.1231056256176605498));\n    r8 = _mm256_add_pd(r0,_mm256_set1_pd(0.37796447300922722721));\n\
    \    r9 = _mm256_add_pd(r1,_mm256_set1_pd(0.24253562503633297352));\n    rA =\
    \ _mm256_sub_pd(r0,_mm256_set1_pd(4.1231056256176605498));\n    rB = _mm256_sub_pd(r1,_mm256_set1_pd(4.1231056256176605498));\n\
    \n    rC = _mm256_set1_pd(1.4142135623730950488);\n    rD = _mm256_set1_pd(1.7320508075688772935);\n\
    \    rE = _mm256_set1_pd(0.57735026918962576451);\n    rF = _mm256_set1_pd(0.70710678118654752440);\n\
    \n    uint64 iMASK = 0x800fffffffffffffull;\n    __m256d MASK = _mm256_set1_pd(*(double*)&iMASK);\n\
    \    __m256d vONE = _mm256_set1_pd(1.0);\n\n    uint64 c = 0;\n    while (c <\
    \ iterations){\n        size_t i = 0;\n        while (i < 1000){\n           \
    \ //  Here's the meat - the part that really matters.\n\n            r0 = _mm256_mul_pd(r0,rC);\n\
    \            r1 = _mm256_add_pd(r1,rD);\n            r2 = _mm256_mul_pd(r2,rE);\n\
    \            r3 = _mm256_sub_pd(r3,rF);\n            r4 = _mm256_mul_pd(r4,rC);\n\
    \            r5 = _mm256_add_pd(r5,rD);\n            r6 = _mm256_mul_pd(r6,rE);\n\
    \            r7 = _mm256_sub_pd(r7,rF);\n            r8 = _mm256_mul_pd(r8,rC);\n\
    \            r9 = _mm256_add_pd(r9,rD);\n            rA = _mm256_mul_pd(rA,rE);\n\
    \            rB = _mm256_sub_pd(rB,rF);\n\n            r0 = _mm256_add_pd(r0,rF);\n\
    \            r1 = _mm256_mul_pd(r1,rE);\n            r2 = _mm256_sub_pd(r2,rD);\n\
    \            r3 = _mm256_mul_pd(r3,rC);\n            r4 = _mm256_add_pd(r4,rF);\n\
    \            r5 = _mm256_mul_pd(r5,rE);\n            r6 = _mm256_sub_pd(r6,rD);\n\
    \            r7 = _mm256_mul_pd(r7,rC);\n            r8 = _mm256_add_pd(r8,rF);\n\
    \            r9 = _mm256_mul_pd(r9,rE);\n            rA = _mm256_sub_pd(rA,rD);\n\
    \            rB = _mm256_mul_pd(rB,rC);\n\n            r0 = _mm256_mul_pd(r0,rC);\n\
    \            r1 = _mm256_add_pd(r1,rD);\n            r2 = _mm256_mul_pd(r2,rE);\n\
    \            r3 = _mm256_sub_pd(r3,rF);\n            r4 = _mm256_mul_pd(r4,rC);\n\
    \            r5 = _mm256_add_pd(r5,rD);\n            r6 = _mm256_mul_pd(r6,rE);\n\
    \            r7 = _mm256_sub_pd(r7,rF);\n            r8 = _mm256_mul_pd(r8,rC);\n\
    \            r9 = _mm256_add_pd(r9,rD);\n            rA = _mm256_mul_pd(rA,rE);\n\
    \            rB = _mm256_sub_pd(rB,rF);\n\n            r0 = _mm256_add_pd(r0,rF);\n\
    \            r1 = _mm256_mul_pd(r1,rE);\n            r2 = _mm256_sub_pd(r2,rD);\n\
    \            r3 = _mm256_mul_pd(r3,rC);\n            r4 = _mm256_add_pd(r4,rF);\n\
    \            r5 = _mm256_mul_pd(r5,rE);\n            r6 = _mm256_sub_pd(r6,rD);\n\
    \            r7 = _mm256_mul_pd(r7,rC);\n            r8 = _mm256_add_pd(r8,rF);\n\
    \            r9 = _mm256_mul_pd(r9,rE);\n            rA = _mm256_sub_pd(rA,rD);\n\
    \            rB = _mm256_mul_pd(rB,rC);\n\n            i++;\n        }\n\n   \
    \     //  Need to renormalize to prevent denormal/overflow.\n        r0 = _mm256_and_pd(r0,MASK);\n\
    \        r1 = _mm256_and_pd(r1,MASK);\n        r2 = _mm256_and_pd(r2,MASK);\n\
    \        r3 = _mm256_and_pd(r3,MASK);\n        r4 = _mm256_and_pd(r4,MASK);\n\
    \        r5 = _mm256_and_pd(r5,MASK);\n        r6 = _mm256_and_pd(r6,MASK);\n\
    \        r7 = _mm256_and_pd(r7,MASK);\n        r8 = _mm256_and_pd(r8,MASK);\n\
    \        r9 = _mm256_and_pd(r9,MASK);\n        rA = _mm256_and_pd(rA,MASK);\n\
    \        rB = _mm256_and_pd(rB,MASK);\n        r0 = _mm256_or_pd(r0,vONE);\n \
    \       r1 = _mm256_or_pd(r1,vONE);\n        r2 = _mm256_or_pd(r2,vONE);\n   \
    \     r3 = _mm256_or_pd(r3,vONE);\n        r4 = _mm256_or_pd(r4,vONE);\n     \
    \   r5 = _mm256_or_pd(r5,vONE);\n        r6 = _mm256_or_pd(r6,vONE);\n       \
    \ r7 = _mm256_or_pd(r7,vONE);\n        r8 = _mm256_or_pd(r8,vONE);\n        r9\
    \ = _mm256_or_pd(r9,vONE);\n        rA = _mm256_or_pd(rA,vONE);\n        rB =\
    \ _mm256_or_pd(rB,vONE);\n\n        c++;\n    }\n\n    r0 = _mm256_add_pd(r0,r1);\n\
    \    r2 = _mm256_add_pd(r2,r3);\n    r4 = _mm256_add_pd(r4,r5);\n    r6 = _mm256_add_pd(r6,r7);\n\
    \    r8 = _mm256_add_pd(r8,r9);\n    rA = _mm256_add_pd(rA,rB);\n\n    r0 = _mm256_add_pd(r0,r2);\n\
    \    r4 = _mm256_add_pd(r4,r6);\n    r8 = _mm256_add_pd(r8,rA);\n\n    r0 = _mm256_add_pd(r0,r4);\n\
    \    r0 = _mm256_add_pd(r0,r8);\n\n    //  Prevent Dead Code Elimination\n   \
    \ double out = 0;\n    __m256d temp = r0;\n    out += ((double*)&temp)[0];\n \
    \   out += ((double*)&temp)[1];\n    out += ((double*)&temp)[2];\n    out += ((double*)&temp)[3];\n\
    \n    return out;\n}\n\nvoid test_dp_mac_AVX(int tds,uint64 iterations){\n\n \
    \   double *sum = (double*)malloc(tds * sizeof(double));\n    double start = omp_get_wtime();\n\
    \n#pragma omp parallel num_threads(tds)\n    {\n        double ret = test_dp_mac_AVX(1.1,2.1,iterations);\n\
    \        sum[omp_get_thread_num()] = ret;\n    }\n\n    double secs = omp_get_wtime()\
    \ - start;\n    uint64 ops = 48 * 1000 * iterations * tds * 4;\n    cout << \"\
    Seconds = \" << secs << endl;\n    cout << \"FP Ops  = \" << ops << endl;\n  \
    \  cout << \"FLOPs   = \" << ops / secs << endl;\n\n    double out = 0;\n    int\
    \ c = 0;\n    while (c < tds){\n        out += sum[c++];\n    }\n\n    cout <<\
    \ \"sum = \" << out << endl;\n    cout << endl;\n\n    free(sum);\n}\n\nint main(){\n\
    \    //  (threads, iterations)\n    test_dp_mac_AVX(8,10000000);\n\n    system(\"\
    pause\");\n}\n\nOutput (1 thread, 10000000 iterations) - Compiled with Visual\
    \ Studio 2010 SP1 - x64 Release:\nSeconds = 57.4679\nFP Ops  = 1920000000000\n\
    FLOPs   = 3.34099e+010\nsum = 4.45305\n\nTheoretical AVX peak is 8 flops * 4.4\
    \ GHz = 35.2 GFlops. Actual is 33.4 GFlops.\nOutput (8 threads, 10000000 iterations)\
    \ - Compiled with Visual Studio 2010 SP1 - x64 Release:\nSeconds = 111.119\nFP\
    \ Ops  = 15360000000000\nFLOPs   = 1.3823e+011\nsum = 35.6244\n\nTheoretical AVX\
    \ peak is 8 flops * 4 cores * 4.4 GHz = 140.8 GFlops. Actual is 138.2 GFlops.\n\
    \nNow for some explanations:\nThe performance critical part is obviously the 48\
    \ instructions inside the inner loop. You'll notice that it's broken into 4 blocks\
    \ of 12 instructions each. Each of these 12 instructions blocks are completely\
    \ independent from each other - and take on average 6 cycles to execute.\nSo there's\
    \ 12 instructions and 6 cycles between issue-to-use. The latency of multiplication\
    \ is 5 cycles, so it's just enough to avoid latency stalls.\nThe normalization\
    \ step is needed to keep the data from over/underflowing. This is needed since\
    \ the do-nothing code will slowly increase/decrease the magnitude of the data.\n\
    So it's actually possible to do better than this if you just use all zeros and\
    \ get rid of the normalization step. However, since I wrote the benchmark to measure\
    \ power consumption and temperature, I had to make sure the flops were on \"real\"\
    \ data, rather than zeros - as the execution units may very well have special\
    \ case-handling for zeros that use less power and produce less heat.\n\nMore Results:\n\
    \nIntel Core i7 920 @ 3.5 GHz\nWindows 7 Ultimate x64\nVisual Studio 2010 SP1\
    \ - x64 Release\n\nThreads: 1\nSeconds = 72.1116\nFP Ops  = 960000000000\nFLOPs\
    \   = 1.33127e+010\nsum = 2.22652\n\nTheoretical SSE Peak: 4 flops * 3.5 GHz =\
    \ 14.0 GFlops. Actual is 13.3 GFlops.\nThreads: 8\nSeconds = 149.576\nFP Ops \
    \ = 7680000000000\nFLOPs   = 5.13452e+010\nsum = 17.8122\n\nTheoretical SSE Peak:\
    \ 4 flops * 4 cores * 3.5 GHz = 56.0 GFlops. Actual is 51.3 GFlops.\nMy processor\
    \ temps hit 76C on the multi-threaded run! If you runs these, be sure the results\
    \ aren't affected by CPU throttling."
- - How do I achieve the theoretical maximum of 4 FLOPs per cycle?
  - "\nThere's a point in the Intel architecture that people often forget, the dispatch\
    \ ports are shared between Int and FP/SIMD. This means that you will only get\
    \ a certain amount of bursts of FP/SIMD before the loop logic will create bubbles\
    \ in your floating point stream. Mystical got more flops out of his code, because\
    \ he used longer strides in his unrolled loop.\nIf you look at the Nehalem/Sandy\
    \ Bridge architecture here \nhttp://www.realworldtech.com/page.cfm?ArticleID=RWT091810191937&p=6\n\
    it's quite clear what happens.\nIn contrast, it should be easier to reach peak\
    \ performance on AMD (Bulldozer) as the INT and FP/SIMD pipes have separate issue\
    \ ports with their own scheduler.\nThis is only theoretical as I have neither\
    \ of these processors to test.\n"
- - How do I achieve the theoretical maximum of 4 FLOPs per cycle?
  - "\nBranches can definitely keep you from sustaining peak theoretical performance.\
    \  Do you see a difference if you manually do some loop-unrolling?  For example,\
    \ if you put 5 or 10 times as many ops per loop iteration:\nfor(int i=0; i<loops/5;\
    \ i++) {\n      mul1*=mul; mul2*=mul; mul3*=mul; mul4*=mul; mul5*=mul;\n     \
    \ sum1+=add; sum2+=add; sum3+=add; sum4+=add; sum5+=add;\n      mul1*=mul; mul2*=mul;\
    \ mul3*=mul; mul4*=mul; mul5*=mul;\n      sum1+=add; sum2+=add; sum3+=add; sum4+=add;\
    \ sum5+=add;\n      mul1*=mul; mul2*=mul; mul3*=mul; mul4*=mul; mul5*=mul;\n \
    \     sum1+=add; sum2+=add; sum3+=add; sum4+=add; sum5+=add;\n      mul1*=mul;\
    \ mul2*=mul; mul3*=mul; mul4*=mul; mul5*=mul;\n      sum1+=add; sum2+=add; sum3+=add;\
    \ sum4+=add; sum5+=add;\n      mul1*=mul; mul2*=mul; mul3*=mul; mul4*=mul; mul5*=mul;\n\
    \      sum1+=add; sum2+=add; sum3+=add; sum4+=add; sum5+=add;\n   }\n\n"
- - Why use static_cast<int>(x) instead of (int)x?
  - "\nThe main reason is that classic C casts make no distinction between what we\
    \ call static_cast<>(), reinterpret_cast<>(), const_cast<>(), and dynamic_cast<>().\
    \ These four things are completely different.\nA static_cast<>() is usually safe.\
    \ There is a valid conversion in the language, or an appropriate constructor that\
    \ makes it possible. The only time it's a bit risky is when you cast down to an\
    \ inherited class; you must make sure that the object is a actually the descendant\
    \ that you claim it is, by means external to the language (like a flag in the\
    \ object). A dynamic_cast<>() is safe as long as the result is checked (pointer)\
    \ or a possible exception is taken into account (reference). \nA reinterpret_cast<>()\
    \ (or a const_cast<>()) on the other hand is always dangerous. You tell the compiler:\
    \ \"trust me: I know this doesn't look like a foo (this looks as if it isn't mutable),\
    \ but it is\". \nThe first problem is that it's almost impossible to tell which\
    \ one will occur in a C-style cast without looking at large and disperse pieces\
    \ of code and knowing all the rules.\nLet's assume these:\nclass CMyClass : public\
    \ CMyBase {...};\nclass CMyOtherStuff {...} ;\n\nCMyBase  *pSomething; // filled\
    \ somewhere\n\nNow, these two are compiled the same way:\nCMyClass *pMyObject;\n\
    pMyObject = static_cast<CMyClass*>(pSomething); // Safe; as long as we checked\n\
    \npMyObject = (CMyClass*)(pSomething); // Same as static_cast<>\n            \
    \                         // Safe; as long as we checked\n                   \
    \                  // but harder to read\n\nHowever, let's see this almost identical\
    \ code:\nCMyOtherStuff *pOther;\npOther = static_cast<CMyOtherStuff*>(pSomething);\
    \ // Compiler error: Can't convert\n\npOther = (CMyOtherStuff*)(pSomething); \
    \           // No compiler error.\n                                          \
    \        // Same as reinterpret_cast<>\n                                     \
    \             // and it's wrong!!!\n\nAs you can see, there is no easy way to\
    \ distinguish between the two situations without knowing a lot about all the classes\
    \ involved.\nThe second problem is that the C-style casts are too hard to locate.\
    \ In complex expressions it can be very hard to see C-style casts. It is virtually\
    \ impossible to write an automated tool that needs to locate C-style casts (for\
    \ example a search tool) without a full blown C++ compiler front-end. On the other\
    \ hand, it's easy to search for \"static_cast<\" or \"reinterpret_cast<\".\npOther\
    \ = reinterpret_cast<CMyOtherStuff*>(pSomething);\n      // No compiler error.\n\
    \      // but the presence of a reinterpret_cast<> is \n      // like a Siren\
    \ with Red Flashing Lights in your code.\n      // The mere typing of it should\
    \ cause you to feel VERY uncomfortable.\n\nThat means that, not only are C-style\
    \ casts more dangerous, but it's a lot harder to find them all to make sure that\
    \ they are correct.\n"
- - Why use static_cast<int>(x) instead of (int)x?
  - "\nOne pragmatic tip: you can search easily for the static_cast keyword in your\
    \ source code if you plan to tidy up the project.\n"
- - Why use static_cast<int>(x) instead of (int)x?
  - "\n\nIn short:\n\nstatic_cast<>() gives you a compile time checking ability, C-Style\n\
    \  cast doesn't.\nstatic_cast<>() can be spotted easily\n  anywhere inside a C++\
    \ source code; in contrast, C_Style cast is harder to spot.\nIntentions are conveyed\
    \ much better using C++ casts.\n\nMore Explanation:\nThe static cast performs\
    \ conversions between compatible types. It\n  is similar to the C-style cast,\
    \ but is more restrictive. For example,\n  the C-style cast would allow an integer\
    \ pointer to point to a char.\nchar c = 10;       // 1 byte\nint *p = (int*)&c;\
    \ // 4 bytes\n\nSince this results in a 4-byte pointer pointing to 1 byte of allocated\n\
    \  memory, writing to this pointer will either cause a run-time error or\n  will\
    \ overwrite some adjacent memory.\n*p = 5; // run-time error: stack corruption\n\
    \nIn contrast to the C-style cast, the static cast will allow the\n  compiler\
    \ to check that the pointer and pointee data types are\n  compatible, which allows\
    \ the programmer to catch this incorrect\n  pointer assignment during compilation.\n\
    int *q = static_cast<int*>(&c); // compile-time error\n\n\nRead more on:\nWhat\
    \ is the difference between static_cast<> and C style casting \nand\nRegular cast\
    \ vs. static_cast vs. dynamic_cast  \n"
- - How to detect integer overflow?
  - "\nI see you're using unsigned integers. By definition, in C (don't know about\
    \ C++), unsigned arithmetic does not overflow ... so, at least for C, your point\
    \ is moot :)\nWith signed integers, once there has been overflow, Undefined Behaviour\
    \ has occurred and your program can do anything (for example: render tests inconclusive).Â \n\
    #include <limits.h>\nint a = <something>;\nint x = <something>;\na += x;     \
    \         /* UB */\nif (a < 0) {         /* unreliable test */\n  /* ... */\n\
    }\n\nTo create a conforming program you need to test for overflow before generating\
    \ said overflow. The method can be used with unsigned integers too\n// for addition\n\
    #include <limits.h>\nint a = <something>;\nint x = <something>;\nif ((x > 0) &&\
    \ (a > INT_MAX - x)) /* `a + x` would overflow */;\nif ((x < 0) && (a < INT_MIN\
    \ - x)) /* `a + x` would underflow */;\n\n\n// for subtraction\n#include <limits.h>\n\
    int a = <something>;\nint x = <something>;\nif ((x < 0) && (a > INT_MAX + x))\
    \ /* `a - x` would overflow */;\nif ((x > 0) && (a < INT_MIN + x)) /* `a - x`\
    \ would underflow */;\n\n\n// for multiplication\n#include <limits.h>\nint a =\
    \ <something>;\nint x = <something>;\nif (a > INT_MAX / x) /* `a * x` would overflow\
    \ */;\nif ((a < INT_MIN / x)) /* `a * x` would underflow */;\n// there may be\
    \ need to check for -1 for two's complement machines\nif ((a == -1) && (x == INT_MIN))\
    \ /* `a * x` can overflow */\nif ((x == -1) && (a == INT_MIN)) /* `a * x` (or\
    \ `a / x`) can overflow */\n\n\nfor division (except for the INT_MIN and -1 special\
    \ case) there is no possibility of going over INT_MIN or INT_MAX.\n"
- - How to detect integer overflow?
  - "\nThere is a way to determine whether an operation is likely to overflow, using\
    \ the positions of the most-significant one-bits in the operands and a little\
    \ basic binary-math knowledge.\nFor addition, any two operands will result in\
    \ (at most) one bit more than the largest operand's highest one-bit. For example:\n\
    bool addition_is_safe(uint32_t a, uint32_t b) {\n    size_t a_bits=highestOneBitPosition(a),\
    \ b_bits=highestOneBitPosition(b);\n    return (a_bits<32 && b_bits<32);\n}\n\n\
    For multiplication, any two operands will result in (at most) the sum of the bits\
    \ of the operands. For example:\nbool multiplication_is_safe(uint32_t a, uint32_t\
    \ b) {\n    size_t a_bits=highestOneBitPosition(a), b_bits=highestOneBitPosition(b);\n\
    \    return (a_bits+b_bits<=32);\n}\n\nSimilarly, you can estimate the maximum\
    \ size of the result of a to the power of b like this:\nbool exponentiation_is_safe(uint32_t\
    \ a, uint32_t b) {\n    size_t a_bits=highestOneBitPosition(a);\n    return (a_bits*b<=32);\n\
    }\n\n(Substitute the number of bits for your target integer, of course.)\nI'm\
    \ not sure of the fastest way to determine the position of the highest one-bit\
    \ in a number, here's a brute-force method:\nsize_t highestOneBitPosition(uint32_t\
    \ a) {\n    size_t bits=0;\n    while (a!=0) {\n        ++bits;\n        a>>=1;\n\
    \    };\n    return bits;\n}\n\nIt's not perfect, but that'll give you a good\
    \ idea whether any two numbers could overflow before you do the operation. I don't\
    \ know whether it would be faster than simply checking the result the way you\
    \ suggested, because of the loop in the highestOneBitPosition function, but it\
    \ might (especially if you knew how many bits were in the operands beforehand).\n"
- - How to detect integer overflow?
  - "\nClang 3.4+ and GCC 5+ offer checked arithmetic builtins. They offer a very\
    \ fast solution to this problem, especially when compared to bit-testing safety\
    \ checks.\nFor the example in OP's question, it would work like that:\nunsigned\
    \ long b, c, c_test;\nif (__builtin_umull_overflow(b, c, &c_test))\n{\n    //\
    \ returned non-zero: there has been an overflow\n}\nelse\n{\n    // return zero:\
    \ there hasn't been an overflow\n}\n\nThe Clang documentation doesn't specify\
    \ whether c_test contains the overflowed result if an overflow occurred, but the\
    \ GCC documentation says that it does. Given that these two like to be __builtin-compatible,\
    \ it's probably safe to assume that this is how Clang works too.\nThere is a __builtin\
    \ for each arithmetic operation that can overflow (addition, subtraction, multiplication),\
    \ with signed and unsigned variants, for int sizes, long sizes, and long long\
    \ sizes. The syntax for the name is __builtin_[us](operation)(l?l?)_overflow:\n\
    \nu for unsigned or s for signed;\noperation is one of add, sub or mul;\nno l\
    \ suffix means that the operands are ints; one l means long; two ls mean long\
    \ long.\n\nSo for a checked signed long integer addition, it would be __builtin_saddl_overflow.\
    \ The full list can be found on the Clang documentation page.\nGCC 5+ and Clang\
    \ 3.8+ additionally offer generic builtins that work without specifying the type\
    \ of the values: __builtin_add_overflow, __builtin_sub_overflow and __builtin_mul_overflow.\
    \ These also work on types smaller than int. \nThe builtins lower to what's best\
    \ for the platform. On x86, they check the carry, overflow and sign flags.\nVisual\
    \ Studio's cl.exe doesn't have direct equivalents. For unsigned additions and\
    \ subtractions, including <intrin.h> will allow you to use addcarry_uNN and subborrow_uNN\
    \ (where NN is the number of bits, like addcarry_u8 or subborrow_u64). Their signature\
    \ is a bit obtuse:\nunsigned char _addcarry_u32(unsigned char c_in, unsigned int\
    \ src1, unsigned int src2, unsigned int *sum);\nunsigned char _subborrow_u32(unsigned\
    \ char b_in, unsigned int src1, unsigned int src2, unsigned int *diff);\n\nc_in/b_in\
    \ is the carry/borrow flag on input, the return value is the carry/borrow on output.\
    \ It does not appear to have equivalents for signed operations or multiplications.\n\
    Otherwise, Clang for Windows is now production-ready (good enough for Chrome),\
    \ so that could be an option, too.\n"
- - Appending a vector to a vector [duplicate]
  - "\na.insert(a.end(), b.begin(), b.end());\n\nor\na.insert(std::end(a), std::begin(b),\
    \ std::end(b));\n\nThe second variant is a more generically applicable solution,\
    \ as b could also be an array. However, it requires C++11\n"
- - Appending a vector to a vector [duplicate]
  - "\nstd::copy (b.begin(), b.end(), std::back_inserter(a));\n\nThis can be used\
    \ in case the items in vector a have no assignment operator (e.g. const member).\n\
    In all other cases this solution is ineffiecent compared to the above insert solution.\n"
- - Appending a vector to a vector [duplicate]
  - "\nWhile saying \"the compiler can reserve\", why rely on it?  And what about\
    \ automatic detection of move semantics?  And what about all that repeating of\
    \ the container name with the begins and ends?\nWouldn't you want something, you\
    \ know, simpler?\n(Scroll down to main for the punchline)\n#include <type_traits>\n\
    #include <vector>\n#include <iterator>\n#include <iostream>\n\ntemplate<typename\
    \ C,typename=void> struct can_reserve: std::false_type {};\n\ntemplate<typename\
    \ T, typename A>\nstruct can_reserve<std::vector<T,A>,void>:\n    std::true_type\n\
    {};\n\ntemplate<int n> struct secret_enum { enum class type {}; };\ntemplate<int\
    \ n>\nusing SecretEnum = typename secret_enum<n>::type;\n\ntemplate<bool b, int\
    \ override_num=1>\nusing EnableFuncIf = typename std::enable_if< b, SecretEnum<override_num>\
    \ >::type;\ntemplate<bool b, int override_num=1>\nusing DisableFuncIf = EnableFuncIf<\
    \ !b, -override_num >;\n\ntemplate<typename C, EnableFuncIf< can_reserve<C>::value\
    \ >... >\nvoid try_reserve( C& c, std::size_t n ) {\n  c.reserve(n);\n}\ntemplate<typename\
    \ C, DisableFuncIf< can_reserve<C>::value >... >\nvoid try_reserve( C& c, std::size_t\
    \ ) { } // do nothing\n\ntemplate<typename C,typename=void>\nstruct has_size_method:std::false_type\
    \ {};\ntemplate<typename C>\nstruct has_size_method<C, typename std::enable_if<std::is_same<\n\
    \  decltype( std::declval<C>().size() ),\n  decltype( std::declval<C>().size()\
    \ )\n>::value>::type>:std::true_type {};\n\nnamespace adl_aux {\n  using std::begin;\
    \ using std::end;\n  template<typename C>\n  auto adl_begin(C&&c)->decltype( begin(std::forward<C>(c))\
    \ );\n  template<typename C>\n  auto adl_end(C&&c)->decltype( end(std::forward<C>(c))\
    \ );\n}\ntemplate<typename C>\nstruct iterable_traits {\n    typedef decltype(\
    \ adl_aux::adl_begin(std::declval<C&>()) ) iterator;\n    typedef decltype( adl_aux::adl_begin(std::declval<C\
    \ const&>()) ) const_iterator;\n};\ntemplate<typename C> using Iterator = typename\
    \ iterable_traits<C>::iterator;\ntemplate<typename C> using ConstIterator = typename\
    \ iterable_traits<C>::const_iterator;\ntemplate<typename I> using IteratorCategory\
    \ = typename std::iterator_traits<I>::iterator_category;\n\ntemplate<typename\
    \ C, EnableFuncIf< has_size_method<C>::value, 1>... >\nstd::size_t size_at_least(\
    \ C&& c ) {\n    return c.size();\n}\n\ntemplate<typename C, EnableFuncIf< !has_size_method<C>::value\
    \ &&\n  std::is_base_of< std::random_access_iterator_tag, IteratorCategory<Iterator<C>>\
    \ >::value, 2>... >\nstd::size_t size_at_least( C&& c ) {\n    using std::begin;\
    \ using std::end;\n  return end(c)-begin(c);\n};\ntemplate<typename C, EnableFuncIf<\
    \ !has_size_method<C>::value &&\n  !std::is_base_of< std::random_access_iterator_tag,\
    \ IteratorCategory<Iterator<C>> >::value, 3>... >\nstd::size_t size_at_least(\
    \ C&& c ) {\n  return 0;\n};\n\ntemplate < typename It >\nauto try_make_move_iterator(It\
    \ i, std::true_type)\n-> decltype(make_move_iterator(i))\n{\n    return make_move_iterator(i);\n\
    }\ntemplate < typename It >\nIt try_make_move_iterator(It i, ...)\n{\n    return\
    \ i;\n}\n\n\n#include <iostream>\ntemplate<typename C1, typename C2>\nC1&& append_containers(\
    \ C1&& c1, C2&& c2 )\n{\n  using std::begin; using std::end;\n  try_reserve( c1,\
    \ size_at_least(c1) + size_at_least(c2) );\n\n  using is_rvref = std::is_rvalue_reference<C2&&>;\n\
    \  c1.insert( end(c1),\n             try_make_move_iterator(begin(c2), is_rvref{}),\n\
    \             try_make_move_iterator(end(c2), is_rvref{}) );\n\n  return std::forward<C1>(c1);\n\
    }\n\nstruct append_infix_op {} append;\ntemplate<typename LHS>\nstruct append_on_right_op\
    \ {\n  LHS lhs;\n  template<typename RHS>\n  LHS&& operator=( RHS&& rhs ) {\n\
    \    return append_containers( std::forward<LHS>(lhs), std::forward<RHS>(rhs)\
    \ );\n  }\n};\n\ntemplate<typename LHS>\nappend_on_right_op<LHS> operator+( LHS&&\
    \ lhs, append_infix_op ) {\n  return { std::forward<LHS>(lhs) };\n}\ntemplate<typename\
    \ LHS,typename RHS>\ntypename std::remove_reference<LHS>::type operator+( append_on_right_op<LHS>&&\
    \ lhs, RHS&& rhs ) {\n  typename std::decay<LHS>::type retval = std::forward<LHS>(lhs.lhs);\n\
    \  return append_containers( std::move(retval), std::forward<RHS>(rhs) );\n}\n\
    \ntemplate<typename C>\nvoid print_container( C&& c ) {\n  for( auto&& x:c )\n\
    \    std::cout << x << \",\";\n  std::cout << \"\\n\";\n};\n\nint main() {\n \
    \ std::vector<int> a = {0,1,2};\n  std::vector<int> b = {3,4,5};\n  print_container(a);\n\
    \  print_container(b);\n  a +append= b;\n  const int arr[] = {6,7,8};\n  a +append=\
    \ arr;\n  print_container(a);\n  print_container(b);\n  std::vector<double> d\
    \ = ( std::vector<double>{-3.14, -2, -1} +append= a );\n  print_container(d);\n\
    \  std::vector<double> c = std::move(d) +append+ a;\n  print_container(c);\n \
    \ print_container(d);\n  std::vector<double> e = c +append+ std::move(a);\n  print_container(e);\n\
    \  print_container(a);\n}\n\nhehe.\nNow with move-data-from-rhs, append-array-to-container,\
    \ append forward_list-to-container, move-container-from-lhs, thanks to @DyP's\
    \ help.\nNote that the above does not compile in clang thanks to the EnableFunctionIf<>...\
    \ technique.  In clang this workaround works.\n"
- - When can I use a forward declaration?
  - "\nPut yourself in the compiler's position: when you forward declare a type, all\
    \ the compiler knows is that this type exists; it knows nothing about its size,\
    \ members, or methods. This is why it's called an incomplete type. Therefore,\
    \ you cannot use the type to declare a member, or a base class, since the compiler\
    \ would need to know the layout of the type.\nAssuming the following forward declaration.\n\
    class X;\n\nHere's what you can and cannot do.\nWhat you can do with an incomplete\
    \ type:\n\nDeclare a member to be a pointer or a reference to the incomplete type:\n\
    class Foo {\n    X *pt;\n    X &pt;\n};\n\nDeclare functions or methods which\
    \ accept/return incomplete types:\nvoid f1(X);\nX    f2();\n\nDefine functions\
    \ or methods which accept/return pointers/references to the incomplete type (but\
    \ without using its members):\nvoid f3(X*, X&) {}\nX&   f4()       {}\nX*   f5()\
    \       {}\n\n\nWhat you cannot do with an incomplete type:\n\nUse it as a base\
    \ class\nclass Foo : X {} // compiler error!\n\nUse it to declare a member:\n\
    class Foo {\n    X m; // compiler error!\n};\n\nDefine functions or methods using\
    \ this type\nvoid f1(X x) {} // compiler error!\nX    f2()    {} // compiler error!\n\
    \nUse its methods or fields, in fact trying to dereference a variable with incomplete\
    \ type\nclass Foo {\n    X *m;            \n    void method()            \n  \
    \  {\n        m->someMethod();      // compiler error!\n        int i = m->someField;\
    \ // compiler error!\n    }\n};\n\n\n\nWhen it comes to templates, there is no\
    \ absolute rule: whether you can use an incomplete type as a template parameter\
    \ is dependent on the way the type is used in the template.\nFor instance, std::vector<T>\
    \ requires its parameter to be a complete type, while boost::container::vector<T>\
    \ does not. Sometimes, a complete type is required only if you use certain member\
    \ functions; this is the case for std::unique_ptr<T>, for example.\nA well-documented\
    \ template should indicate in its documentation all the requirements of its parameters,\
    \ including whether they need to be complete types or not.\n"
- - When can I use a forward declaration?
  - "\nThe main rule is that you can only forward-declare classes whose memory layout\
    \ (and thus member functions and data members) do not need to be known in the\
    \ file you forward-declare it.\nThis would rule out base classes and anything\
    \ but classes used via references and pointers.\n"
- - When can I use a forward declaration?
  - "\nLakos distinguishes between class usage\n\nin-name-only (for which a forward\
    \ declaration is sufficient) and\nin-size (for which the class definition is needed).\n\
    \nI've never seen it pronounced more succinctly :)\n"
- - Use 'class' or 'typename' for template parameters? [duplicate]
  - "\nStan Lippman talked about this here.  I thought it was interesting.\nSummary:\
    \ Stroustrup originally used class to specify types in templates to avoid introducing\
    \ a new keyword. Some in the committee worried that this overloading of the keyword\
    \ led to confusion. Later, the committee introduced a new keyword typename to\
    \ resolve syntactic ambiguity, and decided to let it also be used to specify template\
    \ types to reduce confusion, but for backward compatibility, class kept its overloaded\
    \ meaning.\n"
- - Use 'class' or 'typename' for template parameters? [duplicate]
  - "\nAccording to Scott Myers, Effective C++ (3rd ed.) item 42 (which must, of course,\
    \ be the ultimate answer)  - the difference is \"nothing\".   \nAdvice is to use\
    \ \"class\" if it is expected T will always be a class, with \"typename\"  if\
    \ other types (int, char* whatever) may be expected.  Consider it a usage hint.\n"
- - Use 'class' or 'typename' for template parameters? [duplicate]
  - "\nAs an addition to all above posts, the use of the class keyword is forced (up\
    \ to and including C++14) when dealing with template template parameters, e.g.:\n\
    template <template <typename, typename> class Container, typename Type>\nclass\
    \ MyContainer: public Container<Type, std::allocator<Type>>\n{ /*...*/ };\n\n\
    In this example, typename Container would have generated a compiler error, something\
    \ like this:\nerror: expected 'class' before 'Container'\n\n"
- - What is the easiest way to initialize a std::vector with hardcoded elements?
  - "\nOne method would be to use the array to initialize the vector\nstatic const\
    \ int arr[] = {16,2,77,29};\nvector<int> vec (arr, arr + sizeof(arr) / sizeof(arr[0])\
    \ );\n\n"
- - What is the easiest way to initialize a std::vector with hardcoded elements?
  - "\nIf your compiler supports C++11, you can simply do:\nstd::vector<int> v = {1,\
    \ 2, 3, 4};\n\nThis is available in GCC as of version 4.4. Unfortunately, VC++\
    \ 2010 seems to be lagging behind in this respect.\nAlternatively, the Boost.Assign\
    \ library uses non-macro magic to allow the following:\n#include <boost/assign/list_of.hpp>\n\
    ...\nstd::vector<int> v = boost::assign::list_of(1)(2)(3)(4);\n\nOr:\n#include\
    \ <boost/assign/std/vector.hpp>\nusing namespace boost::assign;\n...\nstd::vector<int>\
    \ v;\nv += 1, 2, 3, 4;\n\nBut keep in mind that this has some overhead (basically,\
    \ list_of constructs a std::deque under the hood) so for performance-critical\
    \ code you'd be better off doing as Yacoby says.\n"
- - What is the easiest way to initialize a std::vector with hardcoded elements?
  - "\nIn C++0x you will be able to do it in the same way that you did with an array,\
    \ but not in the current standard.\nWith only language support you can use:\n\
    int tmp[] = { 10, 20, 30 };\nstd::vector<int>Â v(Â tmp,Â tmp+3Â );Â //Â useÂ someÂ utilityÂ toÂ avoid\
    \ hardcoding the size here\n\nIf you can add other libraries you could try boost::assignment:\n\
    vector<int> v = list_of(10)(20)(30);\n\nTo avoid hardcoding the size of an array:\n\
    // option 1, typesafe, not a compile time constant\ntemplate <typename T, std::size_t\
    \ N>\ninline std::size_t size_of_array( T (&)[N] ) {\n   return N;\n}\n// option\
    \ 2, not typesafe, compile time constant\n#define ARRAY_SIZE(x) (sizeof(x) / sizeof(x[0]))\n\
    \n// option 3, typesafe, compile time constant\ntemplate <typename T, std::size_t\
    \ N>\nchar (&sizeof_array( T(&)[N] ))[N];    // declared, undefined\n#define ARRAY_SIZE(x)\
    \ sizeof(sizeof_array(x))\n\n"
- - How to find out if an item is present in a std::vector?
  - "\nYou can use std::find from <algorithm>:\nstd::find(vector.begin(), vector.end(),\
    \ item) != vector.end()\n\nThis returns a bool (true if present, false otherwise).\
    \  With your example:\n#include <algorithm>\n\nif ( std::find(vector.begin(),\
    \ vector.end(), item) != vector.end() )\n   do_this();\nelse\n   do_that();\n\n"
- - How to find out if an item is present in a std::vector?
  - "\nAs others have said, use the STL find or find_if functions. But if you are\
    \ searching in very large vectors and this impacts performance, you may want to\
    \ sort your vector and then use the binary_search, lower_bound, or upper_bound\
    \ algorithms.\n"
- - How to find out if an item is present in a std::vector?
  - "\nUse find from the algorithm header of stl.I've illustrated its use with int\
    \ type. You can use any type you like as long as you can compare for equality\
    \ (overload == if you need to for your custom class).\n#include <algorithm>\n\
    #include <vector>\n\nusing namespace std;\nint main()\n{   \n    typedef vector<int>\
    \ IntContainer;\n    typedef IntContainer::iterator IntIterator;\n\n    IntContainer\
    \ vw;\n\n    //...\n\n    // find 5\n    IntIterator i = find(vw.begin(), vw.end(),\
    \ 5);\n\n    if (i != vw.end()) {\n        // found it\n    } else {\n       \
    \ // doesn't exist\n    }\n\n    return 0;\n}\n\n"
- - Are the days of passing const std::string & as a parameter over?
  - "\nThe reason Herb said what he said is because of cases like this.\nLet's say\
    \ I have function A which calls function B, which calls function C. And A passes\
    \ a string through B and into C. A does not know or care about C; all A knows\
    \ about is B. That is, C is an implementation detail of B.\nLet's say that A is\
    \ defined as follows:\nvoid A()\n{\n  B(\"value\");\n}\n\nIf B and C take the\
    \ string by const&, then it looks something like this:\nvoid B(const std::string\
    \ &str)\n{\n  C(str);\n}\n\nvoid C(const std::string &str)\n{\n  //Do something\
    \ with `str`. Does not store it.\n}\n\nAll well and good. You're just passing\
    \ pointers around, no copying, no moving, everyone's happy. C takes a const& because\
    \ it doesn't store the string. It simply uses it.\nNow, I want to make one simple\
    \ change: C needs to store the string somewhere.\nvoid C(const std::string &str)\n\
    {\n  //Do something with `str`.\n  m_str = str;\n}\n\nHello, copy constructor\
    \ and potential memory allocation (ignore the Short String Optimization (SSO)).\
    \ C++11's move semantics are supposed to make it possible to remove needless copy-constructing,\
    \ right? And A passes a temporary; there's no reason why C should have to copy\
    \ the data. It should just abscond with what was given to it.\nExcept it can't.\
    \ Because it takes a const&.\nIf I change C to take its parameter by value, that\
    \ just causes B to do the copy into that parameter; I gain nothing.\nSo if I had\
    \ just passed str by value through all of the functions, relying on std::move\
    \ to shuffle the data around, we wouldn't have this problem. If someone wants\
    \ to hold on to it, they can. If they don't, oh well.\nIs it more expensive? Yes;\
    \ moving into a value is more expensive than using references. Is it less expensive\
    \ than the copy? Not for small strings with SSO. Is it worth doing?\nIt depends\
    \ on your use case. How much do you hate memory allocations?\n"
- - Are the days of passing const std::string & as a parameter over?
  - "\n\nAre the days of passing const std::string & as a parameter over?\n\nNo. Many\
    \ people take this advice (including Dave Abrahams) beyond the domain it applies\
    \ to, and simplify it to apply to all std::string parameters -- Always passing\
    \ std::string by value is not a \"best practice\" for any and all arbitrary parameters\
    \ and applications because the optimizations these talks/articles focus on apply\
    \ only to a restricted set of cases.\nIf you're returning a value, mutating the\
    \ parameter, or taking the value, then passing by value could save expensive copying\
    \ and offer syntactical convenience.\nAs ever, passing by const reference saves\
    \ much copying when you don't need a copy.\nNow to the specific example:\n\nHowever\
    \ inval is still quite a lot larger than the size of a reference (which is usually\
    \ implemented as a pointer). This is because a std::string has various components\
    \ including a pointer into the heap and a member char[] for short string optimization.\
    \ So it seems to me that passing by reference is still a good idea. Can anyone\
    \ explain why Herb might have said this?\n\nIf stack size is a concern (and assuming\
    \ this is not inlined/optimized), return_val + inval > return_val -- IOW, peak\
    \ stack usage can be reduced by passing by value here (note: oversimplification\
    \ of ABIs). Meanwhile, passing by const reference can disable the optimizations.\
    \ The primary reason here is not to avoid stack growth, but to ensure the optimization\
    \ can be performed where it is applicable.\nThe days of passing by const reference\
    \ aren't over -- the rules just more complicated than they once were. If performance\
    \ is important, you'll be wise to consider how you pass these types, based on\
    \ the details you use in your implementations.\n"
- - Are the days of passing const std::string & as a parameter over?
  - "\nThis highly depends on the compiler's implementation.\nHowever, it also depends\
    \ on what you use.\nLets consider next functions : \nbool foo1( const std::string\
    \ v )\n{\n  return v.empty();\n}\nbool foo2( const std::string & v )\n{\n  return\
    \ v.empty();\n}\n\nThese functions are implemented in a separate compilation unit\
    \ in order to avoid inlining. Then :\n1. If you pass a literal to these two functions,\
    \ you will not see much difference in performances. In both cases, a string object\
    \ has to be created\n2. If you pass another std::string object, foo2 will outperform\
    \ foo1, because foo1 will do a deep copy.\nOn my PC, using g++ 4.6.1, I got these\
    \ results :\n\nvariable by reference: 1000000000 iterations -> time elapsed: 2.25912\
    \ sec\nvariable by value: 1000000000 iterations -> time elapsed: 27.2259 sec\n\
    literal by reference: 100000000 iterations -> time elapsed: 9.10319 sec\nliteral\
    \ by value: 100000000 iterations -> time elapsed: 8.62659 sec\n\n"
- - Concatenating two std::vectors
  - "\nvector1.insert( vector1.end(), vector2.begin(), vector2.end() );\n\n"
- - Concatenating two std::vectors
  - "\nIf you are using C++11,  and wish to move the elements rather than merely copying\
    \ them, you can use std::move_iterator (http://en.cppreference.com/w/cpp/iterator/move_iterator)\
    \ along with insert (or copy):\n#include <vector>\n#include <iostream>\n#include\
    \ <iterator>\n\nint main(int argc, char** argv) {\n  std::vector<int> dest{1,2,3,4,5};\n\
    \  std::vector<int> src{6,7,8,9,10};\n\n  // Move elements from src to dest.\n\
    \  // src is left in undefined but safe-to-destruct state.\n  dest.insert(\n \
    \     dest.end(),\n      std::make_move_iterator(src.begin()),\n      std::make_move_iterator(src.end())\n\
    \    );\n\n  // Print out concatenated vector.\n  std::copy(\n      dest.begin(),\n\
    \      dest.end(),\n      std::ostream_iterator<int>(std::cout, \"\\n\")\n   \
    \ );\n\n  return 0;\n}\n\nThis will not be more efficient for the example with\
    \ ints, since moving them is no more efficient than copying them, but for a data\
    \ structure with optimized moves, it can avoid copying unnecessary state:\n#include\
    \ <vector>\n#include <iostream>\n#include <iterator>\n\nint main(int argc, char**\
    \ argv) {\n  std::vector<std::vector<int>> dest{{1,2,3,4,5}, {3,4}};\n  std::vector<std::vector<int>>\
    \ src{{6,7,8,9,10}};\n\n  // Move elements from src to dest.\n  // src is left\
    \ in undefined but safe-to-destruct state.\n  dest.insert(\n      dest.end(),\n\
    \      std::make_move_iterator(src.begin()),\n      std::make_move_iterator(src.end())\n\
    \    );\n\n  return 0;\n}\n\nAfter the move, src's element is left in an undefined\
    \ but safe-to-destruct state, and its former elements were transfered directly\
    \ to dest's new element at the end.\n"
- - Concatenating two std::vectors
  - "\nI would use the insert function, something like:\nvector<int> a, b;\n//fill\
    \ with data\nb.insert(b.end(), a.begin(), a.end());\n\n"
- - Read whole ASCII file into C++ std::string [duplicate]
  - "\nUpdate: Turns out that this method, while following STL idioms well, is actually\
    \ surprisingly inefficient! Don't do this with large files. (See: http://insanecoding.blogspot.com/2011/11/how-to-read-in-file-in-c.html)\n\
    You can make a streambuf iterator out of the file and initialize the string with\
    \ it:\n#include <string>\n#include <fstream>\n#include <streambuf>\n\nstd::ifstream\
    \ t(\"file.txt\");\nstd::string str((std::istreambuf_iterator<char>(t)),\n   \
    \              std::istreambuf_iterator<char>());\n\nNot sure where you're getting\
    \ the t.open(\"file.txt\", \"r\") syntax from. As far as I know that's not a method\
    \ that std::ifstream has. It looks like you've confused it with C's fopen.\nEdit:\
    \ Also note the extra parentheses around the first argument to the string constructor.\
    \ These are essential. They prevent the problem known as the \"most vexing parse\"\
    , which in this case won't actually give you a compile error like it usually does,\
    \ but will give you interesting (read: wrong) results.\nFollowing KeithB's point\
    \ in the comments, here's a way to do it that allocates all the memory up front\
    \ (rather than relying on the string class's automatic reallocation):\n#include\
    \ <string>\n#include <fstream>\n#include <streambuf>\n\nstd::ifstream t(\"file.txt\"\
    );\nstd::string str;\n\nt.seekg(0, std::ios::end);   \nstr.reserve(t.tellg());\n\
    t.seekg(0, std::ios::beg);\n\nstr.assign((std::istreambuf_iterator<char>(t)),\n\
    \            std::istreambuf_iterator<char>());\n\n"
- - Read whole ASCII file into C++ std::string [duplicate]
  - "\nThere are a couple of possibilities. One I like to use a stringstream as a\
    \ go-between:\nstd::ifstream t(\"file.txt\");\nstd::stringstream buffer;\nbuffer\
    \ << t.rdbuf();\n\nNow the contents of \"file.txt\" are available in a string\
    \ as buffer.str().\nAnother possibility (though I certainly don't like it as well)\
    \ is much more like your original:\nstd::ifstream t(\"file.txt\");\nt.seekg(0,\
    \ std::ios::end);\nsize_t size = t.tellg();\nstd::string buffer(size, ' ');\n\
    t.seekg(0);\nt.read(&buffer[0], size); \n\nOfficially, this isn't required to\
    \ work under the C++98 or 03 standard (string isn't required to store data contiguously)\
    \ but in fact it works with all known implementations, and C++11 and later do\
    \ require contiguous storage, so it's guaranteed to work with them.\nAs to why\
    \ I don't like the latter as well: first, because it's longer and harder to read.\
    \ Second, because it requires that you initialize the contents of the string with\
    \ data you don't care about, then immediately write over that data (yes, the time\
    \ to initialize is usually trivial compared to the reading, so it probably doesn't\
    \ matter, but to me it still feels kind of wrong). Third, in a text file, position\
    \ X in the file doesn't necessarily mean you'll have read X characters to reach\
    \ that point -- it's not required to take into account things like line-end translations.\
    \ On real systems that do such translations (e.g., Windows) the translated form\
    \ is shorter than what's in the file (i.e., \"\\r\\n\" in the file becomes \"\\\
    n\" in the translated string) so all you've done is reserved a little extra space\
    \ you never use. Again, doesn't really cause a major problem but feels a little\
    \ wrong anyway.\n"
- - Read whole ASCII file into C++ std::string [duplicate]
  - "\nI think best way is to use string stream. simple and quick !!!\nifstream inFile;\n\
    inFile.open(inFileName);//open the input file\n\nstringstream strStream;\nstrStream\
    \ << inFile.rdbuf();//read the file\nstring str = strStream.str();//str holds\
    \ the content of the file\n\ncout << str << endl;//you can do anything with the\
    \ string!!!\n\n"
- - Why is iostream::eof inside a loop condition considered wrong?
  - "\nBecause iostream::eof will only return true after reading the end of the stream.\
    \ It does not indicate, that the next read will be the end of the stream.\nConsider\
    \ this (and assume then next read will be at the end of the stream):\nwhile(!inStream.eof()){\n\
    \  int data;\n  // yay, not end of stream yet, now read ...\n  inStream >> data;\n\
    \  // oh crap, now we read the end and *only* now the eof bit will be set (as\
    \ well as the fail bit)\n  // do stuff with (now uninitialized) data\n}\n\nAgainst\
    \ this:\nint data;\nwhile(inStream >> data){\n  // when we land here, we can be\
    \ sure that the read was successful.\n  // if it wasn't, the returned stream from\
    \ operator>> would be converted to false\n  // and the loop wouldn't even be entered\n\
    \  // do stuff with correctly initialized data (hopefully)\n}\n\nAnd on your second\
    \ question: Because\nif(scanf(\"...\",...)!=EOF)\n\nis the same as\nif(!(inStream\
    \ >> data).eof())\n\nand not the same as\nif(!inStream.eof())\n    inFile >> data\n\
    \n"
- - Why is iostream::eof inside a loop condition considered wrong?
  - "\nBottom-line top:  With proper handling of white-space, the following is how\
    \ eof can be used (and even, be more reliable than fail() for error checking):\n\
    while( !(in>>std::ws).eof() ) {  \n   int data;\n   in >> data;\n   if ( in.fail()\
    \ ) /* handle with break or throw */; \n   // now use data\n}    \n\n(Thanks Tony\
    \ D for the suggestion to highlight the answer. See his comment below for an example\
    \ to why this is more robust.)\n\nThe main argument against using eof() seems\
    \ to be missing an important subtlety about the role of white space. My proposition\
    \ is that, checking eof() explicitly is not only not \"always wrong\" -- which\
    \ seems to be an overriding opinion in this and similar SO threads --, but with\
    \ proper handling of white-space, it provides for a cleaner and more reliable\
    \ error handling, and is the always correct solution (although, not necessarily\
    \ the tersest).\nTo summarize what is being suggested as the \"proper\" termination\
    \ and read order is the following:\nint data;\nwhile(in >> data) {  /* ... */\
    \ }\n\n// which is equivalent to \nwhile( !(in >> data).fail() )  {  /* ... */\
    \ }\n\nThe failure due to read attempt beyond eof is taken as the termination\
    \ condition.  This means is that there is no easy way to distinguish between a\
    \ successful stream and one that really fails for reasons other than eof. Take\
    \ the following streams: \n\n1 2 3 4 5<eof>\n1 2 a 3 4 5<eof> \na<eof>\n\nwhile(in>>data)\
    \ terminates with a set failbit for all three input. In the first and third, eofbit\
    \ is also set. So past the loop one needs very ugly extra logic to distinguish\
    \ a proper input (1st) from improper ones (2nd and 3rd).\nWhereas, take the following:\
    \ \nwhile( !in.eof() ) \n{  \n   int data;\n   in >> data;\n   if ( in.fail()\
    \ ) /* handle with break or throw */; \n   // now use data\n}    \n\nHere, in.fail()\
    \ verifies that as long as there is something to read, it is the correct one.\
    \ It's purpose is not a mere while-loop terminator. \nSo far so good, but what\
    \ happens if there is trailing space in the stream -- what sounds like the major\
    \ concern against eof() as terminator?\nWe don't need to surrender our error handling;\
    \ just eat up the white-space:\nwhile( !in.eof() ) \n{  \n   int data;\n   in\
    \ >> data >> ws; // eat whitespace with std::ws\n   if ( in.fail() ) /* handle\
    \ with break or throw */; \n   // now use data\n}\n\nstd::ws skips any potential\
    \ (zero or more) trailing space in the stream while setting the eofbit, and not\
    \ the failbit.  So, in.fail() works as expected, as long as there is at least\
    \ one data to read. If all-blank streams are also acceptable, then the correct\
    \ form is:\nwhile( !(in>>ws).eof() ) \n{  \n   int data;\n   in >> data; \n  \
    \ if ( in.fail() ) /* handle with break or throw */; \n   /* this will never fire\
    \ if the eof is reached cleanly */\n   // now use data\n}\n\nSummary: A properly\
    \ constructed while(!eof) is not only possible and not wrong, but allows data\
    \ to be localized within scope, and provides a cleaner separation of error checking\
    \ from business as usual.  That being said, while(!fail) is inarguably a more\
    \ common and terse idiom, and may be preferred in simple (single data per read\
    \ type of) scenarios. \n"
- - Why is iostream::eof inside a loop condition considered wrong?
  - "\nBecause if programmers don't write while(stream >> n), they possibly write\
    \ this:\nwhile(!stream.eof())\n{\n    stream >> n;\n    //some work on n;\n}\n\
    \nHere the problem is, you cannot do some work on n without first checking if\
    \ the stream read was successful, because if it was unsuccessful, your some work\
    \ on n would produce undesired result.\nThe whole point is that, eofbit, badbit,\
    \ or failbit are set after an attempt is made to read from the stream. So if stream\
    \ >> n fails, then eofbit, badbit, or failbit is set immediately, so its more\
    \ idiomatic if you write while (stream >> n), because the returned object stream\
    \ converts to false if there was some failure in reading from the stream and consequently\
    \ the loop stops. And it converts to true if the read was successful and the loop\
    \ continues.\n"
- - What exactly is nullptr?
  - "\n\nHow is it a keyword and an instance of a type?\n\nThis isn't surprising.\
    \ Both true and false are keywords and as literals they have a type ( bool ).\
    \ nullptr is a pointer literal of type std::nullptr_t, and it's a prvalue (you\
    \ cannot take the address of it using &). \n\n4.10 about pointer conversion says\
    \ that a prvalue of type std::nullptr_t is a null pointer constant, and that an\
    \ integral null pointer constant can be converted to std::nullptr_t. The opposite\
    \ direction is not allowed. This allows overloading a function for both pointers\
    \ and integers, and passing nullptr to select the pointer version. Passing NULL\
    \ or 0 would confusingly select the int version. \nA cast of nullptr_t to an integral\
    \ type needs a reinterpret_cast, and has the same semantics as a cast of (void*)0\
    \ to an integral type (mapping implementation defined). A reinterpret_cast cannot\
    \ convert nullptr_t to any pointer type. Rely on the implicit conversion if possible\
    \ or use static_cast. \nThe Standard requires that sizeof(nullptr_t) be sizeof(void*).\
    \ \n\n"
- - What exactly is nullptr?
  - "\nFrom nullptr: A Type-safe and Clear-Cut Null Pointer:\n\nThe new C++09 nullptr\
    \ keyword designates an rvalue constant that serves as a universal null pointer\
    \ literal, replacing the buggy and weakly-typed literal 0 and the infamous NULL\
    \ macro. nullptr thus puts an end to more than 30 years of embarrassment, ambiguity,\
    \ and bugs. The following sections present the nullptr facility and show how it\
    \ can remedy the ailments of NULL and 0.\n\nOther references:\n\nWikiBooks, with\
    \ sample code.\nHere at Stack Overflow: Do you use NULL or 0 (zero) for pointers\
    \ in C++?\ntemplate\nGoogle group: comp.lang.c++.moderated - compiler discussion\
    \ \n\n"
- - What exactly is nullptr?
  - "\nWhen you have a function that can receive pointers to more than one type, calling\
    \ it with NULL is ambiguous. The way this is worked around now is very hacky by\
    \ accepting an int and assuming it's NULL.\ntemplate <class T>\nclass ptr {\n\
    \    T* p_;\n    public:\n        ptr(T* p) : p_(p) {}\n\n        template <class\
    \ U>\n        ptr(U* u) : p_(dynamic_cast<T*>(u)) { }\n\n        // Without this\
    \ ptr<T> p(NULL) would be ambiguous\n        ptr(int null) : p_(NULL)  { assert(null\
    \ == NULL); }\n};\n\nIn C++11 you would be able to overload on nullptr_t so that\
    \ ptr<T> p(42); would be a compile-time error rather than a run-time assert.\n\
    ptr(std::nullptr_t) : p_(nullptr)  {  }\n\n"
- - How to automatically generate a stacktrace when my program crashes
  - "\nFor Linux and I believe Mac OS X, if you're using gcc, or any compiler that\
    \ uses glibc, you can use the backtrace() functions in execinfo.h to print a stacktrace\
    \ and exit gracefully when you get a segmentation fault.  Documentation can be\
    \ found in the libc manual.\nHere's an example program that installs a SIGSEGV\
    \ handler and prints a stacktrace to stderr when it segfaults.  The baz() function\
    \ here causes the segfault that triggers the handler:\n#include <stdio.h>\n#include\
    \ <execinfo.h>\n#include <signal.h>\n#include <stdlib.h>\n#include <unistd.h>\n\
    \n\nvoid handler(int sig) {\n  void *array[10];\n  size_t size;\n\n  // get void*'s\
    \ for all entries on the stack\n  size = backtrace(array, 10);\n\n  // print out\
    \ all the frames to stderr\n  fprintf(stderr, \"Error: signal %d:\\n\", sig);\n\
    \  backtrace_symbols_fd(array, size, STDERR_FILENO);\n  exit(1);\n}\n\nvoid baz()\
    \ {\n int *foo = (int*)-1; // make a bad pointer\n  printf(\"%d\\n\", *foo); \
    \      // causes segfault\n}\n\nvoid bar() { baz(); }\nvoid foo() { bar(); }\n\
    \n\nint main(int argc, char **argv) {\n  signal(SIGSEGV, handler);   // install\
    \ our handler\n  foo(); // this will call foo, bar, and baz.  baz segfaults.\n\
    }\n\nCompiling with -g -rdynamic gets you symbol info in your output, which glibc\
    \ can use to make a nice stacktrace:\n$ gcc -g -rdynamic ./test.c -o test\n\n\
    Executing this gets you this output:\n$ ./test\nError: signal 11:\n./test(handler+0x19)[0x400911]\n\
    /lib64/tls/libc.so.6[0x3a9b92e380]\n./test(baz+0x14)[0x400962]\n./test(bar+0xe)[0x400983]\n\
    ./test(foo+0xe)[0x400993]\n./test(main+0x28)[0x4009bd]\n/lib64/tls/libc.so.6(__libc_start_main+0xdb)[0x3a9b91c4bb]\n\
    ./test[0x40086a]\n\nThis shows the load module, offset, and function that each\
    \ frame in the stack came from.  Here you can see the signal handler on top of\
    \ the stack, and the libc functions before main in addition to main, foo, bar,\
    \ and baz.\n"
- - How to automatically generate a stacktrace when my program crashes
  - "\nLinux\nWhile the use of the backtrace() functions in execinfo.h to print a\
    \ stacktrace and exit gracefully when you get a segmentation fault has already\
    \ been suggested, I see no mention of the intricacies necessary to ensure the\
    \ resulting backtrace points to the actual location of the fault (at least for\
    \ some architectures - x86 & ARM).\nThe first two entries in the stack frame chain\
    \ when you get into the signal handler contain a return address inside the signal\
    \ handler and one inside sigaction() in libc. The stack frame of the last function\
    \ called before the signal (which is the location of the fault) is lost.\nCode\n\
    #ifndef _GNU_SOURCE\n#define _GNU_SOURCE\n#endif\n#ifndef __USE_GNU\n#define __USE_GNU\n\
    #endif\n\n#include <execinfo.h>\n#include <signal.h>\n#include <stdio.h>\n#include\
    \ <stdlib.h>\n#include <string.h>\n#include <ucontext.h>\n#include <unistd.h>\n\
    \n/* This structure mirrors the one found in /usr/include/asm/ucontext.h */\n\
    typedef struct _sig_ucontext {\n unsigned long     uc_flags;\n struct ucontext\
    \   *uc_link;\n stack_t           uc_stack;\n struct sigcontext uc_mcontext;\n\
    \ sigset_t          uc_sigmask;\n} sig_ucontext_t;\n\nvoid crit_err_hdlr(int sig_num,\
    \ siginfo_t * info, void * ucontext)\n{\n void *             array[50];\n void\
    \ *             caller_address;\n char **            messages;\n int         \
    \       size, i;\n sig_ucontext_t *   uc;\n\n uc = (sig_ucontext_t *)ucontext;\n\
    \n /* Get the address at the time the signal was raised */\n#if defined(__i386__)\
    \ // gcc specific\n caller_address = (void *) uc->uc_mcontext.eip; // EIP: x86\
    \ specific\n#elif defined(__x86_64__) // gcc specific\n caller_address = (void\
    \ *) uc->uc_mcontext.rip; // RIP: x86_64 specific\n#else\n#error Unsupported architecture.\
    \ // TODO: Add support for other arch.\n#endif\n\n fprintf(stderr, \"signal %d\
    \ (%s), address is %p from %p\\n\", \n  sig_num, strsignal(sig_num), info->si_addr,\
    \ \n  (void *)caller_address);\n\n size = backtrace(array, 50);\n\n /* overwrite\
    \ sigaction with caller's address */\n array[1] = caller_address;\n\n messages\
    \ = backtrace_symbols(array, size);\n\n /* skip first stack frame (points here)\
    \ */\n for (i = 1; i < size && messages != NULL; ++i)\n {\n  fprintf(stderr, \"\
    [bt]: (%d) %s\\n\", i, messages[i]);\n }\n\n free(messages);\n\n exit(EXIT_FAILURE);\n\
    }\n\nint crash()\n{\n char * p = NULL;\n *p = 0;\n return 0;\n}\n\nint foo4()\n\
    {\n crash();\n return 0;\n}\n\nint foo3()\n{\n foo4();\n return 0;\n}\n\nint foo2()\n\
    {\n foo3();\n return 0;\n}\n\nint foo1()\n{\n foo2();\n return 0;\n}\n\nint main(int\
    \ argc, char ** argv)\n{\n struct sigaction sigact;\n\n sigact.sa_sigaction =\
    \ crit_err_hdlr;\n sigact.sa_flags = SA_RESTART | SA_SIGINFO;\n\n if (sigaction(SIGSEGV,\
    \ &sigact, (struct sigaction *)NULL) != 0)\n {\n  fprintf(stderr, \"error setting\
    \ signal handler for %d (%s)\\n\",\n    SIGSEGV, strsignal(SIGSEGV));\n\n  exit(EXIT_FAILURE);\n\
    \ }\n\n foo1();\n\n exit(EXIT_SUCCESS);\n}\n\nOutput\nsignal 11 (Segmentation\
    \ fault), address is (nil) from 0x8c50\n[bt]: (1) ./test(crash+0x24) [0x8c50]\n\
    [bt]: (2) ./test(foo4+0x10) [0x8c70]\n[bt]: (3) ./test(foo3+0x10) [0x8c8c]\n[bt]:\
    \ (4) ./test(foo2+0x10) [0x8ca8]\n[bt]: (5) ./test(foo1+0x10) [0x8cc4]\n[bt]:\
    \ (6) ./test(main+0x74) [0x8d44]\n[bt]: (7) /lib/libc.so.6(__libc_start_main+0xa8)\
    \ [0x40032e44]\n\nAll the hazards of calling the backtrace() functions in a signal\
    \ handler still exist and should not be overlooked, but I find the functionality\
    \ I described here quite helpful in debugging crashes.\nIt is important to note\
    \ that the example I provided is developed/tested on Linux for x86.  I have also\
    \ successfully implemented this on ARM using uc_mcontext.arm_pc instead of uc_mcontext.eip.\
    \  \nHere's a link to the article where I learned the details for this implementation:\n\
    http://www.linuxjournal.com/article/6391\n"
- - How to automatically generate a stacktrace when my program crashes
  - "\nIt's even easier than \"man backtrace\", there's a little-documented library\
    \ (GNU specific) distributed with glibc as libSegFault.so, which was I believe\
    \ was written by Ulrich Drepper to support the program catchsegv (see \"man catchsegv\"\
    ).\nThis gives us 3 possibilities. Instead of running \"program -o hai\":\n\n\
    Run within catchsegv:\n$ catchsegv program -o hai\n\nLink with libSegFault at\
    \ runtime:\n$ LD_PRELOAD=/lib/libSegFault.so program -o hai\n\nLink with libSegFault\
    \ at compile time:\n$ gcc -g1 -lSegFault -o program program.cc\n$ program -o hai\n\
    \n\nIn all 3 cases, you will get clearer backtraces with less optimization (gcc\
    \ -O0 or -O1) and debugging symbols (gcc -g). Otherwise, you may just end up with\
    \ a pile of memory addresses.\nYou can also catch more signals for stack traces\
    \ with something like:\n$ export SEGFAULT_SIGNALS=\"all\"       # \"all\" signals\n\
    $ export SEGFAULT_SIGNALS=\"bus abrt\"  # SIGBUS and SIGABRT\n\nThe output will\
    \ look something like this (notice the backtrace at the bottom):\n*** Segmentation\
    \ fault Register dump:\n\n EAX: 0000000c   EBX: 00000080   ECX:\n00000000   EDX:\
    \ 0000000c  ESI:\nbfdbf080   EDI: 080497e0   EBP:\nbfdbee38   ESP: bfdbee20\n\n\
    \ EIP: 0805640f   EFLAGS: 00010282\n\n CS: 0073   DS: 007b   ES: 007b   FS:\n\
    0000   GS: 0033   SS: 007b\n\n Trap: 0000000e   Error: 00000004  \nOldMask: 00000000\
    \  ESP/signal:\nbfdbee20   CR2: 00000024\n\n FPUCW: ffff037f   FPUSW: ffff0000\
    \  \nTAG: ffffffff  IPOFF: 00000000  \nCSSEL: 0000   DATAOFF: 00000000  \nDATASEL:\
    \ 0000\n\n ST(0) 0000 0000000000000000   ST(1)\n0000 0000000000000000  ST(2) 0000\n\
    0000000000000000   ST(3) 0000\n0000000000000000  ST(4) 0000\n0000000000000000\
    \   ST(5) 0000\n0000000000000000  ST(6) 0000\n0000000000000000   ST(7) 0000\n\
    0000000000000000\n\nBacktrace:\n/lib/libSegFault.so[0xb7f9e100]\n??:0(??)[0xb7fa3400]\n\
    /usr/include/c++/4.3/bits/stl_queue.h:226(_ZNSt5queueISsSt5dequeISsSaISsEEE4pushERKSs)[0x805647a]\n\
    /home/dbingham/src/middle-earth-mud/alpha6/src/engine/player.cpp:73(_ZN6Player5inputESs)[0x805377c]\n\
    /home/dbingham/src/middle-earth-mud/alpha6/src/engine/socket.cpp:159(_ZN6Socket4ReadEv)[0x8050698]\n\
    /home/dbingham/src/middle-earth-mud/alpha6/src/engine/socket.cpp:413(_ZN12ServerSocket4ReadEv)[0x80507ad]\n\
    /home/dbingham/src/middle-earth-mud/alpha6/src/engine/socket.cpp:300(_ZN12ServerSocket4pollEv)[0x8050b44]\n\
    /home/dbingham/src/middle-earth-mud/alpha6/src/engine/main.cpp:34(main)[0x8049a72]\n\
    /lib/tls/i686/cmov/libc.so.6(__libc_start_main+0xe5)[0xb7d1b775]\n/build/buildd/glibc-2.9/csu/../sysdeps/i386/elf/start.S:122(_start)[0x8049801]\n\
    \nIf you want to know the gory details, the best source is unfortunately the source:\
    \ See http://sourceware.org/git/?p=glibc.git;a=blob;f=debug/segfault.c and its\
    \ parent directory http://sourceware.org/git/?p=glibc.git;a=tree;f=debug \n"
- - Read file line by line using ifstream in C++
  - "\nFirst, make an ifstream:\n#include <fstream>\nstd::ifstream infile(\"thefile.txt\"\
    );\n\nThe two standard methods are:\n\nAssume that every line consists of two\
    \ numbers and read token by token:\nint a, b;\nwhile (infile >> a >> b)\n{\n \
    \   // process pair (a,b)\n}\n\nLine-based parsing, using string streams:\n#include\
    \ <sstream>\n#include <string>\n\nstd::string line;\nwhile (std::getline(infile,\
    \ line))\n{\n    std::istringstream iss(line);\n    int a, b;\n    if (!(iss >>\
    \ a >> b)) { break; } // error\n\n    // process pair (a,b)\n}\n\n\nYou shouldn't\
    \ mix (1) and (2), since the token-based parsing doesn't gobble up newlines, so\
    \ you may end up with spurious empty lines if you use getline() after token-based\
    \ extraction got you to the end of a line already.\n"
- - Read file line by line using ifstream in C++
  - "\nUse ifstream to read data from a file:\nstd::ifstream input( \"filename.ext\"\
    \ );\n\nIf you really need to read line by line, then do this:\nfor( std::string\
    \ line; getline( input, line ); )\n{\n    ...for each line in input...\n}\n\n\
    But you probably just need to extract coordinate pairs:\nint x, y;\ninput >> x\
    \ >> y;\n\nUpdate:\nIn your code you use ofstream myfile;, however the o in ofstream\
    \ stands for output. If you want to read from the file (input) use ifstream. If\
    \ you want to both read and write use fstream.\n"
- - Read file line by line using ifstream in C++
  - "\nReading a file line by line in C++ can be done in some different ways.\n[Fast]\
    \ Loop with std::getline()\nThe simplest approach is to open an std::ifstream\
    \ and loop using std::getline() calls. The code is clean and easy to understand.\n\
    #include <fstream>\n\nstd::ifstream file(FILENAME);\nif (file.is_open()) {\n \
    \   std::string line;\n    while (getline(file, line)) {\n        // using printf()\
    \ in all tests for consistency\n        printf(\"%s\", line.c_str());\n    }\n\
    \    file.close();\n}\n\n[Fast] Use Boost's file_description_source\nAnother possibility\
    \ is to use the Boost library, but the code gets a bit more verbose. The performance\
    \ is quite similar to the code above (Loop with std::getline()).\n#include <boost/iostreams/device/file_descriptor.hpp>\n\
    #include <boost/iostreams/stream.hpp>\n#include <fcntl.h>\n\nnamespace io = boost::iostreams;\n\
    \nvoid readLineByLineBoost() {\n    int fdr = open(FILENAME, O_RDONLY);\n    if\
    \ (fdr >= 0) {\n        io::file_descriptor_source fdDevice(fdr, io::file_descriptor_flags::close_handle);\n\
    \        io::stream <io::file_descriptor_source> in(fdDevice);\n        if (fdDevice.is_open())\
    \ {\n            std::string line;\n            while (std::getline(in, line))\
    \ {\n                // using printf() in all tests for consistency\n        \
    \        printf(\"%s\", line.c_str());\n            }\n            fdDevice.close();\n\
    \        }\n    }\n}\n\n[Fastest] Use C code\nIf performance is critical for your\
    \ software, you may consider using the C language. This code can be 4-5 times\
    \ faster than the C++ versions above, see benchmark below\nFILE* fp = fopen(FILENAME,\
    \ \"r\");\nif (fp == NULL)\n    exit(EXIT_FAILURE);\n\nchar* line = NULL;\nsize_t\
    \ len = 0;\nwhile ((getline(&line, &len, fp)) != -1) {\n    // using printf()\
    \ in all tests for consistency\n    printf(\"%s\", line);\n}\nfclose(fp);\nif\
    \ (line)\n    free(line);\n\nBenchmark -- Which one is faster?\nI have done some\
    \ performance benchmarks with the code above and the results are interesting.\
    \ I have tested the code with ASCII files that contain 100,000 lines, 1,000,000\
    \ lines and 10,000,000 lines of text. Each line of text contains 10 words in average.\
    \ The program is compiled with -O3 optimization and its output is forwarded to\
    \ /dev/null in order to remove the logging time variable from the measurement.\
    \ Last, but not least, each piece of code logs each line with the printf() function\
    \ for consistency.\nThe results show the time (in ms) that each piece of code\
    \ took to read the files.\nThe performance difference between the two C++ approaches\
    \ is minimal and shouldn't make any difference in practice. The performance of\
    \ the C code is what makes the benchmark impressive and can be a game changer\
    \ in terms of speed.\n                             10K lines     100K lines  \
    \   1000K lines\nLoop with std::getline()         105ms          894ms       \
    \   9773ms\nBoost code                       106ms          968ms          9561ms\n\
    C code                            23ms          243ms          2397ms\n\n\n"
- - How to determine CPU and memory consumption from inside a process?
  - "\nWindows\nSome of the above values are easily available from the appropriate\
    \ WIN32 API, I just list them here for completeness. Others, however, need to\
    \ be obtained from the Performance Data Helper library (PDH), which is a bit \"\
    unintuitive\" and takes a lot of painful trial and error to get to work. (At least\
    \ it took me quite a while, perhaps I've been only a bit stupid...)\nNote: for\
    \ clarity all error checking has been omitted from the following code. Do check\
    \ the return codes...!\n\n\nTotal Virtual Memory:\n#include \"windows.h\"\n\n\
    MEMORYSTATUSEX memInfo;\nmemInfo.dwLength = sizeof(MEMORYSTATUSEX);\nGlobalMemoryStatusEx(&memInfo);\n\
    DWORDLONG totalVirtualMem = memInfo.ullTotalPageFile;\n\nNote: The name \"TotalPageFile\"\
    \ is a bit misleading here. In reality this parameter gives the \"Virtual Memory\
    \ Size\", which is size of swap file plus installed RAM.\nVirtual Memory currently\
    \ used:\nSame code as in \"Total Virtual Memory\" and then\nDWORDLONG virtualMemUsed\
    \ = memInfo.ullTotalPageFile - memInfo.ullAvailPageFile;\n\nVirtual Memory currently\
    \ used by current process:\n#include \"windows.h\"\n#include \"psapi.h\"\n\nPROCESS_MEMORY_COUNTERS_EX\
    \ pmc;\nGetProcessMemoryInfo(GetCurrentProcess(), &pmc, sizeof(pmc));\nSIZE_T\
    \ virtualMemUsedByMe = pmc.PrivateUsage;\n\n\n\n\nTotal Physical Memory (RAM):\n\
    Same code as in \"Total Virtual Memory\" and then\nDWORDLONG totalPhysMem = memInfo.ullTotalPhys;\n\
    \nPhysical Memory currently used:\nSame code as in \"Total Virtual Memory\" and\
    \ then\n\nDWORDLONG physMemUsed = memInfo.ullTotalPhys - memInfo.ullAvailPhys;\n\
    \nPhysical Memory currently used by current process:\nSame code as in \"Virtual\
    \ Memory currently used by current process\" and then\nSIZE_T physMemUsedByMe\
    \ = pmc.WorkingSetSize;\n\n\n\n\nCPU currently used:\n#include \"TCHAR.h\"\n#include\
    \ \"pdh.h\"\n\nstatic PDH_HQUERY cpuQuery;\nstatic PDH_HCOUNTER cpuTotal;\n\n\
    void init(){\n    PdhOpenQuery(NULL, NULL, &cpuQuery);\n    // You can also use\
    \ L\"\\\\Processor(*)\\\\% Processor Time\" and get individual CPU values with\
    \ PdhGetFormattedCounterArray()\n    PdhAddEnglishCounter(cpuQuery, L\"\\\\Processor(_Total)\\\
    \\% Processor Time\", NULL, &cpuTotal);\n    PdhCollectQueryData(cpuQuery);\n\
    }\n\ndouble getCurrentValue(){\n    PDH_FMT_COUNTERVALUE counterVal;\n\n    PdhCollectQueryData(cpuQuery);\n\
    \    PdhGetFormattedCounterValue(cpuTotal, PDH_FMT_DOUBLE, NULL, &counterVal);\n\
    \    return counterVal.doubleValue;\n}\n\nCPU currently used by current process:\n\
    #include \"windows.h\"\n\nstatic ULARGE_INTEGER lastCPU, lastSysCPU, lastUserCPU;\n\
    static int numProcessors;\nstatic HANDLE self;\n\nvoid init(){\n    SYSTEM_INFO\
    \ sysInfo;\n    FILETIME ftime, fsys, fuser;\n\n    GetSystemInfo(&sysInfo);\n\
    \    numProcessors = sysInfo.dwNumberOfProcessors;\n\n    GetSystemTimeAsFileTime(&ftime);\n\
    \    memcpy(&lastCPU, &ftime, sizeof(FILETIME));\n\n    self = GetCurrentProcess();\n\
    \    GetProcessTimes(self, &ftime, &ftime, &fsys, &fuser);\n    memcpy(&lastSysCPU,\
    \ &fsys, sizeof(FILETIME));\n    memcpy(&lastUserCPU, &fuser, sizeof(FILETIME));\n\
    }\n\ndouble getCurrentValue(){\n    FILETIME ftime, fsys, fuser;\n    ULARGE_INTEGER\
    \ now, sys, user;\n    double percent;\n\n    GetSystemTimeAsFileTime(&ftime);\n\
    \    memcpy(&now, &ftime, sizeof(FILETIME));\n\n    GetProcessTimes(self, &ftime,\
    \ &ftime, &fsys, &fuser);\n    memcpy(&sys, &fsys, sizeof(FILETIME));\n    memcpy(&user,\
    \ &fuser, sizeof(FILETIME));\n    percent = (sys.QuadPart - lastSysCPU.QuadPart)\
    \ +\n        (user.QuadPart - lastUserCPU.QuadPart);\n    percent /= (now.QuadPart\
    \ - lastCPU.QuadPart);\n    percent /= numProcessors;\n    lastCPU = now;\n  \
    \  lastUserCPU = user;\n    lastSysCPU = sys;\n\n    return percent * 100;\n}\n\
    \n\n\nLinux\nOn Linux the choice that seemed obvious at first was to use the POSIX\
    \ APIs like getrusage() etc. I spent some time trying to get this to work, but\
    \ never got meaningful values. When I finally checked the kernel sources themselves,\
    \ I found out that apparently these APIs are not yet completely implemented as\
    \ of Linux kernel 2.6!?\nIn the end I got all values via a combination of reading\
    \ the pseudo-filesystem /proc and kernel calls.\n\nTotal Virtual Memory:\n#include\
    \ \"sys/types.h\"\n#include \"sys/sysinfo.h\"\n\nstruct sysinfo memInfo;\n\nsysinfo\
    \ (&memInfo);\nlong long totalVirtualMem = memInfo.totalram;\n//Add other values\
    \ in next statement to avoid int overflow on right hand side...\ntotalVirtualMem\
    \ += memInfo.totalswap;\ntotalVirtualMem *= memInfo.mem_unit;\n\nVirtual Memory\
    \ currently used:\nSame code as in \"Total Virtual Memory\" and then\nlong long\
    \ virtualMemUsed = memInfo.totalram - memInfo.freeram;\n//Add other values in\
    \ next statement to avoid int overflow on right hand side...\nvirtualMemUsed +=\
    \ memInfo.totalswap - memInfo.freeswap;\nvirtualMemUsed *= memInfo.mem_unit;\n\
    \nVirtual Memory currently used by current process:\n#include \"stdlib.h\"\n#include\
    \ \"stdio.h\"\n#include \"string.h\"\n\nint parseLine(char* line){\n    // This\
    \ assumes that a digit will be found and the line ends in \" Kb\".\n    int i\
    \ = strlen(line);\n    const char* p = line;\n    while (*p <'0' || *p > '9')\
    \ p++;\n    line[i-3] = '\\0';\n    i = atoi(p);\n    return i;\n}\n\nint getValue(){\
    \ //Note: this value is in KB!\n    FILE* file = fopen(\"/proc/self/status\",\
    \ \"r\");\n    int result = -1;\n    char line[128];\n\n    while (fgets(line,\
    \ 128, file) != NULL){\n        if (strncmp(line, \"VmSize:\", 7) == 0){\n   \
    \         result = parseLine(line);\n            break;\n        }\n    }\n  \
    \  fclose(file);\n    return result;\n}\n\n\n\n\nTotal Physical Memory (RAM):\n\
    Same code as in \"Total Virtual Memory\" and then\nlong long totalPhysMem = memInfo.totalram;\n\
    //Multiply in next statement to avoid int overflow on right hand side...\ntotalPhysMem\
    \ *= memInfo.mem_unit;\n\nPhysical Memory currently used:\nSame code as in \"\
    Total Virtual Memory\" and then\nlong long physMemUsed = memInfo.totalram - memInfo.freeram;\n\
    //Multiply in next statement to avoid int overflow on right hand side...\nphysMemUsed\
    \ *= memInfo.mem_unit;\n\nPhysical Memory currently used by current process:\n\
    Change getValue() in \"Virtual Memory currently used by current process\" as follows:\n\
    int getValue(){ //Note: this value is in KB!\n    FILE* file = fopen(\"/proc/self/status\"\
    , \"r\");\n    int result = -1;\n    char line[128];\n\n    while (fgets(line,\
    \ 128, file) != NULL){\n        if (strncmp(line, \"VmRSS:\", 6) == 0){\n    \
    \        result = parseLine(line);\n            break;\n        }\n    }\n   \
    \ fclose(file);\n    return result;\n}\n\n\n\n\n\nCPU currently used:\n#include\
    \ \"stdlib.h\"\n#include \"stdio.h\"\n#include \"string.h\"\n\nstatic unsigned\
    \ long long lastTotalUser, lastTotalUserLow, lastTotalSys, lastTotalIdle;\n\n\
    void init(){\n    FILE* file = fopen(\"/proc/stat\", \"r\");\n    fscanf(file,\
    \ \"cpu %llu %llu %llu %llu\", &lastTotalUser, &lastTotalUserLow,\n        &lastTotalSys,\
    \ &lastTotalIdle);\n    fclose(file);\n}\n\ndouble getCurrentValue(){\n    double\
    \ percent;\n    FILE* file;\n    unsigned long long totalUser, totalUserLow, totalSys,\
    \ totalIdle, total;\n\n    file = fopen(\"/proc/stat\", \"r\");\n    fscanf(file,\
    \ \"cpu %llu %llu %llu %llu\", &totalUser, &totalUserLow,\n        &totalSys,\
    \ &totalIdle);\n    fclose(file);\n\n    if (totalUser < lastTotalUser || totalUserLow\
    \ < lastTotalUserLow ||\n        totalSys < lastTotalSys || totalIdle < lastTotalIdle){\n\
    \        //Overflow detection. Just skip this value.\n        percent = -1.0;\n\
    \    }\n    else{\n        total = (totalUser - lastTotalUser) + (totalUserLow\
    \ - lastTotalUserLow) +\n            (totalSys - lastTotalSys);\n        percent\
    \ = total;\n        total += (totalIdle - lastTotalIdle);\n        percent /=\
    \ total;\n        percent *= 100;\n    }\n\n    lastTotalUser = totalUser;\n \
    \   lastTotalUserLow = totalUserLow;\n    lastTotalSys = totalSys;\n    lastTotalIdle\
    \ = totalIdle;\n\n    return percent;\n}\n\nCPU currently used by current process:\n\
    #include \"stdlib.h\"\n#include \"stdio.h\"\n#include \"string.h\"\n#include \"\
    sys/times.h\"\n#include \"sys/vtimes.h\"\n\nstatic clock_t lastCPU, lastSysCPU,\
    \ lastUserCPU;\nstatic int numProcessors;\n\nvoid init(){\n    FILE* file;\n \
    \   struct tms timeSample;\n    char line[128];\n\n    lastCPU = times(&timeSample);\n\
    \    lastSysCPU = timeSample.tms_stime;\n    lastUserCPU = timeSample.tms_utime;\n\
    \n    file = fopen(\"/proc/cpuinfo\", \"r\");\n    numProcessors = 0;\n    while(fgets(line,\
    \ 128, file) != NULL){\n        if (strncmp(line, \"processor\", 9) == 0) numProcessors++;\n\
    \    }\n    fclose(file);\n}\n\ndouble getCurrentValue(){\n    struct tms timeSample;\n\
    \    clock_t now;\n    double percent;\n\n    now = times(&timeSample);\n    if\
    \ (now <= lastCPU || timeSample.tms_stime < lastSysCPU ||\n        timeSample.tms_utime\
    \ < lastUserCPU){\n        //Overflow detection. Just skip this value.\n     \
    \   percent = -1.0;\n    }\n    else{\n        percent = (timeSample.tms_stime\
    \ - lastSysCPU) +\n            (timeSample.tms_utime - lastUserCPU);\n       \
    \ percent /= (now - lastCPU);\n        percent /= numProcessors;\n        percent\
    \ *= 100;\n    }\n    lastCPU = now;\n    lastSysCPU = timeSample.tms_stime;\n\
    \    lastUserCPU = timeSample.tms_utime;\n\n    return percent;\n}\n\n\n\nTODO:\
    \ Other Platforms\nI would assume, that some of the Linux code also works for\
    \ the Unixes, except for the parts that read the /proc pseudo-filesystem. Perhaps\
    \ on Unix these parts can be replaced by getrusage() and similar functions?\n\
    If someone with Unix know-how could edit this answer and fill in the details?!\n"
- - How to determine CPU and memory consumption from inside a process?
  - "\nMac OS X\nI was hoping to find similar information for Mac OS X as well. Since\
    \ it wasn't here, I went out and dug it up myself. Here are some of the things\
    \ I found. If anyone has any other suggestions, I'd love to hear them.\nTotal\
    \ Virtual Memory\nThis one is tricky on Mac OS X because it doesn't use a preset\
    \ swap partition or file like Linux. Here's an entry from Apple's documentation:\n\
    \nNote: Unlike most Unix-based operating systems, Mac OS X does not use a preallocated\
    \ swap partition for virtual memory. Instead, it uses all of the available space\
    \ on the machineâs boot partition.\n\nSo, if you want to know how much virtual\
    \ memory is still available, you need to get the size of the root partition. You\
    \ can do that like this:\nstruct statfs stats;\nif (0 == statfs(\"/\", &stats))\n\
    {\n    myFreeSwap = (uint64_t)stats.f_bsize * stats.f_bfree;\n}\n\nTotal Virtual\
    \ Currently Used\nCalling systcl with the \"vm.swapusage\" key provides interesting\
    \ information about swap usage:\nsysctl -n vm.swapusage\nvm.swapusage: total =\
    \ 3072.00M  used = 2511.78M  free = 560.22M  (encrypted)\n\nNot that the total\
    \ swap usage displayed here can change if more swap is needed as explained in\
    \ the section above. So the total is actually the current swap total. In C++,\
    \ this data can be queried this way:\nxsw_usage vmusage = {0};\nsize_t size =\
    \ sizeof(vmusage);\nif( sysctlbyname(\"vm.swapusage\", &vmusage, &size, NULL,\
    \ 0)!=0 )\n{\n   perror( \"unable to get swap usage by calling sysctlbyname(\\\
    \"vm.swapusage\\\",...)\" );\n}\n\nNote that the \"xsw_usage\", declared in sysctl.h,\
    \ seems not documented and I suspect there there is a more portable way of accessing\
    \ these values.\nVirtual Memory Currently Used by my Process\nYou can get statistics\
    \ about your current process using the task_info function. That includes the current\
    \ resident size of your process and the current virtual size.\n#include<mach/mach.h>\n\
    \nstruct task_basic_info t_info;\nmach_msg_type_number_t t_info_count = TASK_BASIC_INFO_COUNT;\n\
    \nif (KERN_SUCCESS != task_info(mach_task_self(),\n                          \
    \    TASK_BASIC_INFO, (task_info_t)&t_info, \n                              &t_info_count))\n\
    {\n    return -1;\n}\n// resident size is in t_info.resident_size;\n// virtual\
    \ size is in t_info.virtual_size;\n\nTotal RAM available\nThe amount of physical\
    \ RAM available in your system is available using the sysctl system function like\
    \ this:\n#include <sys/types.h>\n#include <sys/sysctl.h>\n...\nint mib[2];\nint64_t\
    \ physical_memory;\nmib[0] = CTL_HW;\nmib[1] = HW_MEMSIZE;\nlength = sizeof(int64_t);\n\
    sysctl(mib, 2, &physical_memory, &length, NULL, 0);\n\nRAM Currently Used\nYou\
    \ can get general memory statistics from the host_statistics system function.\n\
    #include <mach/vm_statistics.h>\n#include <mach/mach_types.h>\n#include <mach/mach_init.h>\n\
    #include <mach/mach_host.h>\n\nint main(int argc, const char * argv[]) {\n   \
    \ vm_size_t page_size;\n    mach_port_t mach_port;\n    mach_msg_type_number_t\
    \ count;\n    vm_statistics64_data_t vm_stats;\n\n    mach_port = mach_host_self();\n\
    \    count = sizeof(vm_stats) / sizeof(natural_t);\n    if (KERN_SUCCESS == host_page_size(mach_port,\
    \ &page_size) &&\n        KERN_SUCCESS == host_statistics64(mach_port, HOST_VM_INFO,\n\
    \                                        (host_info64_t)&vm_stats, &count))\n\
    \    {\n        long long free_memory = (int64_t)vm_stats.free_count * (int64_t)page_size;\n\
    \n        long long used_memory = ((int64_t)vm_stats.active_count +\n        \
    \                         (int64_t)vm_stats.inactive_count +\n               \
    \                  (int64_t)vm_stats.wire_count) *  (int64_t)page_size;\n    \
    \    printf(\"free memory: %lld\\nused memory: %lld\\n\", free_memory, used_memory);\n\
    \    }\n\n    return 0;\n}\n\nOne thing to note here are that there are five types\
    \ of memory pages in Mac OS X. They are as follows:\n\nWired pages that are locked\
    \ in place and cannot be swapped out\nActive pages that are loading into physical\
    \ memory and would be relatively difficult to swap out\nInactive pages that are\
    \ loaded into memory, but haven't been used recently and may not even be needed\
    \ at all. These are potential candidates for swapping. This memory would probably\
    \ need to be flushed.\nCached pages that have been some how cached that are likely\
    \ to be easily reused. Cached memory probably would not require flushing. It is\
    \ still possible for cached pages to be reactivated\nFree pages that are completely\
    \ free and ready to be used.\n\nIt is good to note that just because Mac OS X\
    \ may show very little actual free memory at times that it may not be a good indication\
    \ of how much is ready to be used on short notice.\nRAM Currently Used by my Process\n\
    See the \"Virtual Memory Currently Used by my Process\" above. The same code applies.\n"
- - How to determine CPU and memory consumption from inside a process?
  - "\nLinux\nIn Linux, this information is available in the /proc file system.  I'm\
    \ not a big fan of the text file format used, as each Linux distribution seems\
    \ to customize at least one important file.  A quick look as the source to 'ps'\
    \ reveals the mess.\nBut here is where to find the information you seek:\n/proc/meminfo\
    \ contains the majority of the system-wide information you seek.  Here it looks\
    \ like on my system; I think you are interested in MemTotal, MemFree, SwapTotal,\
    \ and SwapFree:\nAnderson cxc # more /proc/meminfo\nMemTotal:      4083948 kB\n\
    MemFree:       2198520 kB\nBuffers:         82080 kB\nCached:        1141460 kB\n\
    SwapCached:          0 kB\nActive:        1137960 kB\nInactive:       608588 kB\n\
    HighTotal:     3276672 kB\nHighFree:      1607744 kB\nLowTotal:       807276 kB\n\
    LowFree:        590776 kB\nSwapTotal:     2096440 kB\nSwapFree:      2096440 kB\n\
    Dirty:              32 kB\nWriteback:           0 kB\nAnonPages:      523252 kB\n\
    Mapped:          93560 kB\nSlab:            52880 kB\nSReclaimable:    24652 kB\n\
    SUnreclaim:      28228 kB\nPageTables:       2284 kB\nNFS_Unstable:        0 kB\n\
    Bounce:              0 kB\nCommitLimit:   4138412 kB\nCommitted_AS:  1845072 kB\n\
    VmallocTotal:   118776 kB\nVmallocUsed:      3964 kB\nVmallocChunk:   112860 kB\n\
    HugePages_Total:     0\nHugePages_Free:      0\nHugePages_Rsvd:      0\nHugepagesize:\
    \     2048 kB\n\nFor CPU utilization, you have to do a little work.  Linux makes\
    \ available overall CPU utilization since system start; this probably isn't what\
    \ you are interested in.  If you want to know what the CPU utilization was for\
    \ the last second, or 10 seconds, then you need to query the information and calculate\
    \ it yourself.\nThe information is available in /proc/stat, which is documented\
    \ pretty well at http://www.linuxhowtos.org/System/procstat.htm; here is what\
    \ it looks like on my 4-core box:\nAnderson cxc #  more /proc/stat\ncpu  2329889\
    \ 0 2364567 1063530460 9034 9463 96111 0\ncpu0 572526 0 636532 265864398 2928\
    \ 1621 6899 0\ncpu1 590441 0 531079 265949732 4763 351 8522 0\ncpu2 562983 0 645163\
    \ 265796890 682 7490 71650 0\ncpu3 603938 0 551790 265919440 660 0 9040 0\nintr\
    \ 37124247\nctxt 50795173133\nbtime 1218807985\nprocesses 116889\nprocs_running\
    \ 1\nprocs_blocked 0\n\nFirst, you need to determine how many CPUs (or processors,\
    \ or processing cores) are available in the system.  To do this, count the number\
    \ of 'cpuN' entries, where N starts at 0 and increments.  Don't count the 'cpu'\
    \ line, which is a combination of the cpuN lines.  In my example, you can see\
    \ cpu0 through cpu3, for a total of 4 processors.  From now on, you can ignore\
    \ cpu0..cpu3, and focus only on the 'cpu' line.\nNext, you need to know that the\
    \ fourth number in these lines is a measure of idle time, and thus the fourth\
    \ number on the 'cpu' line is the total idle time for all processors since boot\
    \ time.  This time is measured in Linux \"jiffies\", which are 1/100 of a second\
    \ each. \nBut you don't care about the total idle time; you care about the idle\
    \ time in a given period, e.g., the last second.  Do calculate that, you need\
    \ to read this file twice, 1 second apart.Then you can do a diff of the fourth\
    \ value of the line.  For example, if you take a sample and get:\ncpu  2330047\
    \ 0 2365006 1063853632 9035 9463 96114 0\n\nThen one second later you get this\
    \ sample:\ncpu  2330047 0 2365007 1063854028 9035 9463 96114 0\n\nSubtract the\
    \ two numbers, and you get a diff of 396, which means that your CPU had been idle\
    \ for 3.96 seconds out of the last 1.00 second.  The trick, of course, is that\
    \ you need to divide by the number of processors.  3.96 / 4 = 0.99, and there\
    \ is your idle percentage; 99% idle, and 1% busy.\nIn my code, I have a ring buffer\
    \ of 360 entries, and I read this file every second.  That lets me quickly calculate\
    \ the CPU utilization for 1 second, 10 seconds, etc., all the way up to 1 hour.\n\
    For the process-specific information, you have to look in /proc/pid; if you don't\
    \ care abut your pid, you can look in /proc/self.\nCPU used by your process is\
    \ available in /proc/self/stat.  This is an odd-looking file consisting of a single\
    \ line; for example:\n19340 (whatever) S 19115 19115 3084 34816 19115 4202752\
    \ 118200 607 0 0 770 384 2\n 7 20 0 77 0 266764385 692477952 105074 4294967295\
    \ 134512640 146462952 321468364\n8 3214683328 4294960144 0 2147221247 268439552\
    \ 1276 4294967295 0 0 17 0 0 0 0\n\nThe important data here are the 13th and 14th\
    \ tokens (0 and 770 here).  The 13th token is the number of jiffies that the process\
    \ has executed in user mode, and the 14th is the number of jiffies that the process\
    \ has executed in kernel mode.  Add the two together, and you have its total CPU\
    \ utilization.\nAgain, you will have to sample this file periodically, and calculate\
    \ the diff, in order to determine the process's CPU usage over time.  \nEdit:\
    \  remember that when you calculate your process's CPU utilization, you have to\
    \ take into account 1) the number of threads in your process, and 2) the number\
    \ of processors in the system.  For example, if your single-threaded process is\
    \ using only 25% of the CPU, that could be good or bad.  Good on a single-processor\
    \ system, but bad on a 4-processor system; this means that your process is running\
    \ constantly, and using 100% of the CPU cycles available to it.\nFor the process-specific\
    \ memory information, you ahve to look at /proc/self/status, which looks like\
    \ this:\nName:   whatever\nState:  S (sleeping)\nTgid:   19340\nPid:    19340\n\
    PPid:   19115\nTracerPid:      0\nUid:    0       0       0       0\nGid:    0\
    \       0       0       0\nFDSize: 256\nGroups: 0 1 2 3 4 6 10 11 20 26 27\nVmPeak:\
    \   676252 kB\nVmSize:   651352 kB\nVmLck:         0 kB\nVmHWM:    420300 kB\n\
    VmRSS:    420296 kB\nVmData:   581028 kB\nVmStk:       112 kB\nVmExe:     11672\
    \ kB\nVmLib:     76608 kB\nVmPTE:      1244 kB\nThreads:        77\nSigQ:   0/36864\n\
    SigPnd: 0000000000000000\nShdPnd: 0000000000000000\nSigBlk: fffffffe7ffbfeff\n\
    SigIgn: 0000000010001000\nSigCgt: 20000001800004fc\nCapInh: 0000000000000000\n\
    CapPrm: 00000000ffffffff\nCapEff: 00000000fffffeff\nCpus_allowed:   0f\nMems_allowed:\
    \   1\nvoluntary_ctxt_switches:        6518\nnonvoluntary_ctxt_switches:     6598\n\
    \nThe entries that start with 'Vm' are the interesting ones:\n\nVmPeak is the\
    \ maximum virtual memory space used by the process, in kB (1024 bytes).\nVmSize\
    \ is the current virtual memory space used by the process, in kB.  In my example,\
    \ it's pretty large:  651,352 kB, or about 636 megabytes.\nVmRss is the amount\
    \ of memory that have been mapped into the process' address space, or its resident\
    \ set size.  This is substantially smaller (420,296 kB, or about 410 megabytes).\
    \  The difference:  my program has mapped 636 MB via mmap(), but has only accessed\
    \ 410 MB of it, and thus only 410 MB of pages have been assigned to it.\n\nThe\
    \ only item I'm not sure about is Swapspace currently used by my process.  I don't\
    \ know if this is available.\n"
- - Sleep for milliseconds
  - "\nNote that there is no standard C API for milliseconds, so (on Unix) you will\
    \ have to settle for usleep, which accepts microseconds:\n#include <unistd.h>\n\
    \nunsigned int microseconds;\n...\nusleep(microseconds);\n\n"
- - Sleep for milliseconds
  - "\nIn C++11, you can do this with standard library facilities:\n#include <chrono>\n\
    #include <thread>\n\nstd::this_thread::sleep_for(std::chrono::milliseconds(x));\n\
    \nClear and readable, no more need to guess at what units the sleep() function\
    \ takes.\n"
- - Sleep for milliseconds
  - "\nTo stay portable you could use Boost::Thread for sleeping:\n#include <boost/thread/thread.hpp>\n\
    \nint main()\n{\n    //waits 2 seconds\n    boost::this_thread::sleep( boost::posix_time::seconds(1)\
    \ );\n    boost::this_thread::sleep( boost::posix_time::milliseconds(1000) );\n\
    \n    return 0;\n}\n\nThis answer is a duplicate and has been posted in this question\
    \ before. Perhaps you could find some usable answers there too.\n"
- - How to call a parent class function from derived class function?
  - "\nI'll take the risk of stating the obvious: You call the function, if it's defined\
    \ in the base class it's automatically available in the derived class (unless\
    \ it's private).\nIf there is a function with the same signature in the derived\
    \ class you can disambiguate it by adding the base class's name followed by two\
    \ colons base_class::foo(...). You should note that unlike Java and C#, C++ does\
    \ not have a keyword for \"the base class\" (super or base) since C++ supports\
    \ multiple inheritance which may lead to ambiguity.\nclass left {\npublic:\n \
    \   void foo();\n};\n\nclass right {\npublic:\n    void foo();\n};\n\nclass bottom\
    \ : public left, public right {\npublic:\n    void foo()\n    {\n        //base::foo();//\
    \ ambiguous\n        left::foo();\n        right::foo();\n\n        // and when\
    \ foo() is not called for 'this':\n        bottom b;\n        b.left::foo(); \
    \ // calls b.foo() from 'left'\n        b.right::foo();  // call b.foo() from\
    \ 'right'\n    }\n};\n\nIncidentally, you can't derive directly from the same\
    \ class twice since there will be no way to refer to one of the base classes over\
    \ the other.\nclass bottom : public left, public left { // Illegal\n};\n\n"
- - How to call a parent class function from derived class function?
  - "\nGiven parent class named Parent and child class named Child, you can do something\
    \ like this:\nclass Parent {\npublic:\n    void print(int x);\n}\n\nclass Child\
    \ : public Parent {\n    void print(int x) override;\n}\n\nvoid Parent::print(int\
    \ x) {\n    // some default behavior\n}\n\nvoid Child::print(int x) {\n    //\
    \ use Parent's print method; implicitly passes 'this' to Parent::print\n    Parent::print(x);\n\
    }\n\nNote that Parent is the class's actual name and not a keyword.\n"
- - How to call a parent class function from derived class function?
  - "\nIf your base class is called Base, and your function is called FooBar() you\
    \ can call it directly using Base::FooBar()\nvoid Base::FooBar()\n{\n   printf(\"\
    in Base\\n\");\n}\n\nvoid ChildOfBase::FooBar()\n{\n  Base::FooBar();\n}\n\n"
- - Difference between static and shared libraries?
  - "\nShared libraries are .so (or in Windows .dll, or in OS X .dylib) files. All\
    \ the code relating to the library is in this file, and it is referenced by programs\
    \ using it at run-time. A program using a shared library only makes reference\
    \ to the code that it uses in the shared library.\nStatic libraries are .a (or\
    \ in Windows .lib) files. All the code relating to the library is in this file,\
    \ and it is directly linked into the program at compile time. A program using\
    \ a static library takes copies of the code that it uses from the static library\
    \ and makes it part of the program. [Windows also has .lib files which are used\
    \ to reference .dll files, but they act the same way as the first one].\nThere\
    \ are advantages and disadvantages in each method.\nShared libraries reduce the\
    \ amount of code that is duplicated in each program that makes use of the library,\
    \ keeping the binaries small. It also allows you to replace the shared object\
    \ with one that is functionally equivalent, but may have added performance benefits\
    \ without needing to recompile the program that makes use of it. Shared libraries\
    \ will, however have a small additional cost for the execution of the functions\
    \ as well as a run-time loading cost as all the symbols in the library need to\
    \ be connected to the things they use. Additionally, shared libraries can be loaded\
    \ into an application at run-time, which is the general mechanism for implementing\
    \ binary plug-in systems.\nStatic libraries increase the overall size of the binary,\
    \ but it means that you don't need to carry along a copy of the library that is\
    \ being used. As the code is connected at compile time there are not any additional\
    \ run-time loading costs. The code is simply there.\nPersonally, I prefer shared\
    \ libraries, but use static libraries when needing to ensure that the binary does\
    \ not have many external dependencies that may be difficult to meet, such as specific\
    \ versions of the C++ standard library or specific versions of the Boost C++ library.\n"
- - Difference between static and shared libraries?
  - "\nA static library is like a bookstore, and a shared library is like... a library.\
    \ With the former, you get your own copy of the book/function to take home; with\
    \ the latter you and everyone else go to the library to use the same book/function.\
    \ So anyone who wants to use the (shared) library needs to know where it is, because\
    \ you have to \"go get\" the book/function. With a static library, the book/function\
    \ is yours to own, and you keep it within your home/program, and once you have\
    \ it you don't care where or when you got it.\n"
- - Difference between static and shared libraries?
  - "\nSimplified:\n\nStatic linking: one large executable\nDynamic linking: a small\
    \ executable plus one or more library files (.dll files on Windows, .so on Linux,\
    \ or .dylib on macOS)\n\n"
- - What are Aggregates and PODs and how/why are they special?
  - "\nHow to read:\nThis article is rather long. If you want to know about both aggregates\
    \ and PODs (Plain Old Data) take time and read it. If you are interested just\
    \ in aggregates, read only the first part. If you are interested only in PODs\
    \ then you must first read the definition, implications, and examples of aggregates\
    \ and then you may jump to PODs but I would still recommend reading the first\
    \ part in its entirety. The notion of aggregates is essential for defining PODs.\
    \ If you find any errors (even minor, including grammar, stylistics, formatting,\
    \ syntax, etc.) please leave a comment, I'll edit.\nWhat are aggregates and why\
    \ they are special \nFormal definition from the C++ standard (C++03 8.5.1 Â§1):\n\
    \nAn aggregate is an array or a class (clause 9) with no user-declared \n  constructors\
    \ (12.1), no private or protected non-static data members (clause 11),\n  no base\
    \ classes (clause 10), and no virtual functions (10.3).\n\nSo, OK, let's parse\
    \ this definition. First of all, any array is an aggregate. A class can also be\
    \ an aggregate ifâ¦ wait! nothing is said about structs or unions, can't they be\
    \ aggregates? Yes, they can. In C++, the term class refers to all classes, structs,\
    \ and unions. So, a class (or struct, or union) is an aggregate if and only if\
    \ it satisfies the criteria from the above definitions. What do these criteria\
    \ imply?\n\nThis does not mean an aggregate class cannot have constructors, in\
    \ fact it can have a default constructor and/or a copy constructor as long as\
    \ they are implicitly declared by the compiler, and not explicitly by the user\n\
    No private or protected non-static data members. You can have as many private\
    \ and protected member functions (but not constructors) as well as as many private\
    \ or protected static data members and member functions as you like and not violate\
    \ the rules for aggregate classes\nAn aggregate class can have a user-declared/user-defined\
    \ copy-assignment operator and/or destructor\nAn array is an aggregate even if\
    \ it is an array of non-aggregate class type. \n\nNow let's look at some examples:\n\
    class NotAggregate1\n{\n  virtual void f() {} //remember? no virtual functions\n\
    };\n\nclass NotAggregate2\n{\n  int x; //x is private by default and non-static\
    \ \n};\n\nclass NotAggregate3\n{\npublic:\n  NotAggregate3(int) {} //oops, user-defined\
    \ constructor\n};\n\nclass Aggregate1\n{\npublic:\n  NotAggregate1 member1;  \
    \ //ok, public member\n  Aggregate1& operator=(Aggregate1 const & rhs) {/* */}\
    \ //ok, copy-assignment  \nprivate:\n  void f() {} // ok, just a private function\n\
    };\n\nYou get the idea. Now let's see how aggregates are special. They, unlike\
    \ non-aggregate classes, can be initialized with curly braces {}. This initialization\
    \ syntax is commonly known for arrays, and we just learnt that these are aggregates.\
    \ So, let's start with them.\nType array_name[n] = {a1, a2, â¦, am};\nif(m == n)\n\
    \   the ith element of the array is initialized with ai\nelse if(m < n)\n  the\
    \ first m elements of the array are initialized with a1, a2, â¦, am and the other\
    \ n - m elements are, if possible, value-initialized (see below for the explanation\
    \ of the term)\nelse if(m > n)\n   the compiler will issue an error\nelse (this\
    \ is the case when n isn't specified at all like int a[] = {1, 2, 3};)\n the size\
    \ of the array (n) is assumed to be equal to m, so int a[] = {1, 2, 3}; is equivalent\
    \ to int a[3] = {1, 2, 3};\nWhen an object of scalar type (bool, int, char, double,\
    \ pointers, etc.) is value-initialized it means it is initialized with 0 for that\
    \ type (false for bool, 0.0 for double, etc.). When an object of class type with\
    \ a user-declared default constructor is value-initialized its default constructor\
    \ is called. If the default constructor is implicitly defined then all nonstatic\
    \ members are recursively value-initialized. This definition is imprecise and\
    \ a bit incorrect but it should give you the basic idea. A reference cannot be\
    \ value-initialized. Value-initialization for a non-aggregate class can fail if,\
    \ for example, the class has no appropriate default constructor.\nExamples of\
    \ array initialization:\nclass A\n{\npublic:\n  A(int) {} //no default constructor\n\
    };\nclass B\n{\npublic:\n  B() {} //default constructor available\n};\nint main()\n\
    {\n  A a1[3] = {A(2), A(1), A(14)}; //OK n == m\n  A a2[3] = {A(2)}; //ERROR A\
    \ has no default constructor. Unable to value-initialize a2[1] and a2[2]\n  B\
    \ b1[3] = {B()}; //OK b1[1] and b1[2] are value initialized, in this case with\
    \ the default-ctor\n  int Array1[1000] = {0}; //All elements are initialized with\
    \ 0;\n  int Array2[1000] = {1}; //Attention: only the first element is 1, the\
    \ rest are 0;\n  bool Array3[1000] = {}; //the braces can be empty too. All elements\
    \ initialized with false\n  int Array4[1000]; //no initializer. This is different\
    \ from an empty {} initializer in that\n  //the elements in this case are not\
    \ value-initialized, but have indeterminate values \n  //(unless, of course, Array4\
    \ is a global array)\n  int array[2] = {1, 2, 3, 4}; //ERROR, too many initializers\n\
    }\n\nNow let's see how aggregate classes can be initialized with braces. Pretty\
    \ much the same way. Instead of the array elements we will initialize the non-static\
    \ data members in the order of their appearance in the class definition (they\
    \ are all public by definition). If there are fewer initializers than members,\
    \ the rest are value-initialized. If it is impossible to value-initialize one\
    \ of the members which were not explicitly initialized, we get a compile-time\
    \ error. If there are more initializers than necessary, we get a compile-time\
    \ error as well.\nstruct X\n{\n  int i1;\n  int i2;\n};\nstruct Y\n{\n  char c;\n\
    \  X x;\n  int i[2];\n  float f; \nprotected:\n  static double d;\nprivate:\n\
    \  void g(){}      \n}; \n\nY y = {'a', {10, 20}, {20, 30}};\n\nIn the above example\
    \ y.c is initialized with 'a', y.x.i1 with 10, y.x.i2 with 20, y.i[0] with 20,\
    \ y.i[1] with 30 and y.f is value-initialized, that is, initialized with 0.0.\
    \ The protected static member d is not initialized at all, because it is static.\n\
    Aggregate unions are different in that you may initialize only their first member\
    \ with braces. I think that if you are advanced enough in C++ to even consider\
    \ using unions (their use may be very dangerous and must be thought of carefully),\
    \ you could look up the rules for unions in the standard yourself :). \nNow that\
    \ we know what's special about aggregates, let's try to understand the restrictions\
    \ on classes; that is, why they are there. We should understand that memberwise\
    \ initialization with braces implies that the class is nothing more than the sum\
    \ of its members. If a user-defined constructor is present, it means that the\
    \ user needs to do some extra work to initialize the members therefore brace initialization\
    \ would be incorrect. If virtual functions are present, it means that the objects\
    \ of this class have (on most implementations) a pointer to the so-called vtable\
    \ of the class, which is set in the constructor, so brace-initialization would\
    \ be insufficient. You could figure out the rest of the restrictions in a similar\
    \ manner as an exercise :).\nSo enough about the aggregates. Now we can define\
    \ a stricter set of types, to wit, PODs\nWhat are PODs and why they are special\n\
    Formal definition from the C++ standard (C++03 9 Â§4):\n\nA POD-struct is an aggregate\
    \ class\n  that has no non-static data members of\n  type non-POD-struct, non-POD-union\
    \ (or\n  array of such types) or reference, and\n  has no user-defined copy assignment\n\
    \  operator and no user-defined\n  destructor. Similarly, a POD-union is\n  an\
    \ aggregate union that has no\n  non-static data members of type\n  non-POD-struct,\
    \ non-POD-union (or\n  array of such types) or reference, and\n  has no user-defined\
    \ copy assignment\n  operator and no user-defined\n  destructor. A POD class is\
    \ a class\n  that is either a POD-struct or a\n  POD-union.\n\nWow, this one's\
    \ tougher to parse, isn't it? :) Let's leave unions out (on the same grounds as\
    \ above) and rephrase in a bit clearer way:\n\nAn aggregate class is called a\
    \ POD if\n  it has no user-defined copy-assignment\n  operator and destructor\
    \ and none of\n  its nonstatic members is a non-POD\n  class, array of non-POD,\
    \ or a\n  reference.\n\nWhat does this definition imply? (Did I mention POD stands\
    \ for Plain Old Data?)\n\nAll POD classes are aggregates, or, to put it the other\
    \ way around, if a class is not an aggregate then it is sure not a POD\nClasses,\
    \ just like structs, can be PODs even though the standard term is POD-struct for\
    \ both cases\nJust like in the case of aggregates, it doesn't matter what static\
    \ members the class has\n\nExamples:\nstruct POD\n{\n  int x;\n  char y;\n  void\
    \ f() {} //no harm if there's a function\n  static std::vector<char> v; //static\
    \ members do not matter\n};\n\nstruct AggregateButNotPOD1\n{\n  int x;\n  ~AggregateButNotPOD1()\
    \ {} //user-defined destructor\n};\n\nstruct AggregateButNotPOD2\n{\n  AggregateButNotPOD1\
    \ arrOfNonPod[3]; //array of non-POD class\n};\n\nPOD-classes, POD-unions, scalar\
    \ types, and arrays of such types are collectively called POD-types.\nPODs are\
    \ special in many ways. I'll provide just some examples.\n\nPOD-classes are the\
    \ closest to C structs. Unlike them, PODs can have member functions and arbitrary\
    \ static members, but neither of these two change the memory layout of the object.\
    \ So if you want to write a more or less portable dynamic library that can be\
    \ used from C and even .NET, you should try to make all your exported functions\
    \ take and return only parameters of POD-types.\nThe lifetime of objects of non-POD\
    \ class type begins when the constructor has finished and ends when the destructor\
    \ has finished. For POD classes, the lifetime begins when storage for the object\
    \ is occupied and finishes when that storage is released or reused.  \nFor objects\
    \ of POD types it is guaranteed by the standard that when you memcpy the contents\
    \ of your object into an array of char or unsigned char, and then memcpy the contents\
    \ back into your object, the object will hold its original value. Do note that\
    \ there is no such guarantee for objects of non-POD types. Also, you can safely\
    \ copy POD objects with memcpy. The following example assumes T is a POD-type:\n\
    #define N sizeof(T)\nchar buf[N];\nT obj; // obj initialized to its original value\n\
    memcpy(buf, &obj, N); // between these two calls to memcpy,\n// obj might be modified\n\
    memcpy(&obj, buf, N); // at this point, each subobject of obj of scalar type\n\
    // holds its original value\n\ngoto statement. As you may know, it is illegal\
    \ (the compiler should issue an error) to make a jump via goto from a point where\
    \ some variable was not yet in scope to a point where it is already in scope.\
    \ This restriction applies only if the variable is of non-POD type. In the following\
    \ example f() is ill-formed whereas g() is well-formed. Note that Microsoft's\
    \ compiler is too liberal with this ruleâit just issues a warning in both cases.\n\
    int f()\n{\n  struct NonPOD {NonPOD() {}};\n  goto label;\n  NonPOD x;\nlabel:\n\
    \  return 0;\n}\n\nint g()\n{\n  struct POD {int i; char c;};\n  goto label;\n\
    \  POD x;\nlabel:\n  return 0;\n}\n\nIt is guaranteed that there will be no padding\
    \ in the beginning of a POD object. In other words, if a POD-class A's first member\
    \ is of type T, you can safely reinterpret_cast from A* to T* and get the pointer\
    \ to the first member and vice versa.\n\nThe list goes on and onâ¦\nConclusion\n\
    It is important to understand what exactly a POD is because many language features,\
    \ as you see, behave differently for them.\n"
- - What are Aggregates and PODs and how/why are they special?
  - "\nWhat changes for C++11?\nAggregates\nThe standard definition of an aggregate\
    \ has changed slightly, but it's still pretty much the same:\n\nAn aggregate is\
    \ an array or a class (Clause 9) with no user-provided constructors (12.1),\n\
    \  no brace-or-equal-initializers for non-static data members (9.2), no private\
    \ or protected\n  non-static data members (Clause 11), no base classes (Clause\
    \ 10), and no virtual functions (10.3).\n\nOk, what changed?\n\nPreviously, an\
    \ aggregate could have no user-declared constructors, but now it can't have user-provided\
    \ constructors. Is there a difference? Yes, there is, because now you can declare\
    \ constructors and default them:\nstruct Aggregate {\n    Aggregate() = default;\
    \ // asks the compiler to generate the default implementation\n};\n\nThis is still\
    \ an aggregate because a constructor (or any special member function) that is\
    \ defaulted on the first declaration is not user-provided.\nNow an aggregate cannot\
    \ have any brace-or-equal-initializers for non-static data members. What does\
    \ this mean? Well, this is just because with this new standard, we can initialize\
    \ members directly in the class like this:\nstruct NotAggregate {\n    int x =\
    \ 5; // valid in C++11\n    std::vector<int> s{1,2,3}; // also valid\n};\n\nUsing\
    \ this feature makes the class no longer an aggregate because it's basically equivalent\
    \ to providing your own default constructor.\n\nSo, what is an aggregate didn't\
    \ change much at all. It's still the same basic idea, adapted to the new features.\n\
    What about PODs?\nPODs went through a lot of changes. Lots of previous rules about\
    \ PODs were relaxed in this new standard, and the way the definition is provided\
    \ in the standard was radically changed.\nThe idea of a POD is to capture basically\
    \ two distinct properties:\n\nIt supports static initialization, and\nCompiling\
    \ a POD in C++ gives you the same memory layout as a struct compiled in C.\n\n\
    Because of this, the definition has been split into two distinct concepts: trivial\
    \ classes and standard-layout classes, because these are more useful than POD.\
    \ The standard now rarely uses the term POD, preferring the more specific trivial\
    \ and standard-layout concepts.\nThe new definition basically says that a POD\
    \ is a class that is both trivial and has standard-layout, and this property must\
    \ hold recursively for all non-static data members:\n\nA POD struct is a non-union\
    \ class that is both a trivial class and a standard-layout class,\n  and has no\
    \ non-static data members of type non-POD struct, non-POD union (or array of such\
    \ types).\n  Similarly, a POD union is a union that is both a trivial class and\
    \ a standard layout class, and has\n  no non-static data members of type non-POD\
    \ struct, non-POD union (or array of such types).\n  A POD class is a class that\
    \ is either a POD struct or a POD union.\n\nLet's go over each of these two properties\
    \ in detail separately.\nTrivial classes\nTrivial is the first property mentioned\
    \ above: trivial classes support static initialization. \nIf a class is trivially\
    \ copyable (a superset of trivial classes), it is ok to copy its representation\
    \ over the place with things like memcpy and expect the result to be the same.\n\
    The standard defines a trivial class as follows:\n\nA trivially copyable class\
    \ is a class that:\nâ has no non-trivial copy constructors (12.8),\nâ has no non-trivial\
    \ move constructors (12.8),\nâ has no non-trivial copy assignment operators (13.5.3,\
    \ 12.8),\nâ has no non-trivial move assignment operators (13.5.3, 12.8), and\n\
    â has a trivial destructor (12.4).\nA trivial class is a class that has a trivial\
    \ default constructor (12.1) and is trivially copyable.\n[ Note: In particular,\
    \ a trivially copyable or trivial class does not have virtual functions\n  or\
    \ virtual base classes.âend note ]\n\nSo, what are all those trivial and non-trivial\
    \ things?\n\nA copy/move constructor for class X is trivial if it is not user-provided\
    \ and if\nâ class X has no virtual functions (10.3) and no virtual base classes\
    \ (10.1), and\nâ the constructor selected to copy/move each direct base class\
    \ subobject is trivial, and\nâ for each non-static data member of X that is of\
    \ class type (or array thereof), the constructor\n  selected to copy/move that\
    \ member is trivial;\notherwise the copy/move constructor is non-trivial.\n\n\
    Basically this means that a copy or move constructor is trivial if it is not user-provided,\
    \ the class has nothing virtual in it, and this property holds recursively for\
    \ all the members of the class and for the base class.\nThe definition of a trivial\
    \ copy/move assignment operator is very similar, simply replacing the word \"\
    constructor\" with \"assignment operator\".\nA trivial destructor also has a similar\
    \ definition, with the added constraint that it can't be virtual.\nAnd yet another\
    \ similar rule exists for trivial default constructors, with the addition that\
    \ a default constructor is not-trivial if the class has non-static data members\
    \ with brace-or-equal-initializers, which we've seen above.\nHere are some examples\
    \ to clear everything up:\n// empty classes are trivial\nstruct Trivial1 {};\n\
    \n// all special members are implicit\nstruct Trivial2 {\n    int x;\n};\n\nstruct\
    \ Trivial3 : Trivial2 { // base class is trivial\n    Trivial3() = default; //\
    \ not a user-provided ctor\n    int y;\n};\n\nstruct Trivial4 {\npublic:\n   \
    \ int a;\nprivate: // no restrictions on access modifiers\n    int b;\n};\n\n\
    struct Trivial5 {\n    Trivial1 a;\n    Trivial2 b;\n    Trivial3 c;\n    Trivial4\
    \ d;\n};\n\nstruct Trivial6 {\n    Trivial2 a[23];\n};\n\nstruct Trivial7 {\n\
    \    Trivial6 c;\n    void f(); // it's okay to have non-virtual functions\n};\n\
    \nstruct Trivial8 {\n     int x;\n     static NonTrivial1 y; // no restrictions\
    \ on static members\n};\n\nstruct Trivial9 {\n     Trivial9() = default; // not\
    \ user-provided\n      // a regular constructor is okay because we still have\
    \ default ctor\n     Trivial9(int x) : x(x) {};\n     int x;\n};\n\nstruct NonTrivial1\
    \ : Trivial3 {\n    virtual void f(); // virtual members make non-trivial ctors\n\
    };\n\nstruct NonTrivial2 {\n    NonTrivial2() : z(42) {} // user-provided ctor\n\
    \    int z;\n};\n\nstruct NonTrivial3 {\n    NonTrivial3(); // user-provided ctor\n\
    \    int w;\n};\nNonTrivial3::NonTrivial3() = default; // defaulted but not on\
    \ first declaration\n                                      // still counts as\
    \ user-provided\nstruct NonTrivial5 {\n    virtual ~NonTrivial5(); // virtual\
    \ destructors are not trivial\n};\n\nStandard-layout\nStandard-layout is the second\
    \ property. The standard mentions that these are useful for communicating with\
    \ other languages, and that's because a standard-layout class has the same memory\
    \ layout of the equivalent C struct or union.\nThis is another property that must\
    \ hold recursively for members and all base classes. And as usual, no virtual\
    \ functions or virtual base classes are allowed. That would make the layout incompatible\
    \ with C.\nA relaxed rule here is that standard-layout classes must have all non-static\
    \ data members with the same access control. Previously these had to be all public,\
    \ but now you can make them private or protected, as long as they are all private\
    \ or all protected.\nWhen using inheritance, only one class in the whole inheritance\
    \ tree can have non-static data members, and the first non-static data member\
    \ cannot be of a base class type (this could break aliasing rules), otherwise,\
    \ it's not a standard-layout class.\nThis is how the definition goes in the standard\
    \ text:\n\nA standard-layout class is a class that:\nâ has no non-static data\
    \ members of type non-standard-layout class (or array of such types)\n  or reference,\n\
    â has no virtual functions (10.3) and no virtual base classes (10.1),\nâ has the\
    \ same access control (Clause 11) for all non-static data members,\nâ has no non-standard-layout\
    \ base classes,\nâ either has no non-static data members in the most derived class\
    \ and at most one base class with\n  non-static data members, or has no base classes\
    \ with non-static data members, and\nâ has no base classes of the same type as\
    \ the first non-static data member.\nA standard-layout struct is a standard-layout\
    \ class defined with the class-key struct or\n  the class-key class.\nA standard-layout\
    \ union is a standard-layout class defined with the class-key union.\n[ Note:\
    \ Standard-layout classes are useful for communicating with code written in other\
    \ programming languages. Their layout is specified in 9.2.âend note ]\n\nAnd let's\
    \ see a few examples.\n// empty classes have standard-layout\nstruct StandardLayout1\
    \ {};\n\nstruct StandardLayout2 {\n    int x;\n};\n\nstruct StandardLayout3 {\n\
    private: // both are private, so it's ok\n    int x;\n    int y;\n};\n\nstruct\
    \ StandardLayout4 : StandardLayout1 {\n    int x;\n    int y;\n\n    void f();\
    \ // perfectly fine to have non-virtual functions\n};\n\nstruct StandardLayout5\
    \ : StandardLayout1 {\n    int x;\n    StandardLayout1 y; // can have members\
    \ of base type if they're not the first\n};\n\nstruct StandardLayout6 : StandardLayout1,\
    \ StandardLayout5 {\n    // can use multiple inheritance as long only\n    //\
    \ one class in the hierarchy has non-static data members\n};\n\nstruct StandardLayout7\
    \ {\n    int x;\n    int y;\n    StandardLayout7(int x, int y) : x(x), y(y) {}\
    \ // user-provided ctors are ok\n};\n\nstruct StandardLayout8 {\npublic:\n   \
    \ StandardLayout8(int x) : x(x) {} // user-provided ctors are ok\n// ok to have\
    \ non-static data members and other members with different access\nprivate:\n\
    \    int x;\n};\n\nstruct StandardLayout9 {\n    int x;\n    static NonStandardLayout1\
    \ y; // no restrictions on static members\n};\n\nstruct NonStandardLayout1 {\n\
    \    virtual f(); // cannot have virtual functions\n};\n\nstruct NonStandardLayout2\
    \ {\n    NonStandardLayout1 X; // has non-standard-layout member\n};\n\nstruct\
    \ NonStandardLayout3 : StandardLayout1 {\n    StandardLayout1 x; // first member\
    \ cannot be of the same type as base\n};\n\nstruct NonStandardLayout4 : StandardLayout3\
    \ {\n    int z; // more than one class has non-static data members\n};\n\nstruct\
    \ NonStandardLayout5 : NonStandardLayout3 {}; // has a non-standard-layout base\
    \ class\n\nConclusion\nWith these new rules a lot more types can be PODs now.\
    \ And even if a type is not POD, we can take advantage of some of the POD properties\
    \ separately (if it is only one of trivial or standard-layout).\nThe standard\
    \ library has traits to test these properties in the header <type_traits>:\ntemplate\
    \ <typename T>\nstruct std::is_pod;\ntemplate <typename T>\nstruct std::is_trivial;\n\
    template <typename T>\nstruct std::is_trivially_copyable;\ntemplate <typename\
    \ T>\nstruct std::is_standard_layout;\n\n"
- - What are Aggregates and PODs and how/why are they special?
  - "\nWhat has changed for C++14\nWe can refer to the Draft C++14 standard for reference.\
    \ \nAggregates\nThis is covered in section 8.5.1 Aggregates which gives us the\
    \ following definition:\n\nAn aggregate is an array or a class (Clause 9) with\
    \ no user-provided\n  constructors (12.1), no private or protected non-static\
    \ data members\n  (Clause 11), no base classes (Clause 10), and no virtual functions\n\
    \  (10.3).\n\nThe only change is now adding in-class member initializers does\
    \ not make a class a non-aggregate. So the following example from C++11 aggregate\
    \ initialization for classes with member in-pace initializers:\nstruct A\n{\n\
    \  int a = 3;\n  int b = 3;\n};\n\nwas not an aggregate in C++11 but it is in\
    \ C++14. This change is covered in N3605: Member initializers and aggregates,\
    \ which has the following abstract:\n\nBjarne Stroustrup and Richard Smith raised\
    \ an issue about aggregate\n  initialization and member-initializers not working\
    \ together. This\n  paper proposes to fix the issue by adopting Smith's proposed\
    \ wording\n  that removes a restriction that aggregates can't have\n  member-initializers.\n\
    \nPOD stays the same\nThe definition for POD(plain old data) struct is covered\
    \ in section 9 Classes which says:\n\nA POD struct110 is a non-union class that\
    \ is both a trivial class and\n  a standard-layout class, and has no non-static\
    \ data members of type\n  non-POD struct, non-POD union (or array of such types).\
    \ Similarly, a\n  POD union is a union that is both a trivial class and a\n  standard-layout\
    \ class, and has no non-static data members of type\n  non-POD struct, non-POD\
    \ union (or array of such types). A POD class is\n  a class that is either a POD\
    \ struct or a POD union.\n\nwhich is the same wording as C++11.\nStandard-Layout\
    \ Changes for C++14\nAs noted in the comments pod relies on the definition of\
    \ standard-layout and that did change for C++14 but this was via defect reports\
    \ that were applied to C++14 after the fact.\nThere were three DRs:\n\nDR 1672\n\
    DR 1813\nDR 2120\n\nSo standard-layout went from this Pre C++14:\n\nA standard-layout\
    \ class is a class that:\n\n(7.1) has no non-static data members of type non-standard-layout\
    \ class (or array of such types) or reference,  \n(7.2) has no virtual functions\
    \ ([class.virtual]) and no virtual base classes ([class.mi]),  \n(7.3) has the\
    \ same access control (Clause [class.access]) for all non-static data members,\
    \  \n(7.4) has no non-standard-layout base classes,  \n(7.5) either has no non-static\
    \ data members in the most derived class and at most one base class with non-static\
    \ data members, or has\n  no base classes with non-static data members, and  \n\
    (7.6) has no base classes of the same type as the first non-static data member.109\n\
    \n\nTo this in C++14:\n\nA class S is a standard-layout class if it:\n\n(3.1)\
    \ has no non-static data members of type non-standard-layout class (or array of\
    \ such types) or reference,  \n(3.2) has no virtual functions and no virtual base\
    \ classes,  \n(3.3) has the same access control for all non-static data members,\
    \  \n(3.4) has no non-standard-layout base classes,  \n(3.5) has at most one base\
    \ class subobject of any given type,  \n(3.6) has all non-static data members\
    \ and bit-fields in the class and its base classes first declared in the same\
    \ class, and  \n(3.7) has no element of the set M(S) of types as a base class,\
    \ where for any type X, M(X) is defined as follows.104\n  [âNote: M(X) is the\
    \ set of the types of all non-base-class subobjects that may be at a zero offset\
    \ in X.\n  ââend note\n  â]  \n  \n  \n(3.7.1) If X is a non-union class type\
    \ with no (possibly inherited) non-static data members, the set M(X) is empty.\
    \  \n(3.7.2) If X is a non-union class type with a non-static data member of type\
    \ X0 that is either of zero size or is the first\n  non-static data member of\
    \ X (where said member may be an anonymous\n  union), the set M(X) consists of\
    \ X0 and the elements of M(X0).  \n(3.7.3) If X is a union type, the set M(X)\
    \ is the union of all M(Ui) and the set containing all Ui, where each Ui is the\
    \ type of the\n  ith non-static data member of X.  \n(3.7.4) If X is an array\
    \ type with element type Xe, the set M(X) consists of Xe and the elements of M(Xe).\
    \  \n(3.7.5) If X is a non-class, non-array type, the set M(X) is empty.\n\n\n\
    \n"
- - Why does C++ compilation take so long?
  - "\nSeveral reasons\nHeader files\nEvery single compilation unit requires hundreds\
    \ or even thousands of headers to be (1) loaded and (2) compiled.\nEvery one of\
    \ them typically has to be recompiled for every compilation unit,\nbecause the\
    \ preprocessor ensures that the result of compiling a header might vary between\
    \ every compilation unit.\n(A macro may be defined in one compilation unit which\
    \ changes the content of the header).\nThis is probably the main reason, as it\
    \ requires huge amounts of code to be compiled for every compilation unit,\nand\
    \ additionally, every header has to be compiled multiple times\n(once for every\
    \ compilation unit that includes it).\nLinking\nOnce compiled, all the object\
    \ files have to be linked together.\n  This is basically a monolithic process\
    \ that can't very well be parallelized, and has to process your entire project.\n\
    Parsing\nThe syntax is extremely complicated to parse, depends heavily on context,\
    \ and is very hard to disambiguate.\nThis takes a lot of time.\nTemplates\nIn\
    \ C#, List<T> is the only type that is compiled, no matter how many instantiations\
    \ of List you have in your program.\nIn C++, vector<int> is a completely separate\
    \ type from vector<float>, and each one will have to be compiled separately.\n\
    Add to this that templates make up a full Turing-complete \"sub-language\" that\
    \ the compiler has to interpret,\nand this can become ridiculously complicated.\n\
    Even relatively simple template metaprogramming code can define recursive templates\
    \ that create dozens and dozens of template instantiations.\nTemplates may also\
    \ result in extremely complex types, with ridiculously long names, adding a lot\
    \ of extra work to the linker.\n(It has to compare a lot of symbol names, and\
    \ if these names can grow into many thousand characters, that can become fairly\
    \ expensive).\nAnd of course, they exacerbate the problems with header files,\
    \ because templates generally have to be defined in headers,\nwhich means far\
    \ more code has to be parsed and compiled for every compilation unit.\nIn plain\
    \ C code, a header typically only contains forward declarations, but very little\
    \ actual code.\nIn C++, it is not uncommon for almost all the code to reside in\
    \ header files.\nOptimization\nC++ allows for some very dramatic optimizations.\n\
    C# or Java don't allow classes to be completely eliminated (they have to be there\
    \ for reflection purposes),\nbut even a simple C++ template metaprogram can easily\
    \ generate dozens or hundreds of classes,\nall of which are inlined and eliminated\
    \ again in the optimization phase.\nMoreover, a C++ program must be fully optimized\
    \ by the compiler.\nA C# program can rely on the JIT compiler to perform additional\
    \ optimizations at load-time,\nC++ doesn't get any such \"second chances\". What\
    \ the compiler generates is as optimized as it's going to get.\nMachine\nC++ is\
    \ compiled to machine code which may be somewhat more complicated than the bytecode\
    \ Java or .NET use (especially in the case of x86).\n(This is mentioned out of\
    \ completeness only because it was mentioned in comments and such.\nIn practice,\
    \ this step is unlikely to take more than a tiny fraction of the total compilation\
    \ time).\nConclusion\nMost of these factors are shared by C code, which actually\
    \ compiles fairly efficiently.\nThe parsing step is a lot more complicated in\
    \ C++, and can take up significantly more time, but the main offender is probably\
    \ templates.\nThey're useful, and make C++ a far more powerful language, but they\
    \ also take their toll in terms of compilation speed.\n"
- - Why does C++ compilation take so long?
  - "\nThe slowdown is not necessarily the same with any compiler.\nI haven't used\
    \ Delphi or Kylix but back in the MS-DOS days, a Turbo Pascal program would compile\
    \ almost instantaneously, while the equivalent Turbo C++ program would just crawl.\
    \ \nThe two main differences were a very strong module system and a syntax that\
    \ allowed single-pass compilation.\nIt's certainly possible that compilation speed\
    \ just hasn't been a priority for C++ compiler developers, but there are also\
    \ some inherent complications in the C/C++ syntax that make it more difficult\
    \ to process. (I'm not an expert on C, but Walter Bright is, and after building\
    \ various commercial C/C++ compilers, he created the D language. One of his changes\
    \ was to enforce a context-free grammar to make the language easier to parse.)\n\
    Also, you'll notice that generally Makefiles are set up so that every file is\
    \ compiled separately in C, so if 10 source files all use the same include file,\
    \ that include file is processed 10 times.\n"
- - Why does C++ compilation take so long?
  - "\nParsing and code generation are actually rather fast.  The real problem is\
    \ opening and closing files.  Remember, even with include guards, the compiler\
    \ still have open the .H file, and read each line (and then ignore it).\nA friend\
    \ once (while bored at work), took his company's application and put everything\
    \ -- all source and header files--  into one big file.   Compile time dropped\
    \ from 3 hours to 7 minutes.\n"
- - Does the 'mutable' keyword have any purpose other than allowing the variable to
    be modified by a const function?
  - "\nIt allows the differentiation of bitwise const and logical const.  Logical\
    \ const is when an object doesn't change in a way that is visible through the\
    \ public interface, like your locking example.  Another example would be a class\
    \ that computes a value the first time it is requested, and caches the result.\
    \ \nSince c++11 mutable can be used on a lambda to denote that things captured\
    \ by value are modifiable (they aren't by default):\nint x = 0;\nauto f1 = [=]()\
    \ mutable {x = 42;};  // OK\nauto f2 = [=]()         {x = 42;};  // Error: a by-value\
    \ capture cannot be modified in a non-mutable lambda\n\n"
- - Does the 'mutable' keyword have any purpose other than allowing the variable to
    be modified by a const function?
  - "\nThe mutable keyword is a way to pierce the const veil you drape over your objects.\
    \ If you have a const reference or pointer to an object, you cannot modify that\
    \ object in any way except when and how it is marked mutable.\nWith your const\
    \ reference or pointer you are constrained to:\n\nonly read access for any visible\
    \ data members\npermission to call only methods that are marked as const. \n\n\
    The mutable exception makes it so you can now write or set data members that are\
    \ marked mutable. That's the only externally visible difference.\nInternally those\
    \ const methods that are visible to you can also write to data members that are\
    \ marked mutable.  Essentially the const veil is pierced comprehensively.  It\
    \ is completely up to the API designer to ensure that mutable doesn't destroy\
    \ the const concept and is only used in useful special cases. The mutable keyword\
    \ helps because it clearly marks data members that are subject to these special\
    \ cases.\nIn practice you can use const obsessively throughout your codebase (you\
    \ essentially want to \"infect\" your codebase with the const \"disease\").  In\
    \ this world pointers and references are const with very few exceptions, yielding\
    \ code that is easier to reason about and understand.  For a interesting digression\
    \ look up \"referential transparency\". \nWithout the mutable keyword you will\
    \ eventually be forced to use const_cast to handle the various useful special\
    \ cases it allows (caching, ref counting, debug data, etc.). Unfortunately const_cast\
    \ is significantly more destructive than mutable because it forces the API client\
    \ to destroy the const protection of the objects (s)he is using. Additionally\
    \ it causes widespread const destruction: const_casting a const pointer or reference\
    \ allows unfettered write and method calling access to visible members.  In contrast\
    \ mutable requires the API designer to exercise fine grained control over the\
    \ const exceptions, and usually these exceptions are hidden in const methods operating\
    \ on private data.\n(N.B. I refer to to data and method visibility a few times.\
    \ I'm talking about members marked as public vs. private or protected which is\
    \ a totally different type of object protection discussed here.)\n"
- - Does the 'mutable' keyword have any purpose other than allowing the variable to
    be modified by a const function?
  - "\nYour use with boost::mutex is exactly what this keyword is intended for. Another\
    \ use is for internal result caching to speed access.\nBasically, 'mutable' applies\
    \ to any class attribute that does not affect the externally visible state of\
    \ the object.\nIn the sample code in your question, mutable might be inappropriate\
    \ if the value of done_ affects external state, it depends on what is in the ...;\
    \ part.\n"
- - How can I get the list of files in a directory using C or C++?
  - "\nIn small and simple tasks I do not use boost, I use dirent.h which is also\
    \ available for windows:\nDIR *dir;\nstruct dirent *ent;\nif ((dir = opendir (\"\
    c:\\\\src\\\\\")) != NULL) {\n  /* print all the files and directories within\
    \ directory */\n  while ((ent = readdir (dir)) != NULL) {\n    printf (\"%s\\\
    n\", ent->d_name);\n  }\n  closedir (dir);\n} else {\n  /* could not open directory\
    \ */\n  perror (\"\");\n  return EXIT_FAILURE;\n}\n\nIt is just a small header\
    \ file and does most of the simple stuff you need without using a big template-based\
    \ approach like boost(no offence, I like boost!).\nThe author of the windows compatibility\
    \ layer is Toni Ronkko. In Unix, it is a standard header.\nUPDATE 2017:\nIn C++17\
    \ there is now an official way to list files of your file system: std::filesystem.\
    \ There is an excellent answer from Shreevardhan below with this source code:\n\
    #include <string>\n#include <iostream>\n#include <filesystem>\nnamespace fs =\
    \ std::filesystem;\n\nint main()\n{\n    std::string path = \"/path/to/directory\"\
    ;\n    for (const auto & entry : fs::directory_iterator(path))\n        std::cout\
    \ << entry.path() << std::endl;\n}\n\nConsider upvoting his answer, if you are\
    \ using the C++17 approach.\n"
- - How can I get the list of files in a directory using C or C++?
  - "\nC++17 now has a std::filesystem::directory_iterator, which can be used as\n\
    #include <string>\n#include <iostream>\n#include <filesystem>\nnamespace fs =\
    \ std::filesystem;\n\nint main()\n{\n    std::string path = \"/path/to/directory\"\
    ;\n    for (const auto & entry : fs::directory_iterator(path))\n        std::cout\
    \ << entry.path() << std::endl;\n}\n\nAlso, std::filesystem::recursive_directory_iterator\
    \ can iterate the subdirectories as well.\n"
- - How can I get the list of files in a directory using C or C++?
  - "\nUnfortunately the C++ standard does not define a standard way of working with\
    \ files and folders in this way. \nSince there is no cross platform way, the best\
    \ cross platform way is to use a library such as the boost filesystem module.\n\
    Cross platform boost method:\n\nThe following function, given a directory path\
    \ and a file name, recursively searches the directory and its sub-directories\
    \ for the file name, returning a bool, and if successful, the path to the file\
    \ that was found. \n\nbool find_file(const path & dir_path,         // in this\
    \ directory,\n               const std::string & file_name, // search for this\
    \ name,\n               path & path_found)             // placing path here if\
    \ found\n{\n    if (!exists(dir_path)) \n        return false;\n\n    directory_iterator\
    \ end_itr; // default construction yields past-the-end\n\n    for (directory_iterator\
    \ itr(dir_path); itr != end_itr; ++itr)\n    {\n        if (is_directory(itr->status()))\n\
    \        {\n            if (find_file(itr->path(), file_name, path_found)) \n\
    \                return true;\n        }\n        else if (itr->leaf() == file_name)\
    \ // see below\n        {\n            path_found = itr->path();\n           \
    \ return true;\n        }\n    }\n    return false;\n}\n\nSource from the boost\
    \ page mentioned above.\n\nFor Unix/Linux based systems: \nYou can use opendir\
    \ / readdir / closedir. \n\nSample code which searches a directory for entry ``name''\
    \ is:\n\n   len = strlen(name);\n   dirp = opendir(\".\");\n   while ((dp = readdir(dirp))\
    \ != NULL)\n           if (dp->d_namlen == len && !strcmp(dp->d_name, name)) {\n\
    \                   (void)closedir(dirp);\n                   return FOUND;\n\
    \           }\n   (void)closedir(dirp);\n   return NOT_FOUND;\n\nSource code from\
    \ the above man pages.\n\nFor a windows based systems: \nyou can use the Win32\
    \ API FindFirstFile / FindNextFile / FindClose functions.\n\nThe following C++\
    \ example shows you a minimal use of FindFirstFile.\n\n#include <windows.h>\n\
    #include <tchar.h>\n#include <stdio.h>\n\nvoid _tmain(int argc, TCHAR *argv[])\n\
    {\n   WIN32_FIND_DATA FindFileData;\n   HANDLE hFind;\n\n   if( argc != 2 )\n\
    \   {\n      _tprintf(TEXT(\"Usage: %s [target_file]\\n\"), argv[0]);\n      return;\n\
    \   }\n\n   _tprintf (TEXT(\"Target file is %s\\n\"), argv[1]);\n   hFind = FindFirstFile(argv[1],\
    \ &FindFileData);\n   if (hFind == INVALID_HANDLE_VALUE) \n   {\n      printf\
    \ (\"FindFirstFile failed (%d)\\n\", GetLastError());\n      return;\n   } \n\
    \   else \n   {\n      _tprintf (TEXT(\"The first file found is %s\\n\"), \n \
    \               FindFileData.cFileName);\n      FindClose(hFind);\n   }\n}\n\n\
    Source code from the above msdn pages.\n"
- - What is std::move(), and when should it be used?
  - "\nWikipedia Page on C++11 R-value references and move constructors\n\nIn C++11,\
    \ in addition to copy constructors, objects can have move constructors.\n(And\
    \ in addition to copy assignment operators, they have move assignment operators.)\n\
    The move constructor is used instead of the copy constructor, if the object has\
    \ type \"rvalue-reference\" (Type &&).\nstd::move() is a cast that produces an\
    \ rvalue-reference to an object, to enable moving from it.\n\nIt's a new C++ way\
    \ to avoid copies. For example, using a move constructor, a std::vector could\
    \ just copy its internal pointer to data to the new object, leaving the moved\
    \ object in an incorrect state, avoiding to copy all data. This would be C++-valid.\n\
    Try googling for move semantics, rvalue, perfect forwarding.\n"
- - What is std::move(), and when should it be used?
  - "\nYou can use move when you need to \"transfer\" the content of an object somewhere\
    \ else, without doing a copy (e.g the content is not duplicated, that's why it\
    \ could be use on some non-copyable objects, like an unique_ptr). It's also possible\
    \ for an object to take the content of a temporary object without doing a copy\
    \ (and save a lot of time), with std::move.\nThis link really helped me out :\
    \ \nhttp://thbecker.net/articles/rvalue_references/section_01.html\nI'm sorry\
    \ if my answer is coming too late, but I was also looking for a good link for\
    \ the std::move, and I found the links above a little bit \"austere\". \nThis\
    \ put the emphasis on r-value reference, in which context you should use them,\
    \ and I think it's more detailed, that's why I wanted to share this link here.\n"
- - What is std::move(), and when should it be used?
  - "\n1. \"What is it?\"\nWhile std::move()  is technically a function - I would\
    \ say it isn't really a function. It's sort of a converter between ways the compiler\
    \ considers an expression's value.\n2. \"What does it do?\"\nThe first thing to\
    \ note is that std::move() doesn't actually move anything.\nIf you've ever watched\
    \ the animation series Bleach - it does the equivalent of Quincy Seele Schneider's\
    \  Reishi softening.\nSeriously, though, it converts an expression from being\
    \ an lvalue or pure rvalue (such as a variable you might be using for a long time\
    \ yet, or a temporary you're passing around for a while, respectively) to being\
    \ an xvalue. An xvalue tells the compiler:\n\nYou can plunder me, move anything\
    \ I'm holding and use it elsewhere (since I'm going to be destroyed soon anyway)\"\
    .\n\nin other words, when you use std::move(x), you're allowing the compiler to\
    \ cannibalize x. Thus if x has, say, its own buffer in memory - after std::move()ing\
    \ the compiler can have another object own it instead.\n3. \"When should it be\
    \ used?\"\nAnother way to ask this question is \"What would I cannibalize an existing\
    \ object's resources for?\" well, if you're writing application code, you would\
    \ probably not be messing around a lot with temporary objects created by the compiler.\
    \ So mainly you would do this in places like constructors, operator methods, STL-algorithm-like\
    \ functions etc. where objects get created and destroyed automagically alot. Of\
    \ course, that's just a rule of thumb.\nA typical use is 'moving' resources from\
    \ one object to another instead of copying.  @Guillaume links to this page which\
    \ has a straightforward short example: swapping two objects with less copying.\n\
    template <class T>\nswap(T& a, T& b) {\n    T tmp(a);   // we now have two copies\
    \ of a\n    a = b;      // we now have two copies of b (+ discarded a copy of\
    \ a)\n    b = tmp;    // we now have two copies of tmp (+ discarded a copy of\
    \ b)\n}\n\nusing move allows you to swap the resources instead of copying them\
    \ around:\ntemplate <class T>\nswap(T& a, T& b) {\n    T tmp(std::move(a));\n\
    \    a = std::move(b);   \n    b = std::move(tmp);\n}\n\nThink of what happens\
    \ when T is, say, vector<int> of size n. In the first version you read and write\
    \ 3*n elements, in the second version you basically read and write just the 3\
    \ pointers to the vectors' buffers. Of course, class T needs to know how to do\
    \ the moving; you should have a move-assignment operator and a move-constructor\
    \ for class T for this to work.\n"
- - What is a segmentation fault?
  - "\nSegmentation fault is a specific kind of error caused by accessing memory that\
    \ âdoes not belong to you.â Itâs a helper mechanism that keeps you from corrupting\
    \ the memory and introducing hard-to-debug memory bugs. Whenever you get a segfault\
    \ you know you are doing something wrong with memory â accessing variable that\
    \ has already been freed, writing to a read-only portion of the memory, etc. Segmentation\
    \ fault is essentially the same in most languages that let you mess with the memory\
    \ management, there is no principial difference between segfaults in C and C++.\n\
    There are many ways to get a segfault, at least in the lower-level languages such\
    \ as C(++). AÂ common way to get a segfault is to dereference a null pointer:\n\
    int *p = NULL;\n*p = 1;\n\nAnother segfault happens when you try to write to a\
    \ portion of memory that was marked as read-only:\nchar *str = \"Foo\"; // Compiler\
    \ marks the constant string as read-only\n*str = 'b'; // Which means this is illegal\
    \ and results in a segfault\n\nDangling pointer points to a thing that does not\
    \ exist any more, like here:\nchar *p = NULL;\n{\n    char c;\n    p = &c;\n}\n\
    // Now p is dangling\n\nThe pointer p dangles because it points to character variable\
    \ c that ceased to exist after the block ended. And when you try to dereference\
    \ dangling pointer (like *p='A'), you would probably get a segfault.\n"
- - What is a segmentation fault?
  - "\nIt would be worth noting that segmentation fault isn't caused by directly accessing\
    \ another process memory (this is what I'm hearing sometimes), as it is simply\
    \ not possible. With virtual memory every process has its own virtual address\
    \ space and there is no way to access another one using any value of pointer.\
    \ Exception to this can be shared libraries which are same physical address space\
    \ mapped to (possibly) different virtual addresses and kernel memory which is\
    \ even mapped in the same way in every process (to avoid TLB flushing on syscall,\
    \ I think). And things like shmat ;) - these are what I count as 'indirect' access.\
    \ One can, however, check that they are usually located long way from process\
    \ code and we are usually able to access them (this is why they are there, nevertheless\
    \ accessing them in a improper way will produce segmentation fault).\nStill, segmentation\
    \ fault can occur in case of accessing our own (process) memory in improper way\
    \ (for instance trying to write to non-writable space). But the most common reason\
    \ for it is the access to the part of the virtual address space that is not mapped\
    \ to physical one at all.\nAnd all of this with respect to virtual memory systems.\n"
- - What is a segmentation fault?
  - "\nA segmentation fault is caused by a request for a page that the process does\
    \ not have listed in its descriptor table, or an invalid request for a page that\
    \ it does have listed (e.g. a write request on a read-only page).\nA dangling\
    \ pointer is a pointer that may or may not point to a valid page, but does point\
    \ to an \"unexpected\" segment of memory.\n"
- - C++ code file extension? .cc vs .cpp
  - "\nAt the end of the day it doesn't matter because C++ compilers can deal with\
    \ the files in either format.  If it's a real issue within your team, flip a coin\
    \ and move on to the actual work.  \n"
- - C++ code file extension? .cc vs .cpp
  - "\nGNU GCC recognises all of the following as C++ files, and will use C++ compilation\
    \ regardless of whether you invoke it through gcc or g++: .C, .cc, .cpp, .CPP,\
    \ .c++, .cp, or .cxx.\nNote the .C - case matters in GCC, .c is a C file whereas\
    \ .C is a C++ file (if you let the compiler decide what it is compiling that is).\n\
    GCC also supports other suffixes to indicate special handling, for example a .ii\
    \ file will be compiled as C++, but not pre-processed (intended for separately\
    \ pre-processed code). All the recognised suffixes are detailed at gcc.gnu.org\n"
- - C++ code file extension? .cc vs .cpp
  - "\nGreat advice on which to use for the makefile and other tools, considering\
    \ non-compiler tools while deciding on which extension to use is a great approach\
    \ to help find an answer that works for you.\nI just wanted to add the following\
    \ to help with some .cc vs .cpp info that I found. The following are extensions\
    \ broken down by different environments (from the \"C++ Primer Plus\" book):\n\
    Unix uses: .C, .cc, .cxx, .c\nGNU C++ uses:  .C, .cc, .cxx, .cpp, .c++\nDigital\
    \ Mars uses:  .cpp, .cxx\nBorland C++ uses: .cpp\nWatcom uses:  .cpp\nMicrosoft\
    \ Visual C++ uses:  .cpp, .cxx, .cc\nMetrowerks CodeWarrior uses:  .cpp, .cp,\
    \ .cc, .cxx, .c++\nThe different environments support different extensions.  I\
    \ too was looking to answer this question and found this post. Based on this post\
    \ I think I might go with .hpp and .cpp for ease of cross-platform/cross-tool\
    \ recognition.  \n"
- - Why is this program erroneously rejected by three C++ compilers?
  - "\nIn the standard, Â§2.1/1 specifies:\n\nPhysical source file characters are mapped,\
    \ in an implementation-defined manner, to the basic source character set (introducing\
    \ new-line characters for end-of-line indicators) if necessary.\n\nYour compiler\
    \ doesn't support that format (aka cannot map it to the basic source character\
    \ set), so it cannot move into further processing stages, hence the error. It\
    \ is entirely possible that your compiler support a mapping from image to basic\
    \ source character set, but is not required to. \nSince this mapping is implementation-defined,\
    \ you'll need to look at your implementations documentation to see the file formats\
    \ it supports. Typically, every major compiler vendor supports (canonically defined)\
    \ text files: any file produced by a text editor, typically a series of characters.\
    \ \n\nNote that the C++ standard is based off the C standard (Â§1.1/2), and the\
    \ C(99) standard says, in Â§1.2:\n\nThis International Standard does not specify\n\
    \  â the mechanism by which C programs are transformed for use by a data-processing\n\
    \  system;\n  â the mechanism by which C programs are invoked for use by a data-processing\n\
    \  system;\n  â the mechanism by which input data are transformed for use by a\
    \ C program;\n\nSo, again, the treatment of source files is something you need\
    \ to find in your compilers documentation.\n"
- - Why is this program erroneously rejected by three C++ compilers?
  - "\n\nOriginally from Overv @ reddit.\n"
- - Why is this program erroneously rejected by three C++ compilers?
  - "\nTry this way:\n\n"
- - 'C++: âstd::endlâ vs â\nâ'
  - "\nThe varying line-ending characters don't matter, assuming the file is open\
    \ in text mode, which is what you get unless you ask for binary.  The compiled\
    \ program will write out the correct thing for the system compiled for.\nThe only\
    \ difference is that std::endl flushes the output buffer, and '\\n' doesn't. \
    \ If you don't want the buffer flushed frequently, use '\\n'.  If you do (for\
    \ example, if you want to get all the output, and the program is unstable), use\
    \ std::endl.\n"
- - 'C++: âstd::endlâ vs â\nâ'
  - "\nThe difference can be illustrated by the following:\nstd::cout << std::endl;\n\
    \nis equivalent to\nstd::cout << '\\n' << std::flush;\n\nSo,\n\nUse std::endl\
    \ If you want to force an immediate flush to the output.\nUse \\n if you are worried\
    \ about performance (which is probably not the case if you are using the << operator).\n\
    \nI use \\n on most lines.\nThen use std::endl at the end of a paragraph (but\
    \ that is just a habit and not usually necessary).\nContrary to other claims,\
    \ the \\n character is mapped to the correct platform end of line sequence only\
    \ if the stream is going to a file (std::cin and std::cout being special but still\
    \ files (or file-like)).\n"
- - 'C++: âstd::endlâ vs â\nâ'
  - "\nThere might be performance issues, std::endl forces a flush of the output stream.\n"
- - 'Which is faster: Stack allocation or Heap allocation'
  - "\nStack allocation is much faster since all it really does is move the stack\
    \ pointer. \nUsing memory pools, you can get comparable performance out of heap\
    \ allocation, but that comes with a slight added complexity and its own headaches.\
    \ \nAlso, stack vs. heap is not only a performance consideration; it also tells\
    \ you a lot about the expected lifetime of objects. \n"
- - 'Which is faster: Stack allocation or Heap allocation'
  - "\nStack is much faster.  It literally only uses a single instruction on most\
    \ architectures, in most cases, e.g. on x86:\nsub esp, 0x10\n\n(That moves the\
    \ stack pointer down by 0x10 bytes and thereby \"allocates\" those bytes for use\
    \ by a variable.)\nOf course, the stack's size is very, very finite, as you will\
    \ quickly find out if you overuse stack allocation or try to do recursion :-)\n\
    Also, there's little reason to optimize the performance of code that doesn't verifiably\
    \ need it, such as demonstrated by profiling.  \"Premature optimization\" often\
    \ causes more problems than it's worth.\nMy rule of thumb: if I know I'm going\
    \ to need some data at compile-time, and it's under a few hundred bytes in size,\
    \ I stack-allocate it.  Otherwise I heap-allocate it.\n"
- - 'Which is faster: Stack allocation or Heap allocation'
  - "\nHonestly, it's trivial to write a program to compare the performance:\n#include\
    \ <ctime>\n#include <iostream>\n\nnamespace {\n    class empty { }; // even empty\
    \ classes take up 1 byte of space, minimum\n}\n\nint main()\n{\n    std::clock_t\
    \ start = std::clock();\n    for (int i = 0; i < 100000; ++i)\n        empty e;\n\
    \    std::clock_t duration = std::clock() - start;\n    std::cout << \"stack allocation\
    \ took \" << duration << \" clock ticks\\n\";\n    start = std::clock();\n   \
    \ for (int i = 0; i < 100000; ++i) {\n        empty* e = new empty;\n        delete\
    \ e;\n    };\n    duration = std::clock() - start;\n    std::cout << \"heap allocation\
    \ took \" << duration << \" clock ticks\\n\";\n}\n\nIt's said that a foolish consistency\
    \ is the hobgoblin of little minds.  Apparently optimizing compilers are the hobgoblins\
    \ of many programmers' minds.  This discussion used to be at the bottom of the\
    \ answer, but people apparently can't be bothered to read that far, so I'm moving\
    \ it up here to avoid getting questions that I've already answered.\nAn optimizing\
    \ compiler may notice that this code does nothing, and may optimize it all away.\
    \ It is the optimizer's job to do stuff like that, and fighting the optimizer\
    \ is a fool's errand.\nI would recommend compiling this code with optimization\
    \ turned off because there is no good way to fool every optimizer currently in\
    \ use or that will be in use in the future.\nAnybody who turns the optimizer on\
    \ and then complains about fighting it should be subject to public ridicule.\n\
    If I cared about nanosecond precision I wouldn't use std::clock(). If I wanted\
    \ to publish the results as a doctoral thesis I would make a bigger deal about\
    \ this, and I would probably compare GCC, Tendra/Ten15, LLVM, Watcom, Borland,\
    \ Visual C++, Digital Mars, ICC and other compilers.  As it is, heap allocation\
    \ takes hundreds of times longer than stack allocation, and I don't see anything\
    \ useful about investigating the question any further.\nThe optimizer has a mission\
    \ to get rid of the code I'm testing.  I don't see any reason to tell the optimizer\
    \ to run and then try to fool the optimizer into not actually optimizing.  But\
    \ if I saw value in doing that, I would do one or more of the following:\n\nAdd\
    \ a data member to empty, and access that data member in the loop; but if I only\
    \ ever read from the data member the optimizer can do constant folding and remove\
    \ the loop; if I only ever write to the data member, the optimizer may skip all\
    \ but the very last iteration of the loop.  Additionally, the question wasn't\
    \ \"stack allocation and data access vs. heap allocation and data access.\"\n\
    Declare e volatile, but volatile is often compiled incorrectly (PDF).\nTake the\
    \ address of e inside the loop (and maybe assign it to a variable that is declared\
    \ extern and defined in another file).  But even in this case, the compiler may\
    \ notice that -- on the stack at least -- e will always be allocated at the same\
    \ memory address, and then do constant folding like in (1) above.  I get all iterations\
    \ of the loop, but the object is never actually allocated.\n\nBeyond the obvious,\
    \ this test is flawed in that it measures both allocation and deallocation, and\
    \ the original question didn't ask about deallocation.  Of course variables allocated\
    \ on the stack are automatically deallocated at the end of their scope, so not\
    \ calling delete would (1) skew the numbers (stack deallocation is included in\
    \ the numbers about stack allocation, so it's only fair to measure heap deallocation)\
    \ and (2) cause a pretty bad memory leak, unless we keep a reference to the new\
    \ pointer and call delete after we've got our time measurement.\nOn my machine,\
    \ using g++ 3.4.4 on Windows, I get \"0 clock ticks\" for both stack and heap\
    \ allocation for anything less than 100000 allocations, and even then I get \"\
    0 clock ticks\" for stack allocation and \"15 clock ticks\" for heap allocation.\
    \  When I measure 10,000,000 allocations, stack allocation takes 31 clock ticks\
    \ and heap allocation takes 1562 clock ticks.\n\nYes, an optimizing compiler may\
    \ elide creating the empty objects.  If I understand correctly, it may even elide\
    \ the whole first loop.  When I bumped up the iterations to 10,000,000 stack allocation\
    \ took 31 clock ticks and heap allocation took 1562 clock ticks.  I think it's\
    \ safe to say that without telling g++ to optimize the executable, g++ did not\
    \ elide the constructors.\n\nIn the years since I wrote this, the preference on\
    \ Stack Overflow has been to post performance from optimized builds.  In general,\
    \ I think this is correct.  However, I still think it's silly to ask the compiler\
    \ to optimize code when you in fact do not want that code optimized.  It strikes\
    \ me as being very similar to paying extra for valet parking, but refusing to\
    \ hand over the keys.  In this particular case, I don't want the optimizer running.\n\
    Using a slightly modified version of the benchmark (to address the valid point\
    \ that the original program didn't allocate something on the stack each time through\
    \ the loop) and compiling without optimizations but linking to release libraries\
    \ (to address the valid point that we don't want to include any slowdown caused\
    \ by linking to debug libraries):\n#include <cstdio>\n#include <chrono>\n\nnamespace\
    \ {\n    void on_stack()\n    {\n        int i;\n    }\n\n    void on_heap()\n\
    \    {\n        int* i = new int;\n        delete i;\n    }\n}\n\nint main()\n\
    {\n    auto begin = std::chrono::system_clock::now();\n    for (int i = 0; i <\
    \ 1000000000; ++i)\n        on_stack();\n    auto end = std::chrono::system_clock::now();\n\
    \n    std::printf(\"on_stack took %f seconds\\n\", std::chrono::duration<double>(end\
    \ - begin).count());\n\n    begin = std::chrono::system_clock::now();\n    for\
    \ (int i = 0; i < 1000000000; ++i)\n        on_heap();\n    end = std::chrono::system_clock::now();\n\
    \n    std::printf(\"on_heap took %f seconds\\n\", std::chrono::duration<double>(end\
    \ - begin).count());\n    return 0;\n}\n\ndisplays:\non_stack took 2.070003 seconds\n\
    on_heap took 57.980081 seconds\n\non my system when compiled with the command\
    \ line cl foo.cc /Od /MT /EHsc.\nYou may not agree with my approach to getting\
    \ a non-optimized build.  That's fine:  feel free modify the benchmark as much\
    \ as you want.  When I turn on optimization, I get:\non_stack took 0.000000 seconds\n\
    on_heap took 51.608723 seconds\n\nNot because stack allocation is actually instantaneous\
    \ but because any half-decent compiler can notice that on_stack doesn't do anything\
    \ useful and can be optimized away.  GCC on my Linux laptop also notices that\
    \ on_heap doesn't do anything useful, and optimizes it away as well:\non_stack\
    \ took 0.000003 seconds\non_heap took 0.000002 seconds\n\n"
- - Iterator invalidation rules
  - "\nC++17 (All references are from the final working draft of CPP17 - n4659)\n\n\
    Insertion\nSequence Containers \n\nvector:  The functions insert, emplace_back,\
    \ emplace, push_back cause reallocation if the new size is greater than the old\
    \ capacity. Reallocation invalidates all the references, pointers, and iterators\
    \ referring to the elements in the sequence. If no reallocation\nhappens, all\
    \ the iterators and references before the insertion point remain valid. [26.3.11.5/1]\n\
    With respect to the reserve function, reallocation invalidates all the references,\
    \ pointers, and iterators referring to the elements in the sequence. No reallocation\
    \ shall take place during insertions that happen after a call to reserve() until\
    \ the time when an insertion would make the size of the vector greater than the\
    \ value of capacity(). [26.3.11.3/6]   \ndeque: An insertion in the middle of\
    \ the deque invalidates all the iterators and references to elements of the deque.\
    \ An insertion at either end of the deque invalidates all the iterators to the\
    \ deque, but has no effect on the validity of references to elements of the deque.\
    \ [26.3.8.4/1]\nlist: Does not affect the validity of iterators and references.\
    \ If an exception is thrown there are no effects. [26.3.10.4/1].\nThe insert,\
    \ emplace_front, emplace_back, emplace, push_front, push_back functions are covered\
    \ under this rule.\nforward_list: None of the overloads of insert_after shall\
    \ affect the validity of iterators and references [26.3.9.5/1]\narray: As a rule,\
    \ iterators to an array are never invalidated throughout the lifetime of the array.\
    \ One should take note, however, that during swap, the iterator will continue\
    \ to point to the same array element, and will thus change its value.\n\nAssociative\
    \ Containers\n\nAll Associative Containers: The insert and emplace members shall\
    \ not affect the validity of iterators and references to the container [26.2.6/9]\n\
    \nUnordered Associative Containers\n\nAll Unordered Associative Containers: Rehashing\
    \ invalidates iterators, changes ordering between elements, and changes which\
    \ buckets elements appear in, but does not invalidate pointers or references to\
    \ elements. [26.2.7/9]\nThe insert and emplace members shall not affect the validity\
    \ of references to container elements, but may invalidate all iterators to the\
    \ container. [26.2.7/14]\nThe insert and emplace members shall not affect the\
    \ validity of iterators if (N+n) <= z * B, where N is the number of elements in\
    \ the container prior to the insert operation, n is the number of elements inserted,\
    \ B is the containerâs bucket count, and z is the containerâs maximum load factor.\
    \ [26.2.7/15]\nAll Unordered Associative Containers: In case of a merge operation\
    \  (e.g., a.merge(a2)), iterators referring to the transferred elements and all\
    \ iterators referring to a will be invalidated, but iterators to elements remaining\
    \ in a2 will remain valid. (Table 91 â Unordered associative container requirements)\n\
    \nContainer Adaptors\n\nstack: inherited from underlying container  \nqueue: inherited\
    \ from underlying container  \npriority_queue: inherited from underlying container\
    \  \n\n\nErasure\nSequence Containers \n\nvector: The functions erase and pop_back\
    \ invalidate iterators and references at or after the point of the erase. [26.3.11.5/3]\n\
    deque: An erase operation that erases the last element of a deque invalidates\
    \ only the past-the-end iterator and all iterators and references to the erased\
    \ elements. An erase operation that erases the first element of a deque but not\
    \ the last element invalidates only iterators and references to the erased elements.\
    \ An erase operation that erases neither the first element nor the last element\
    \ of a deque invalidates the past-the-end iterator and all iterators and references\
    \ to all the elements of the deque.\n[ Note: pop_front and pop_back are erase\
    \ operations. âend note ] [26.3.8.4/4]\nlist: Invalidates only the iterators and\
    \ references to the erased elements. [26.3.10.4/3]. This applies to erase, pop_front,\
    \ pop_back, clear functions.\nremove and remove_if member functions: Erases all\
    \ the elements in the list referred by a list iterator i for which the following\
    \ conditions hold: *i == value, pred(*i) != false. Invalidates only the iterators\
    \ and references to the erased elements [26.3.10.5/15].\nunique member function\
    \ - Erases all but the first element from every consecutive group of equal elements\
    \ referred to by the iterator i in the range [first + 1, last) for which *i ==\
    \ *(i-1) (for the version of unique with no arguments) or pred(*i, *(i - 1)) (for\
    \ the version of unique with a predicate argument) holds. Invalidates only the\
    \ iterators and references to the erased elements. [26.3.10.5/19]\nforward_list:\
    \ erase_after shall invalidate only iterators and references to the erased elements.\
    \ [26.3.9.5/1].\nremove and remove_if member functions - Erases all the elements\
    \ in the list referred by a list iterator i for which the following conditions\
    \ hold: *i == value (for remove()), pred(*i) is true (for remove_if()). Invalidates\
    \ only the iterators and references to the erased elements. [26.3.9.6/12].\nunique\
    \ member function - Erases all but the first element from every consecutive group\
    \ of equal elements referred to by the iterator i in the range [first + 1, last)\
    \ for which *i == *(i-1) (for the version with no arguments) or pred(*i, *(i -\
    \ 1)) (for the version with a predicate argument) holds. Invalidates only the\
    \ iterators and references to the erased elements.  [26.3.9.6/16]\nAll Sequence\
    \ Containers: clear invalidates all references, pointers, and iterators referring\
    \ to the elements of a and may invalidate the past-the-end iterator (Table 87\
    \ â Sequence container requirements). But for forward_list, clear does not invalidate\
    \ past-the-end iterators. [26.3.9.5/32]\nAll Sequence Containers: assign invalidates\
    \ all references, pointers and\niterators referring to the elements of the container.\
    \ For vector and deque, also invalidates the past-the-end iterator. (Table 87\
    \ â Sequence container requirements)\n\nAssociative Containers\n\nAll Associative\
    \ Containers: The erase members shall invalidate only iterators and references\
    \ to the erased elements [26.2.6/9]\nAll Associative Containers: The extract members\
    \ invalidate only iterators to the removed element; pointers and references to\
    \ the removed element remain valid [26.2.6/10]\n\nContainer Adaptors\n\nstack:\
    \ inherited from underlying container  \nqueue: inherited from underlying container\
    \  \npriority_queue: inherited from underlying container  \n\n\nGeneral container\
    \ requirements relating to iterator invalidation: \n\nUnless otherwise specified\
    \ (either explicitly or by defining a function in terms of other functions), invoking\
    \ a container member function or passing a container as an argument to a library\
    \ function shall not invalidate iterators to, or change the values of, objects\
    \ within that container. [26.2.1/12]   \nno swap() function invalidates any references,\
    \ pointers, or iterators referring to the elements of the containers being swapped.\
    \ [ Note: The end() iterator does not refer to any element, so it may be invalidated.\
    \ âend note ] [26.2.1/(11.6)] \n\nAs examples of the above requirements: \n\n\
    transform algorithm: The op and binary_op functions shall not invalidate iterators\
    \ or subranges, or modify elements in the ranges [28.6.4/1]\naccumulate algorithm:\
    \ In the range [first, last], binary_op shall neither modify elements nor invalidate\
    \ iterators or subranges [29.8.2/1]\nreduce algorithm: binary_op shall neither\
    \ invalidate iterators or subranges, nor modify elements in the range [first,\
    \ last]. [29.8.3/5]\n\nand so on...\n"
- - Iterator invalidation rules
  - "\nC++03 (Source: Iterator Invalidation Rules (C++03))\n\nInsertion\nSequence\
    \ containers\n\nvector: all iterators and references before the point of insertion\
    \ are unaffected, unless the new container size is greater than the previous capacity\
    \ (in which case all iterators and references are invalidated) [23.2.4.3/1]\n\
    deque: all iterators and references are invalidated, unless the inserted member\
    \ is at an end (front or back) of the deque (in which case all iterators are invalidated,\
    \ but references to elements are unaffected) [23.2.1.3/1]\nlist: all iterators\
    \ and references unaffected [23.2.2.3/1]\n\nAssociative containers\n\n[multi]{set,map}:\
    \ all iterators and references unaffected [23.1.2/8]\n\nContainer adaptors\n\n\
    stack: inherited from underlying container\nqueue: inherited from underlying container\n\
    priority_queue: inherited from underlying container\n\n\nErasure\nSequence containers\n\
    \nvector: every iterator and reference after the point of erase is invalidated\
    \ [23.2.4.3/3]\ndeque: all iterators and references are invalidated, unless the\
    \ erased members are at an end (front or back) of the deque (in which case only\
    \ iterators and references to the erased members are invalidated) [23.2.1.3/4]\n\
    list: only the iterators and references to the erased element is invalidated [23.2.2.3/3]\n\
    \nAssociative containers\n\n[multi]{set,map}: only iterators and references to\
    \ the erased elements are invalidated [23.1.2/8]\n\nContainer adaptors\n\nstack:\
    \ inherited from underlying container\nqueue: inherited from underlying container\n\
    priority_queue: inherited from underlying container\n\n\nResizing\n\nvector: as\
    \ per insert/erase [23.2.4.2/6]\ndeque: as per insert/erase [23.2.1.2/1]\nlist:\
    \ as per insert/erase [23.2.2.2/1]\n\n\nNote 1\n\nUnless otherwise specified (either\n\
    \  explicitly or by defining a function\n  in terms of other functions), invoking\n\
    \  a container member function or passing\n  a container as an argument to a\n\
    \  library function shall not invalidate\n  iterators to, or change the values\
    \ of,\n  objects within that container.\n  [23.1/11]\n\nNote 2\nIt's not clear\
    \ in C++2003 whether \"end\" iterators are subject to the above rules; you should\
    \ assume, anyway, that they are (as this is the case in practice).\nNote 3\nThe\
    \ rules for invalidation of pointers are the sames as the rules for invalidation\
    \ of references.\n"
- - Iterator invalidation rules
  - "\nC++11 (Source: Iterator Invalidation Rules (C++0x))\n\nInsertion\nSequence\
    \ containers\n\nvector: all iterators and references before the point of insertion\
    \ are unaffected, unless the new container size is greater than the previous capacity\
    \ (in which case all iterators and references are invalidated) [23.3.6.5/1]\n\
    deque: all iterators and references are invalidated, unless the inserted member\
    \ is at an end (front or back) of the deque (in which case all iterators are invalidated,\
    \ but references to elements are unaffected) [23.3.3.4/1]\nlist: all iterators\
    \ and references unaffected [23.3.5.4/1]\nforward_list: all iterators and references\
    \ unaffected (applies to insert_after) [23.3.4.5/1]\narray: (n/a)\n\nAssociative\
    \ containers\n\n[multi]{set,map}: all iterators and references unaffected [23.2.4/9]\n\
    \nUnsorted associative containers\n\nunordered_[multi]{set,map}: all iterators\
    \ invalidated when rehashing occurs, but references unaffected [23.2.5/8]. Rehashing\
    \ does not occur if the insertion does not cause the container's size to exceed\
    \ z * B where z is the maximum load factor and B the current number of buckets.\
    \ [23.2.5/14]\n\nContainer adaptors\n\nstack: inherited from underlying container\n\
    queue: inherited from underlying container\npriority_queue: inherited from underlying\
    \ container\n\n\nErasure\nSequence containers\n\nvector: every iterator and reference\
    \ at or after the point of erase is invalidated [23.3.6.5/3]\ndeque: erasing the\
    \ last element invalidates only iterators and references to the erased elements\
    \ and the past-the-end iterator; erasing the first element invalidates only iterators\
    \ and references to the erased elements; erasing any other elements invalidates\
    \ all iterators and references (including the past-the-end iterator) [23.3.3.4/4]\n\
    list: only the iterators and references to the erased element is invalidated [23.3.5.4/3]\n\
    forward_list: only the iterators and references to the erased element is invalidated\
    \ (applies to erase_after) [23.3.4.5/1]\narray: (n/a)\n\nAssociative containers\n\
    \n[multi]{set,map}: only iterators and references to the erased elements are invalidated\
    \ [23.2.4/9]\n\nUnordered associative containers\n\nunordered_[multi]{set,map}:\
    \ only iterators and references to the erased elements are invalidated [23.2.5/13]\n\
    \nContainer adaptors\n\nstack: inherited from underlying container\nqueue: inherited\
    \ from underlying container\npriority_queue: inherited from underlying container\n\
    \n\nResizing\n\nvector: as per insert/erase [23.3.6.5/12]\ndeque: as per insert/erase\
    \ [23.3.3.3/3]\nlist: as per insert/erase [23.3.5.3/1]\nforward_list: as per insert/erase\
    \ [23.3.4.5/25]\narray: (n/a)\n\n\nNote 1\n\nUnless otherwise specified (either\n\
    \  explicitly or by defining a function\n  in terms of other functions), invoking\n\
    \  a container member function or passing\n  a container as an argument to a\n\
    \  library function shall not invalidate\n  iterators to, or change the values\
    \ of,\n  objects within that container.\n  [23.2.1/11]\n\nNote 2\n\nno swap()\
    \ function invalidates any\n  references, pointers, or iterators\n  referring\
    \ to the elements of the\n  containers being swapped. [ Note: The\n  end() iterator\
    \ does not refer to any\n  element, so it may be invalidated.\n  âend note ] [23.2.1/10]\n\
    \nNote 3\nOther than the above caveat regarding swap(), it's not clear whether\
    \ \"end\" iterators are subject to the above listed per-container rules; you should\
    \ assume, anyway, that they are.\nNote 4\nvector and all unordered associative\
    \ containers support reserve(n) which guarantees that no automatic resizing will\
    \ occur at least until the size of the container grows to n. Caution should be\
    \ taken with unordered associative containers because a future proposal will allow\
    \ the specification of a minimum load factor, which would allow rehashing to occur\
    \ on insert after enough erase operations reduce the container size below the\
    \ minimum; the guarantee should be considered potentially void after an erase.\n"
- - What is the most effective way for float and double comparison?
  - "\nBe extremely careful using any of the other suggestions. It all depends on\
    \ context. \nI have spent a long time tracing a bugs in a system that presumed\
    \ a==b if |a-b|<epsilon. The underlying problems were:\n\nThe implicit presumption\
    \ in an algorithm that if a==b and b==c then a==c. \nUsing the same epsilon for\
    \ lines measured in inches and lines measured in mils (.001 inch). That is a==b\
    \ but 1000a!=1000b. (This is why AlmostEqual2sComplement asks for the epsilon\
    \ or max ULPS).\nThe use of the same epsilon for both the cosine of angles and\
    \ the length of lines!\nUsing such a compare function to sort items in a collection.\
    \ (In this case using the builtin C++ operator == for doubles produced correct\
    \ results.)\n\nLike I said: it all depends on context and the expected size of\
    \ a and b.\nBTW, std::numeric_limits<double>::epsilon() is the \"machine epsilon\"\
    . It is the difference between 1.0 and the next value representable by a double.\
    \ I guess that it could be used in the compare function but only if the expected\
    \ values are less than 1. (This is in response to @cdv's answer...)\nAlso, if\
    \ you basically have int arithmetic in doubles (here we use doubles to hold int\
    \ values in certain cases) your arithmetic will be correct. For example 4.0/2.0\
    \ will be the same as 1.0+1.0. This is as long as you do not do things that result\
    \ in fractions (4.0/3.0) or do not go outside of the size of an int.\n"
- - What is the most effective way for float and double comparison?
  - "\nThe comparison with an epsilon value is what most people do (even in game programming).\n\
    You should change your implementation a little though:\nbool AreSame(double a,\
    \ double b)\n{\n    return fabs(a - b) < EPSILON;\n}\n\n\nEdit: Christer has added\
    \ a stack of great info on this topic on a recent blog post. Enjoy.\n"
- - What is the most effective way for float and double comparison?
  - "\nI found that the Google C++ Testing Framework contains a nice cross-platform\
    \ template-based implementation of AlmostEqual2sComplement which works on both\
    \ doubles and floats. Given that it is released under the BSD license, using it\
    \ in your own code should be no problem, as long as you retain the license. I\
    \ extracted the below code from http://code.google.com/p/googletest/source/browse/trunk/include/gtest/internal/gtest-internal.h\
    \ https://github.com/google/googletest/blob/master/googletest/include/gtest/internal/gtest-internal.h\
    \ and added the license on top.\nBe sure to #define GTEST_OS_WINDOWS to some value\
    \ (or to change the code where it's used to something that fits your codebase\
    \ - it's BSD licensed after all).\nUsage example:\ndouble left  = // something\n\
    double right = // something\nconst FloatingPoint<double> lhs(left), rhs(right);\n\
    \nif (lhs.AlmostEquals(rhs)) {\n  //they're equal!\n}\n\nHere's the code:\n//\
    \ Copyright 2005, Google Inc.\n// All rights reserved.\n//\n// Redistribution\
    \ and use in source and binary forms, with or without\n// modification, are permitted\
    \ provided that the following conditions are\n// met:\n//\n//     * Redistributions\
    \ of source code must retain the above copyright\n// notice, this list of conditions\
    \ and the following disclaimer.\n//     * Redistributions in binary form must\
    \ reproduce the above\n// copyright notice, this list of conditions and the following\
    \ disclaimer\n// in the documentation and/or other materials provided with the\n\
    // distribution.\n//     * Neither the name of Google Inc. nor the names of its\n\
    // contributors may be used to endorse or promote products derived from\n// this\
    \ software without specific prior written permission.\n//\n// THIS SOFTWARE IS\
    \ PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS\
    \ OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES\
    \ OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED.\
    \ IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY\
    \ DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\
    \ (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\
    \ LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED\
    \ AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR\
    \ TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n\
    // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n\
    // Authors: wan@google.com (Zhanyong Wan), eefacm@gmail.com (Sean Mcafee)\n//\n\
    // The Google C++ Testing Framework (Google Test)\n\n\n// This template class\
    \ serves as a compile-time function from size to\n// type.  It maps a size in\
    \ bytes to a primitive type with that\n// size. e.g.\n//\n//   TypeWithSize<4>::UInt\n\
    //\n// is typedef-ed to be unsigned int (unsigned integer made up of 4\n// bytes).\n\
    //\n// Such functionality should belong to STL, but I cannot find it\n// there.\n\
    //\n// Google Test uses this class in the implementation of floating-point\n//\
    \ comparison.\n//\n// For now it only handles UInt (unsigned int) as that's all\
    \ Google Test\n// needs.  Other types can be easily added in the future if need\n\
    // arises.\ntemplate <size_t size>\nclass TypeWithSize {\n public:\n  // This\
    \ prevents the user from using TypeWithSize<N> with incorrect\n  // values of\
    \ N.\n  typedef void UInt;\n};\n\n// The specialization for size 4.\ntemplate\
    \ <>\nclass TypeWithSize<4> {\n public:\n  // unsigned int has size 4 in both\
    \ gcc and MSVC.\n  //\n  // As base/basictypes.h doesn't compile on Windows, we\
    \ cannot use\n  // uint32, uint64, and etc here.\n  typedef int Int;\n  typedef\
    \ unsigned int UInt;\n};\n\n// The specialization for size 8.\ntemplate <>\nclass\
    \ TypeWithSize<8> {\n public:\n#if GTEST_OS_WINDOWS\n  typedef __int64 Int;\n\
    \  typedef unsigned __int64 UInt;\n#else\n  typedef long long Int;  // NOLINT\n\
    \  typedef unsigned long long UInt;  // NOLINT\n#endif  // GTEST_OS_WINDOWS\n\
    };\n\n\n// This template class represents an IEEE floating-point number\n// (either\
    \ single-precision or double-precision, depending on the\n// template parameters).\n\
    //\n// The purpose of this class is to do more sophisticated number\n// comparison.\
    \  (Due to round-off error, etc, it's very unlikely that\n// two floating-points\
    \ will be equal exactly.  Hence a naive\n// comparison by the == operation often\
    \ doesn't work.)\n//\n// Format of IEEE floating-point:\n//\n//   The most-significant\
    \ bit being the leftmost, an IEEE\n//   floating-point looks like\n//\n//    \
    \ sign_bit exponent_bits fraction_bits\n//\n//   Here, sign_bit is a single bit\
    \ that designates the sign of the\n//   number.\n//\n//   For float, there are\
    \ 8 exponent bits and 23 fraction bits.\n//\n//   For double, there are 11 exponent\
    \ bits and 52 fraction bits.\n//\n//   More details can be found at\n//   http://en.wikipedia.org/wiki/IEEE_floating-point_standard.\n\
    //\n// Template parameter:\n//\n//   RawType: the raw floating-point type (either\
    \ float or double)\ntemplate <typename RawType>\nclass FloatingPoint {\n public:\n\
    \  // Defines the unsigned integer type that has the same size as the\n  // floating\
    \ point number.\n  typedef typename TypeWithSize<sizeof(RawType)>::UInt Bits;\n\
    \n  // Constants.\n\n  // # of bits in a number.\n  static const size_t kBitCount\
    \ = 8*sizeof(RawType);\n\n  // # of fraction bits in a number.\n  static const\
    \ size_t kFractionBitCount =\n    std::numeric_limits<RawType>::digits - 1;\n\n\
    \  // # of exponent bits in a number.\n  static const size_t kExponentBitCount\
    \ = kBitCount - 1 - kFractionBitCount;\n\n  // The mask for the sign bit.\n  static\
    \ const Bits kSignBitMask = static_cast<Bits>(1) << (kBitCount - 1);\n\n  // The\
    \ mask for the fraction bits.\n  static const Bits kFractionBitMask =\n    ~static_cast<Bits>(0)\
    \ >> (kExponentBitCount + 1);\n\n  // The mask for the exponent bits.\n  static\
    \ const Bits kExponentBitMask = ~(kSignBitMask | kFractionBitMask);\n\n  // How\
    \ many ULP's (Units in the Last Place) we want to tolerate when\n  // comparing\
    \ two numbers.  The larger the value, the more error we\n  // allow.  A 0 value\
    \ means that two numbers must be exactly the same\n  // to be considered equal.\n\
    \  //\n  // The maximum error of a single floating-point operation is 0.5\n  //\
    \ units in the last place.  On Intel CPU's, all floating-point\n  // calculations\
    \ are done with 80-bit precision, while double has 64\n  // bits.  Therefore,\
    \ 4 should be enough for ordinary use.\n  //\n  // See the following article for\
    \ more details on ULP:\n  // http://www.cygnus-software.com/papers/comparingfloats/comparingfloats.htm.\n\
    \  static const size_t kMaxUlps = 4;\n\n  // Constructs a FloatingPoint from a\
    \ raw floating-point number.\n  //\n  // On an Intel CPU, passing a non-normalized\
    \ NAN (Not a Number)\n  // around may change its bits, although the new value\
    \ is guaranteed\n  // to be also a NAN.  Therefore, don't expect this constructor\
    \ to\n  // preserve the bits in x when x is a NAN.\n  explicit FloatingPoint(const\
    \ RawType& x) { u_.value_ = x; }\n\n  // Static methods\n\n  // Reinterprets a\
    \ bit pattern as a floating-point number.\n  //\n  // This function is needed\
    \ to test the AlmostEquals() method.\n  static RawType ReinterpretBits(const Bits\
    \ bits) {\n    FloatingPoint fp(0);\n    fp.u_.bits_ = bits;\n    return fp.u_.value_;\n\
    \  }\n\n  // Returns the floating-point number that represent positive infinity.\n\
    \  static RawType Infinity() {\n    return ReinterpretBits(kExponentBitMask);\n\
    \  }\n\n  // Non-static methods\n\n  // Returns the bits that represents this\
    \ number.\n  const Bits &bits() const { return u_.bits_; }\n\n  // Returns the\
    \ exponent bits of this number.\n  Bits exponent_bits() const { return kExponentBitMask\
    \ & u_.bits_; }\n\n  // Returns the fraction bits of this number.\n  Bits fraction_bits()\
    \ const { return kFractionBitMask & u_.bits_; }\n\n  // Returns the sign bit of\
    \ this number.\n  Bits sign_bit() const { return kSignBitMask & u_.bits_; }\n\n\
    \  // Returns true iff this is NAN (not a number).\n  bool is_nan() const {\n\
    \    // It's a NAN if the exponent bits are all ones and the fraction\n    //\
    \ bits are not entirely zeros.\n    return (exponent_bits() == kExponentBitMask)\
    \ && (fraction_bits() != 0);\n  }\n\n  // Returns true iff this number is at most\
    \ kMaxUlps ULP's away from\n  // rhs.  In particular, this function:\n  //\n \
    \ //   - returns false if either number is (or both are) NAN.\n  //   - treats\
    \ really large numbers as almost equal to infinity.\n  //   - thinks +0.0 and\
    \ -0.0 are 0 DLP's apart.\n  bool AlmostEquals(const FloatingPoint& rhs) const\
    \ {\n    // The IEEE standard says that any comparison operation involving\n \
    \   // a NAN must return false.\n    if (is_nan() || rhs.is_nan()) return false;\n\
    \n    return DistanceBetweenSignAndMagnitudeNumbers(u_.bits_, rhs.u_.bits_)\n\
    \        <= kMaxUlps;\n  }\n\n private:\n  // The data type used to store the\
    \ actual floating-point number.\n  union FloatingPointUnion {\n    RawType value_;\
    \  // The raw floating-point number.\n    Bits bits_;      // The bits that represent\
    \ the number.\n  };\n\n  // Converts an integer from the sign-and-magnitude representation\
    \ to\n  // the biased representation.  More precisely, let N be 2 to the\n  //\
    \ power of (kBitCount - 1), an integer x is represented by the\n  // unsigned\
    \ number x + N.\n  //\n  // For instance,\n  //\n  //   -N + 1 (the most negative\
    \ number representable using\n  //          sign-and-magnitude) is represented\
    \ by 1;\n  //   0      is represented by N; and\n  //   N - 1  (the biggest number\
    \ representable using\n  //          sign-and-magnitude) is represented by 2N\
    \ - 1.\n  //\n  // Read http://en.wikipedia.org/wiki/Signed_number_representations\n\
    \  // for more details on signed number representations.\n  static Bits SignAndMagnitudeToBiased(const\
    \ Bits &sam) {\n    if (kSignBitMask & sam) {\n      // sam represents a negative\
    \ number.\n      return ~sam + 1;\n    } else {\n      // sam represents a positive\
    \ number.\n      return kSignBitMask | sam;\n    }\n  }\n\n  // Given two numbers\
    \ in the sign-and-magnitude representation,\n  // returns the distance between\
    \ them as an unsigned number.\n  static Bits DistanceBetweenSignAndMagnitudeNumbers(const\
    \ Bits &sam1,\n                                                     const Bits\
    \ &sam2) {\n    const Bits biased1 = SignAndMagnitudeToBiased(sam1);\n    const\
    \ Bits biased2 = SignAndMagnitudeToBiased(sam2);\n    return (biased1 >= biased2)\
    \ ? (biased1 - biased2) : (biased2 - biased1);\n  }\n\n  FloatingPointUnion u_;\n\
    };\n\nEDIT: This post is 4 years old. It's probably still valid, and the code\
    \ is nice, but some people found improvements. Best go get the latest version\
    \ of AlmostEquals right from the Google Test source code, and not the one I pasted\
    \ up here.\n"
- - Undefined, unspecified and implementation-defined behavior
  - "\nUndefined behavior is one of those aspects of the C and C++ language that can\
    \ be surprising to programmers coming from other languages (other languages try\
    \ to hide it better). Basically,  it is possible to write C++ programs that do\
    \ not behave in a predictable way, even though many C++ compilers will not report\
    \ any errors in the program!\nLet's look at a classic example:\n#include <iostream>\n\
    \nint main()\n{\n    char* p = \"hello!\\n\";   // yes I know, deprecated conversion\n\
    \    p[0] = 'y';\n    p[5] = 'w';\n    std::cout << p;\n}\n\nThe variable p points\
    \ to the string literal \"hello!\\n\", and the two assignments below try to modify\
    \ that string literal. What does this program do? According to section 2.14.5\
    \ paragraph 11 of the C++ standard, it invokes undefined behavior:\n\nThe effect\
    \ of attempting to modify a string literal is undefined.\n\nI can hear people\
    \ screaming \"But wait, I can compile this no problem and get the output yellow\"\
    \ or \"What do you mean undefined, string literals are stored in read-only memory,\
    \ so the first assignment attempt results in a core dump\". This is exactly the\
    \ problem with undefined behavior. Basically, the standard allows anything to\
    \ happen once you invoke undefined behavior (even nasal demons). If there is a\
    \ \"correct\" behavior according to your mental model of the language, that model\
    \ is simply wrong; The C++ standard has the only vote, period.\nOther examples\
    \ of undefined behavior include accessing an array beyond its bounds, dereferencing\
    \ the null pointer, accessing objects after their lifetime ended or writing allegedly\
    \ clever expressions like i++ + ++i.\nSection 1.9 of the C++ standard also mentions\
    \ undefined behavior's two less dangerous brothers, unspecified behavior and implementation-defined\
    \ behavior:\n\nThe semantic descriptions in this International Standard define\
    \ a parameterized nondeterministic abstract machine.\nCertain aspects and operations\
    \ of the abstract machine are described in this International Standard as implementation-defined\
    \ (for example, sizeof(int)). These constitute the parameters of the abstract\
    \ machine. Each implementation shall include documentation describing its characteristics\
    \ and behavior in these respects.\nCertain other aspects and operations of the\
    \ abstract machine are described in this International Standard as unspecified\
    \ (for example, order of evaluation of arguments to a function). Where possible,\
    \ this International Standard defines a set of allowable behaviors. These define\
    \ the nondeterministic aspects of the abstract machine.\nCertain other operations\
    \ are described in this International Standard as undefined (for example, the\
    \  effect of dereferencing the null pointer). [ Note: this International Standard\
    \ imposes no requirements on the behavior of programs that contain undefined behavior.\
    \ âend note ]\n\nSpecifically, section 1.3.24 states:\n\nPermissible undefined\
    \ behavior ranges from ignoring the situation completely with unpredictable results,\
    \ to behaving during translation or program execution in a documented manner characteristic\
    \ of the environment (with or without the issuance of a diagnostic message), to\
    \ terminating a translation or execution (with the issuance of a diagnostic message).\n\
    \nWhat can you do to avoid running into undefined behavior? Basically, you have\
    \ to read good C++ books by authors who know what they're talking about. Screw\
    \ internet tutorials. Screw bullschildt.\n"
- - Undefined, unspecified and implementation-defined behavior
  - "\nWell, this is basically a straight copy-paste from the standard\n\n3.4.1 1\
    \ implementation-defined behavior unspecified behavior where\n  each implementation\
    \ documents how the\n  choice is made \n2 EXAMPLE An example of\n  implementation-defined\
    \ behavior is the\n  propagation of the high-order bit when\n  a signed integer\
    \ is shifted right.\n3.4.3 1 undefined behavior behavior, upon use of a nonportable\
    \ or erroneous\n  program construct or of erroneous\n  data, for which this International\n\
    \  Standard imposes no requirements \n2\n  NOTE Possible undefined behavior\n\
    \  ranges from ignoring the situation\n  completely with unpredictable results,\n\
    \  to behaving during translation or\n  program execution in a documented\n  manner\
    \ characteristic of the\n  environment (with or without the\n  issuance of a diagnostic\
    \ message), to\n  terminating a translation or execution\n  (with the issuance\
    \ of a diagnostic\n  message).\n3 EXAMPLE An example of\n  undefined behavior\
    \ is the behavior on\n  integer overflow.\n3.4.4 1 unspecified behavior use of\
    \ an unspecified value, or other behavior\n  where this International Standard\n\
    \  provides two or more possibilities and\n  imposes no further requirements on\n\
    \  which is chosen in any instance \n2\n  EXAMPLE An example of unspecified\n\
    \  behavior is the order in which the\n  arguments to a function are evaluated.\n\
    \n"
- - Undefined, unspecified and implementation-defined behavior
  - "\nMaybe easy wording could be easier for understanding than the rigorous definition\
    \ of the standards.\nimplementation-defined behavior\nThe language says that we\
    \ have data-types. The compiler vendors specify what sizes shall they use, and\
    \ provide a documentation of what they did.\nundefined behavior\nYou are doing\
    \ something wrong. For example, you have a very large value in an int that doesn't\
    \ fit in char. How do you put that value in char? actually there is no way! Anything\
    \ could happen, but the most sensible thing would be to take the first byte of\
    \ that int and put it in char. It is just wrong to do that to assign the first\
    \ byte, but thats what happens under the hood.\nunspecified behavior\nWhich function\
    \ of these two is executed first?\nvoid fun(int n, int m);\n\nint fun1()\n{\n\
    \  cout << \"fun1\";\n  return 1;\n}\nint fun2()\n{\n  cout << \"fun2\";\n  return\
    \ 2;\n}\n...\nfun(fun1(), fun2()); // which one is executed first?\n\nThe language\
    \ doesn't specify the evaluation, left to right or right to left! So an unspecified\
    \ behavior may or mayn't result in an undefined behavior, but certainly your program\
    \ should not produce an unspecified behavior.\n\n@eSKay I think your question\
    \ is worth editing the answer to clarify more :)\n\nfor fun(fun1(), fun2()); isn't\
    \ the\n  behaviour \"implementation defined\"?\n  The compiler has to choose one\
    \ or the\n  other course, after all?\n\nThe difference between implementation-defined\
    \ and unspecified, is that the compiler is supposed to pick a behavior in the\
    \ first case but it doesn't have to in the second case. For example, an implementation\
    \ must have one and only one definition of sizeof(int). So, it can't say that\
    \ sizeof(int) is 4 for some portion of the program and 8 for others. Unlike unspecified\
    \ behavior, where the compiler can say OK I am gonna evaluate these arguments\
    \ left-to-right and the next function's arguments are evaluated right-to-left.\
    \ It can happen in the same program, that's why it is called unspecified. In fact,\
    \ C++ could have been made easier if some of the unspecified behaviors were specified.\
    \ Take a look here at Dr. Stroustrup's answer for that:\n\nIt is claimed that\
    \ the difference\n  between what can be produced giving\n  the compiler this freedom\
    \ and\n  requiring \"ordinary left-to-right\n  evaluation\" can be significant.\
    \ I'm\n  unconvinced, but with innumerable\n  compilers \"out there\" taking advantage\n\
    \  of the freedom and some people\n  passionately defending that freedom, a\n\
    \  change would be difficult and could\n  take decades to penetrate to the\n \
    \ distant corners of the C and C++\n  worlds. I am disappointed that not all\n\
    \  compilers warn against code such as\n  ++i+i++. Similarly, the order of evaluation\
    \ of arguments is\n  unspecified.\nIMO far too many \"things\" are left\n  undefined,\
    \ unspecified,\n  implementation-defined, etc. However,\n  that's easy to say\
    \ and even to give\n  examples of, but hard to fix. It\n  should also be noted\
    \ that it is not\n  all that difficult to avoid most of\n  the problems and produce\
    \ portable\n  code.\n\n"
- - When should I write the keyword 'inline' for a function/method?
  - "\nOh man, one of my pet peeves.\ninline is more like static or extern than a\
    \ directive telling the compiler to inline your functions.  extern, static, inline\
    \ are linkage directives, used almost exclusively by the linker, not the compiler.\n\
    It is said that inline hints to the compiler that you think the function should\
    \ be inlined.  That may have been true in 1998, but a decade later the compiler\
    \ needs no such hints.  Not to mention humans are usually wrong when it comes\
    \ to optimizing code, so most compilers flat out ignore the 'hint'.\n\nstatic\
    \ - the variable/function name cannot be used in other translation units.  Linker\
    \ needs to make sure it doesn't accidentally use a statically defined variable/function\
    \ from another translation unit.\nextern - use this variable/function name in\
    \ this translation unit but don't complain if it isn't defined.  The linker will\
    \ sort it out and make sure all the code that tried to use some extern symbol\
    \ has its address.\ninline - this function will be defined in multiple translation\
    \ units, don't worry about it.  The linker needs to make sure all translation\
    \ units use a single instance of the variable/function.\n\nNote: Generally, declaring\
    \ templates inline is pointless, as they have the linkage semantics of inline\
    \ already. However, explicit specialization and instantiation of templates require\
    \ inline to be used.\n\nSpecific answers to your questions:\n\n\nWhen should I\
    \ write the keyword 'inline' for a function/method in C++?\n\nOnly when you want\
    \ the function to be defined in a header.  More exactly only when the function's\
    \ definition can show up in multiple translation units.  It's a good idea to define\
    \ small (as in one liner) functions in the header file as it gives the compiler\
    \ more information to work with while optimizing your code.  It also increases\
    \ compilation time.\n\nWhen should I not write the keyword 'inline' for a function/method\
    \ in C++?\n\nDon't add inline just because you think your code will run faster\
    \ if the compiler inlines it.\n\nWhen will the compiler not know when to make\
    \ a function/method 'inline'?\n\nGenerally, the compiler will be able to do this\
    \ better than you.  However, the compiler doesn't have the option to inline code\
    \ if it doesn't have the function definition.  In maximally optimized code usually\
    \ all private methods are inlined whether you ask for it or not.\nAs an aside\
    \ to prevent inlining in GCC, use __attribute__(( noinline )), and in Visual Studio,\
    \ use __declspec(noinline).\n\nDoes it matter if an application is multithreaded\
    \ when one writes 'inline' for a function/method?\n\nMultithreading doesn't affect\
    \ inlining in any way.\n\n"
- - When should I write the keyword 'inline' for a function/method?
  - "\nI'd like to contribute to all of the great answers in this thread with a convincing\
    \ example to disperse any remaining misunderstanding.\nGiven two source files,\
    \ such as:\n\ninline111.cpp:\n#include <iostream>\n\nvoid bar();\n\ninline int\
    \ fun() {\n  return 111;\n}\n\nint main() {\n  std::cout << \"inline111: fun()\
    \ = \" << fun() << \", &fun = \" << (void*) &fun;\n  bar();\n}\n\ninline222.cpp:\n\
    #include <iostream>\n\ninline int fun() {\n  return 222;\n}\n\nvoid bar() {\n\
    \  std::cout << \"inline222: fun() = \" << fun() << \", &fun = \" << (void*) &fun;\n\
    }\n\n\n\n\nCase A:\nCompile:\ng++ -std=c++11 inline111.cpp inline222.cpp\n\nOutput:\n\
    inline111: fun() = 111, &fun = 0x4029a0\ninline222: fun() = 111, &fun = 0x4029a0\n\
    \nDiscussion:\n\nEven thou you ought to have identical definitions of your inline\n\
    functions, C++ compiler does not flag it if that is not the case (actually, due\
    \ to separate compilation it has no ways to check it). It is your own duty to\
    \ ensure this!\nLinker does not complain about One Definition Rule, as fun() is\
    \ declared as inline. However, because inline111.cpp is the first translation\
    \ unit (which actually calls fun()) processed by compiler, the compiler instantiates\
    \ fun() upon its first call-encounter in inline111.cpp. If compiler decides not\
    \ to expand fun() upon its call from anywhere else in your program (e.g. from\
    \ inline222.cpp), the call to fun() will always be linked to its instance produced\
    \ from inline111.cpp (the call to fun() inside inline222.cpp may also produce\
    \ an instance in that translation unit, but it will remain unlinked). Indeed,\
    \ that is evident from the identical &fun = 0x4029a0 print-outs.\nFinally, despite\
    \ the inline suggestion to the compiler to actually expand the one-liner fun(),\
    \ it ignores your suggestion completely, which is clear because fun() = 111 in\
    \ both of the lines.\n\n\n\n\nCase B:\nCompile (notice reverse order):\ng++ -std=c++11\
    \ inline222.cpp inline111.cpp\n\nOutput:\ninline111: fun() = 222, &fun = 0x402980\n\
    inline222: fun() = 222, &fun = 0x402980\n\nDiscussion:\n\nThis case asserts what\
    \ have been discussed in Case A.\nNotice an important point, that if you comment\
    \ out the actual call to fun() in inline222.cpp (e.g. comment out cout-statement\
    \ in inline222.cpp completely) then, despite the compilation order of your translation\
    \ units, fun() will be instantiated upon it's first call encounter in inline111.cpp,\
    \ resulting in print-out for Case B as inline111: fun() = 111, &fun = 0x402980.\n\
    \n\n\n\nCase C:\nCompile (notice -O2):\ng++ -std=c++11 -O2 inline222.cpp inline111.cpp\n\
    \nor   \ng++ -std=c++11 -O2 inline111.cpp inline222.cpp\n\nOutput:\ninline111:\
    \ fun() = 111, &fun = 0x402900\ninline222: fun() = 222, &fun = 0x402900\n\nDiscussion:\n\
    \nAs is described here, -O2 optimization encourages compiler to actually expand\
    \ the functions that can be inlined (Notice also that -fno-inline is default without\
    \ optimization options). As is evident from the outprint here, the fun() has actually\
    \ been inline expanded (according to its definition in that particular translation\
    \ unit), resulting in two different fun() print-outs. Despite this, there is still\
    \ only one globally linked instance of fun() (as required by the standard), as\
    \ is evident from identical &fun print-out.\n\n\n"
- - When should I write the keyword 'inline' for a function/method?
  - "\nYou still need to explicitly inline your function when doing template specialization\
    \ (if specialization is in .h file)\n"
- - unsigned int vs. size_t
  - "\nThe size_t type is the unsigned integer type that is the result of the sizeof\
    \ operator (and the offsetof operator), so it is guaranteed to be big enough to\
    \ contain the size of the biggest object your system can handle (e.g., a static\
    \ array of 8Gb).\nThe size_t type may be bigger than, equal to, or smaller than\
    \ an unsigned int, and your compiler might make assumptions about it for optimization.\n\
    You may find more precise information in the C99 standard, section 7.17, a draft\
    \ of which is available on the Internet in pdf format, or in the C11 standard,\
    \ section 7.19, also available as a pdf draft.\n"
- - unsigned int vs. size_t
  - "\nClassic C (the early dialect of C described by Brian Kernighan and Dennis Ritchie\
    \ in The C Programming Language, Prentice-Hall, 1978) didn't provide size_t. The\
    \ C standards committee introduced size_t to eliminate a portability problem\n\
    Explained in detail at embedded.com (with a very good example)\n"
- - unsigned int vs. size_t
  - "\nIn short, size_t is never negative, and it maximizes performance because it's\
    \ typedef'd to be the unsigned integer type that's big enough -- but not too big\
    \ -- to represent the size of the largest possible object on the target platform.\n\
    Sizes should never be negative, and indeed size_t is an unsigned type. Also, because\
    \ size_t is unsigned, you can store numbers that are roughly twice as big as in\
    \ the corresponding signed type, because we can use the sign bit to represent\
    \ magnitude, like all the other bits in the unsigned integer. When we gain one\
    \ more bit, we are multiplying the range of numbers we can represents by a factor\
    \ of about two. \nSo, you ask, why not just use an unsigned int? It may not be\
    \ able to hold big enough numbers. In an implementation where unsigned int is\
    \ 32 bits, the biggest number it can represent is 4294967295. Some processors,\
    \ such as the IP16L32, can copy objects larger than 4294967295 bytes.\nSo, you\
    \ ask, why not use an unsigned long int? It exacts a performance toll on some\
    \ platforms. Standard C requires that a long occupy at least 32 bits. An IP16L32\
    \ platform implements each 32-bit long as a pair of 16-bit words. Almost all 32-bit\
    \ operators on these platforms require two instructions, if not more, because\
    \ they work with the 32 bits in two 16-bit chunks. For example, moving a 32-bit\
    \ long usually requires two machine instructions -- one to move each 16-bit chunk.\n\
    Using size_t avoids this performance toll. According to this fantastic article,\
    \ \"Type size_t is a typedef that's an alias for some unsigned integer type, typically\
    \ unsigned int or unsigned long, but possibly even unsigned long long. Each Standard\
    \ C implementation is supposed to choose the unsigned integer that's big enough--but\
    \ no bigger than needed--to represent the size of the largest possible object\
    \ on the target platform.\"\n"
- - '*.h or *.hpp for your class definitions'
  - "\nHere are a couple of reasons for having different naming of C vs C++ headers:\n\
    \nAutomatic code formatting, you might have different guidelines for formatting\
    \ C and C++ code. If the headers are separated by extension you can set your editor\
    \ to apply the appropriate formatting automatically\nNaming, I've been on projects\
    \ where there were libraries written in C and then wrappers had been implemented\
    \ in C++. Since the headers usually had similar names, i.e. Feature.h vs Feature.hpp,\
    \ they were easy to tell apart.\nInclusion, maybe your project has more appropriate\
    \ versions available written in C++ but you are using the C version (see above\
    \ point). If headers are named after the language they are implemented in you\
    \ can easily spot all the C-headers and check for C++ versions.\n\nRemember, C\
    \ is not C++ and it can be very dangerous to mix and match unless you know what\
    \ you are doing. Naming your sources appropriately helps you tell the languages\
    \ apart.\n"
- - '*.h or *.hpp for your class definitions'
  - "\nI use .hpp because I want the user to differentiate what headers are C++ headers,\
    \ and what headers are C headers.\nThis can be important when your project is\
    \ using both C and C++ modules: Like someone else explained before me, you should\
    \ do it very carefully, and its starts by the \"contract\" you offer through the\
    \ extension\n.hpp : C++ Headers\n(Or .hxx, or .hh, or whatever)\nThis header is\
    \ for C++ only.\nIf you're in a C module, don't even try to include it. You won't\
    \ like it, because no effort is done to make it C-friendly (too much would be\
    \ lost, like function overloading, namespaces, etc. etc.).\n.h : C/C++ compatible\
    \ or pure C Headers\nThis header can be included by both a C source, and a C++\
    \ source, directly or indirectly.\nIt can included directly, being protected by\
    \ the __cplusplus macro:\n\nWhich mean that, from a C++ viewpoint, the C-compatible\
    \ code will be defined as extern \"C\".\nFrom a C viewpoint, all the C code will\
    \ be plainly visible, but the C++ code will be hidden (because it won't compile\
    \ in a C compiler).\n\nFor example:\n#ifndef MY_HEADER_H\n#define MY_HEADER_H\n\
    \n   #ifdef __cplusplus\n      extern \"C\"\n      {\n   #endif\n\n   void myCFunction()\
    \ ;\n\n   #ifdef __cplusplus\n      } // extern \"C\"\n   #endif\n\n#endif //\
    \ MY_HEADER_H\n\nOr it could be included indirectly by the corresponding .hpp\
    \ header enclosing it with the extern \"C\" declaration.\nFor example:\n#ifndef\
    \ MY_HEADER_HPP\n#define MY_HEADER_HPP\n\nextern \"C\"\n{\n#include \"my_header.h\"\
    \n}\n\n#endif // MY_HEADER_HPP\n\nand:\n#ifndef MY_HEADER_H\n#define MY_HEADER_H\n\
    \nvoid myCFunction() ;\n\n#endif // MY_HEADER_H\n\n"
- - '*.h or *.hpp for your class definitions'
  - "\nI always considered the .hpp header to be a sort of portmanteau of .h and .cpp\
    \ files...a header which contains implementation details as well.  \nTypically\
    \ when I've seen (and use) .hpp as an extension, there is no corresponding .cpp\
    \ file.  As others have said, this isn't a hard and fast rule, just how I tend\
    \ to use .hpp files.\n"
- - Debugging with command-line parameters in Visual Studio
  - "\nYes, it's on the Debugging section of the properties page of the project.\n\
    In VS since 2008: right-click the project, choose properties, go to the Debugging\
    \ section -- there is a box for \"Command Arguments\". (Tip: not solution, but\
    \ project)\n"
- - Debugging with command-line parameters in Visual Studio
  - "\nThe Mozilla.org FAQ on debugging Mozilla on Windows is of interest here.\n\
    In short, the Visual Studio debugger can be invoked on a program from the command\
    \ line, allowing one to specify the command line arguments when invoking a command\
    \ line program, directly on the command line.\nThis looks like the following for\
    \ Visual Studio 8 or 9\n\n devenv /debugexe 'program name' 'program arguments'\n\
    \nIt is also possible to have an explorer action to start a program in the Visual\
    \ Studio debugger.\n"
- - Debugging with command-line parameters in Visual Studio
  - "\nEven if you do start the executable outside Visual Studio, you can still use\
    \ the \"Attach\" command to connect Visual Studio to your already-running executable.\
    \ This can be useful e.g. when your application is run as a plug-in within another\
    \ application.\n"
- - Unnamed/anonymous namespaces vs. static functions
  - "\nThe C++ Standard reads in section 7.3.1.1 Unnamed namespaces, paragraph 2:\n\
    \n\nThe use of the static keyword is\n  deprecated when declaring objects in a\n\
    \  namespace scope, the unnamed-namespace\n  provides a superior alternative.\n\
    \  "
- - Unnamed/anonymous namespaces vs. static functions
  - "\nPutting methods in an anonymous namespace prevents you from accidentally violating\
    \ the One Definition Rule, allowing you to never worry about naming your helper\
    \ methods the same as some other method you may link in.\nAnd, as pointed out\
    \ by luke, anonymous namespaces are preferred by the standard over static members.\n"
- - Unnamed/anonymous namespaces vs. static functions
  - "\nThere is one edge case where static has a surprising affect (at least it was\
    \ to me).  The C++03 Standard states in 14.6.4.2/1:\n\nFor a function call that\
    \ depends on a template parameter, if the function name is an unqualified-id but\
    \ not a template-id, the candidate functions are found using the usual lookup\
    \ rules (3.4.1, 3.4.2) except that:\n\nFor the part of the lookup using unqualified\
    \ name lookup (3.4.1), only function declarations with external linkage from the\
    \ template definition context are found.\nFor the part of the lookup using associated\
    \ namespaces (3.4.2), only function declarations with external linkage found in\
    \ either the template definition context or the template instantiation context\
    \ are found.\n\n...\n\nThe below code will call foo(void*) and not foo(S const\
    \ &) as you might expect.\ntemplate <typename T>\nint b1 (T const & t)\n{\n  foo(t);\n\
    }\n\nnamespace NS\n{\n  namespace\n  {\n    struct S\n    {\n    public:\n   \
    \   operator void * () const;\n    };\n\n    void foo (void*);\n    static void\
    \ foo (S const &);   // Not considered 14.6.4.2(b1)\n  }\n\n}\n\nvoid b2()\n{\n\
    \  NS::S s;\n  b1 (s);\n}\n\nIn itself this is probably not that big a deal, but\
    \ it does highlight that for a fully compliant C++ compiler (ie. one with support\
    \ for export) the static keyword will still have functionality that is not available\
    \ in any other way.\n// bar.h\nexport template <typename T>\nint b1 (T const &\
    \ t);\n\n// bar.cc\n#include \"bar.h\"\ntemplate <typename T>\nint b1 (T const\
    \ & t)\n{\n  foo(t);\n}\n\n// foo.cc\n#include \"bar.h\"\nnamespace NS\n{\n  namespace\n\
    \  {\n    struct S\n    {\n    };\n\n    void foo (S const & s);  // Will be found\
    \ by different TU 'bar.cc'\n  }\n}\n\nvoid b2()\n{\n  NS::S s;\n  b1 (s);\n}\n\
    \nThe only way to ensure that the function in our unnamed namespace will not be\
    \ found in templates using ADL is to make it static.\nUpdate for Modern C++\n\
    As of C++ '11, members of an unnamed namespace have internal linkage implicitly\
    \ (3.5/4):\n\nAn unnamed namespace or a namespace declared directly or indirectly\
    \ within an unnamed namespace has internal linkage.\n\nBut at the same time, 14.6.4.2/1\
    \ was updated to remove mention of linkage (this taken from C++ '14):\n\nFor a\
    \ function call where the postfix-expression is a dependent name, the candidate\
    \ functions are found using\n  the usual lookup rules (3.4.1, 3.4.2) except that:\n\
    \nFor the part of the lookup using unqualified name lookup (3.4.1), only function\
    \ declarations from the template definition context are found.\nFor the part of\
    \ the lookup using associated namespaces (3.4.2), only function declarations found\
    \ in either the template definition context or the template instantiation context\
    \ are found.\n\n\nThe result is that this particular difference between static\
    \ and unnamed namespace members no longer exists.\n"
- - How to initialize private static members in C++?
  - "\nThe class declaration should be in the header file (Or in the source file if\
    \ not shared).\nFile: foo.h\nclass foo\n{\n    private:\n        static int i;\n\
    };\n\nBut the initialization should be in source file.\nFile: foo.cpp\nint foo::i\
    \ = 0;\n\nIf the initialization is in the header file then each file that includes\
    \ the header file will have a definition of the static member. Thus during the\
    \ link phase you will get linker errors as the code to initialize the variable\
    \ will be defined in multiple source files.\nNote: Matt Curtis: points out that\
    \ C++ allows the simplification of the above if the static member variable is\
    \ of const int type (e.g. int, bool, char). You can then declare and initialize\
    \ the member variable directly inside the class declaration in the header file:\n\
    class foo\n{\n    private:\n        static int const i = 42;\n};\n\n"
- - How to initialize private static members in C++?
  - "\nFor a variable:\nfoo.h:\nclass foo\n{\nprivate:\n    static int i;\n};\n\n\
    foo.cpp:\nint foo::i = 0;\n\nThis is because there can only be one instance of\
    \ foo::i in your program. It's sort of the equivalent of extern int i in a header\
    \ file and int i in a source file.\nFor a constant you can put the value straight\
    \ in the class declaration:\nclass foo\n{\nprivate:\n    static int i;\n    const\
    \ static int a = 42;\n};\n\n"
- - How to initialize private static members in C++?
  - "\nFor future viewers of this question, I want to point out that you should avoid\
    \ what monkey0506 is suggesting.\nHeader files are for declarations.\nHeader files\
    \ get compiled once for every .cpp file that directly or indirectly #includes\
    \ them, and code outside of any function is run at program initialization, before\
    \ main().\nBy putting: foo::i = VALUE; into the header, foo:i will be assigned\
    \ the value VALUE (whatever that is) for every .cpp file, and these assignments\
    \ will happen in an indeterminate order (determined by the linker) before main()\
    \ is run.\nWhat if we #define VALUE to be a different number in one of our .cpp\
    \ files? It will compile fine and we will have no way of knowing which one wins\
    \ until we run the program.\nNever put executed code into a header for the same\
    \ reason that you never #include a .cpp file.\ninclude guards (which I agree you\
    \ should always use) protect you from something different: the same header being\
    \ indirectly #included multiple times while compiling a single .cpp file\n"
- - What is meant with âconstâ at end of function declaration? [duplicate]
  - "\nA \"const function\", denoted with the keyword const after a function declaration,\
    \ makes it a compiler error for this class function to change a member variable\
    \ of the class.  However, reading of a class variables is okay inside of the function,\
    \ but writing inside of this function will generate a compiler error.  \nAnother\
    \ way of thinking about such \"const function\" is by viewing a class function\
    \ as a normal function taking an implicit this pointer. So a method int Foo::Bar(int\
    \ random_arg) (without the const at the end) results in a function like int Foo_Bar(Foo*\
    \ this, int random_arg), and a call such as Foo f; f.Bar(4) will internally correspond\
    \ to something like Foo f; Foo_Bar(&f, 4). Now adding the const at the end (int\
    \ Foo::Bar(int random_arg) const) can then be understood as a declaration with\
    \ a const this pointer: int Foo_Bar(const Foo* this, int random_arg). Since the\
    \ type of this in such case is const, no modifications of member variables are\
    \ possible.\nIt is possible to loosen the \"const function\" restriction of not\
    \ allowing the function to write to any variable of a class. To allow some of\
    \ the variables to be writable even when the function is marked as a \"const function\"\
    , these class variables are marked with the keyword mutable.  Thus, if a class\
    \ variable is marked as mutable, and a \"const function\" writes to this variable\
    \ then the code will compile cleanly and the variable is possible to change. (C++11)\n\
    As usual when dealing with the const keyword, changing the location of the const\
    \ key word in a C++ statement has entirely different meanings.  The above usage\
    \ of const only applies when adding const to the end of the function declaration\
    \ after the parenthesis. \nconst is a highly overused qualifier in C++: the syntax\
    \ and ordering is often not straightforward in combination with pointers. Some\
    \ readings about const correctness and the const keyword:\nConst correctness\n\
    The C++ 'const' Declaration: Why & How\n"
- - What is meant with âconstâ at end of function declaration? [duplicate]
  - "\nConsider two class-typed variables:\nclass Boo { ... };\n\nBoo b0;       //\
    \ mutable object\nconst Boo b1; // non-mutable object\n\nNow you are able to call\
    \ any member function of Boo on b0, but only const-qualified member functions\
    \ on b1.\n"
- - What is meant with âconstâ at end of function declaration? [duplicate]
  - "\nBar is guaranteed not to change the object it is being invoked on. See the\
    \ section about const correctness in the C++ FAQ, for example.\n"
- - How do I declare a 2d array in C++ using new?
  - "\nA dynamic 2D array is basically an array of pointers to arrays. You can initialize\
    \ it using a loop, like this:\nint** a = new int*[rowCount];\nfor(int i = 0; i\
    \ < rowCount; ++i)\n    a[i] = new int[colCount];\n\nThe above, for colCount=\
    \ 5 and rowCount = 4, would produce the following:\n\n"
- - How do I declare a 2d array in C++ using new?
  - "\nint** ary = new int[sizeY][sizeX]\n\nshould be:\nint **ary = new int*[sizeY];\n\
    for(int i = 0; i < sizeY; ++i) {\n    ary[i] = new int[sizeX];\n}\n\nand then\
    \ clean up would be:\nfor(int i = 0; i < sizeY; ++i) {\n    delete [] ary[i];\n\
    }\ndelete [] ary;\n\nEDIT: as Dietrich Epp pointed out in the comments this is\
    \ not exactly a light weight solution. An alternative approach would be to use\
    \ one large block of memory:\nint *ary = new int[sizeX*sizeY];\n\n// ary[i][j]\
    \ is then rewritten as\nary[i*sizeY+j]\n\n"
- - How do I declare a 2d array in C++ using new?
  - "\nAlthough this popular answer will give you your desired indexing syntax, it\
    \ is doubly inefficient: big and slow both in space and time. There's a better\
    \ way.\nWhy That Answer is Big and Slow\nThe proposed solution is to create a\
    \ dynamic array of pointers, then initializing each pointer to its own, independent\
    \ dynamic array. The advantage of this approach is that it gives you the indexing\
    \ syntax you're used to, so if you want to find the value of the matrix at position\
    \ x,y, you say:\nint val = matrix[ x ][ y ];\n\nThis works because matrix[x] returns\
    \ a pointer to an array, which is then indexed with [y]. Breaking it down:\nint*\
    \ row = matrix[ x ];\nint  val = row[ y ];\n\nConvenient, yes? We like our [ x\
    \ ][ y ] syntax. \nBut the solution has a big disadvantage, which is that it is\
    \ both fat and slow. \nWhy?\nThe reason that it's both fat and slow is actually\
    \ the same. Each \"row\" in the matrix is a separately allocated dynamic array.\
    \ Making a heap allocation is expensive both in time and space. The allocator\
    \ takes time to make the allocation, sometimes running O(n) algorithms to do it.\
    \ And the allocator \"pads\" each of your row arrays with extra bytes for bookkeeping\
    \ and alignment. That extra space costs...well...extra space. The deallocator\
    \ will also take extra time when you go to deallocate the matrix, painstakingly\
    \ free-ing up each individual row allocation. Gets me in a sweat just thinking\
    \ about it.\nThere's another reason it's slow. These separate allocations tend\
    \ to live in discontinuous parts of memory. One row may be at address 1,000, another\
    \ at address 100,000âyou get the idea. This means that when you're traversing\
    \ the matrix, you're leaping through memory like a wild person. This tends to\
    \ result in cache misses that vastly slow down your processing time.\nSo, if you\
    \ absolute must have your cute [x][y] indexing syntax, use that solution. If you\
    \ want quickness and smallness (and if you don't care about those, why are you\
    \ working in C++?), you need a different solution.\nA Different Solution\nThe\
    \ better solution is to allocate your whole matrix as a single dynamic array,\
    \ then use (slightly) clever indexing math of your own to access cells. The indexing\
    \ math is only very slightly clever; nah, it's not clever at all: it's obvious.\n\
    class Matrix\n{\n    ...\n    size_t index( int x, int y ) const { return x +\
    \ m_width * y; }\n};\n\nGiven this index() function (which I'm imagining is a\
    \ member of a class because it needs to know the m_width of your matrix), you\
    \ can access cells within your matrix array. The matrix array is allocated like\
    \ this:\narray = new int[ width * height ];\n\nSo the equivalent of this in the\
    \ slow, fat solution:\narray[ x ][ y ]\n\n...is this in the quick, small solution:\n\
    array[ index( x, y )]\n\nSad, I know. But you'll get used to it. And your CPU\
    \ will thank you.\n"
- - What are the barriers to understanding pointers and what can be done to overcome
    them? [closed]
  - "\nPointers is a concept that for many can be confusing at first, in particular\
    \ when it comes to copying pointer values around and still referencing the same\
    \ memory block.\nI've found that the best analogy is to consider the pointer as\
    \ a piece of paper with a house address on it, and the memory block it references\
    \ as the actual house. All sorts of operations can thus be easily explained.\n\
    I've added some Delphi code down below, and some comments where appropriate. I\
    \ chose Delphi since my other main programming language, C#, does not exhibit\
    \ things like memory leaks in the same way.\nIf you only wish to learn the high-level\
    \ concept of pointers, then you should ignore the parts labelled \"Memory layout\"\
    \ in the explanation below. They are intended to give examples of what memory\
    \ could look like after operations, but they are more low-level in nature. However,\
    \ in order to accurately explain how buffer overruns really work, it was important\
    \ that I added these diagrams.\nDisclaimer: For all intents and purposes, this\
    \ explanation and the example memory\nlayouts are vastly simplified. There's more\
    \ overhead and a lot more details you would\nneed to know if you need to deal\
    \ with memory on a low-level basis. However, for the\nintents of explaining memory\
    \ and pointers, it is accurate enough.\n\nLet's assume the THouse class used below\
    \ looks like this:\ntype\n    THouse = class\n    private\n        FName : array[0..9]\
    \ of Char;\n    public\n        constructor Create(name: PChar);\n    end;\n\n\
    When you initialize the house object, the name given to the constructor is copied\
    \ into the private field FName. There is a reason it is defined as a fixed-size\
    \ array.\nIn memory, there will be some overhead associated with the house allocation,\
    \ I'll illustrate this below like this:\n\n---[ttttNNNNNNNNNN]---\n     ^   ^\n\
    \     |   |\n     |   +- the FName array\n     |\n     +- overhead\n\nThe \"tttt\"\
    \ area is overhead, there will typically be more of this for various types of\
    \ runtimes and languages, like 8 or 12 bytes. It is imperative that whatever values\
    \ are stored in this area never gets changed by anything other than the memory\
    \ allocator or the core system routines, or you risk crashing the program.\n\n\
    Allocate memory\nGet an entrepreneur to build your house, and give you the address\
    \ to the house. In contrast to the real world, memory allocation cannot be told\
    \ where to allocate, but will find a suitable spot with enough room, and report\
    \ back the address to the allocated memory.\nIn other words, the entrepreneur\
    \ will choose the spot.\nTHouse.Create('My house');\n\nMemory layout:\n\n---[ttttNNNNNNNNNN]---\n\
    \    1234My house\n\n\nKeep a variable with the address\nWrite the address to\
    \ your new house down on a piece of paper. This paper will serve as your reference\
    \ to your house. Without this piece of paper, you're lost, and cannot find the\
    \ house, unless you're already in it.\nvar\n    h: THouse;\nbegin\n    h := THouse.Create('My\
    \ house');\n    ...\n\nMemory layout:\n\n    h\n    v\n---[ttttNNNNNNNNNN]---\n\
    \    1234My house\n\n\nCopy pointer value \nJust write the address on a new piece\
    \ of paper. You now have two pieces of paper that will get you to the same house,\
    \ not two separate houses. Any attempts to follow the address from one paper and\
    \ rearrange the furniture at that house will make it seem that the other house\
    \ has been modified in the same manner, unless you can explicitly detect that\
    \ it's actually just one house.\nNote This is usually the concept that I have\
    \ the most problem explaining to people, two pointers does not mean two objects\
    \ or memory blocks.\nvar\n    h1, h2: THouse;\nbegin\n    h1 := THouse.Create('My\
    \ house');\n    h2 := h1; // copies the address, not the house\n    ...\n\n\n\
    \    h1\n    v\n---[ttttNNNNNNNNNN]---\n    1234My house\n    ^\n    h2\n\n\n\
    Freeing the memory \nDemolish the house. You can then later on reuse the paper\
    \ for a new address if you so wish, or clear it to forget the address to the house\
    \ that no longer exists.\nvar\n    h: THouse;\nbegin\n    h := THouse.Create('My\
    \ house');\n    ...\n    h.Free;\n    h := nil;\n\nHere I first construct the\
    \ house, and get hold of its address. Then I do something to the house (use it,\
    \ the ... code, left as an exercise for the reader), and then I free it. Lastly\
    \ I clear the address from my variable.\nMemory layout:\n\n    h             \
    \           <--+\n    v                           +- before free\n---[ttttNNNNNNNNNN]---\
    \          |\n    1234My house             <--+\n\n    h (now points nowhere)\
    \   <--+\n                                +- after free\n----------------------\
    \          | (note, memory might still\n    xx34My house             <--+  contain\
    \ some data)\n\n\nDangling pointers\nYou tell your entrepreneur to destroy the\
    \ house, but you forget to erase the address from your piece of paper. When later\
    \ on you look at the piece of paper, you've forgotten that the house is no longer\
    \ there, and goes to visit it, with failed results (see also the part about an\
    \ invalid reference below).\nvar\n    h: THouse;\nbegin\n    h := THouse.Create('My\
    \ house');\n    ...\n    h.Free;\n    ... // forgot to clear h here\n    h.OpenFrontDoor;\
    \ // will most likely fail\n\nUsing h after the call to .Free might work, but\
    \ that is just pure luck. Most likely it will fail, at a customers place, in the\
    \ middle of a critical operation.\n\n    h                        <--+\n    v\
    \                           +- before free\n---[ttttNNNNNNNNNN]---          |\n\
    \    1234My house             <--+\n\n    h                        <--+\n    v\
    \                           +- after free\n----------------------          |\n\
    \    xx34My house             <--+\n\nAs you can see, h still points to the remnants\
    \ of the data in memory, but\nsince it might not be complete, using it as before\
    \ might fail.\n\nMemory leak \nYou lose the piece of paper and cannot find the\
    \ house. The house is still standing somewhere though, and when you later on want\
    \ to construct a new house, you cannot reuse that spot.\nvar\n    h: THouse;\n\
    begin\n    h := THouse.Create('My house');\n    h := THouse.Create('My house');\
    \ // uh-oh, what happened to our first house?\n    ...\n    h.Free;\n    h :=\
    \ nil;\n\nHere we overwrote the contents of the h variable with the address of\
    \ a new house, but the old one is still standing... somewhere. After this code,\
    \ there is no way to reach that house, and it will be left standing. In other\
    \ words, the allocated memory will stay allocated until the application closes,\
    \ at which point the operating system will tear it down.\nMemory layout after\
    \ first allocation:\n\n    h\n    v\n---[ttttNNNNNNNNNN]---\n    1234My house\n\
    \nMemory layout after second allocation:\n\n                       h\n       \
    \                v\n---[ttttNNNNNNNNNN]---[ttttNNNNNNNNNN]\n    1234My house \
    \      5678My house\n\nA more common way to get this method is just to forget\
    \ to free something, instead of overwriting it as above. In Delphi terms, this\
    \ will occur with the following method:\nprocedure OpenTheFrontDoorOfANewHouse;\n\
    var\n    h: THouse;\nbegin\n    h := THouse.Create('My house');\n    h.OpenFrontDoor;\n\
    \    // uh-oh, no .Free here, where does the address go?\nend;\n\nAfter this method\
    \ has executed, there's no place in our variables that the address to the house\
    \ exists, but the house is still out there.\nMemory layout:\n\n    h         \
    \               <--+\n    v                           +- before losing pointer\n\
    ---[ttttNNNNNNNNNN]---          |\n    1234My house             <--+\n\n    h\
    \ (now points nowhere)   <--+\n                                +- after losing\
    \ pointer\n---[ttttNNNNNNNNNN]---          |\n    1234My house             <--+\n\
    \nAs you can see, the old data is left intact in memory, and will not\nbe reused\
    \ by the memory allocator. The allocator keeps track of which\nareas of memory\
    \ has been used, and will not reuse them unless you\nfree it.\n\nFreeing the memory\
    \ but keeping a (now invalid) reference \nDemolish the house, erase one of the\
    \ pieces of paper but you also have another piece of paper with the old address\
    \ on it, when you go to the address, you won't find a house, but you might find\
    \ something that resembles the ruins of one.\nPerhaps you will even find a house,\
    \ but it is not the house you were originally given the address to, and thus any\
    \ attempts to use it as though it belongs to you might fail horribly.\nSometimes\
    \ you might even find that a neighbouring address has a rather big house set up\
    \ on it that occupies three address (Main Street 1-3), and your address goes to\
    \ the middle of the house. Any attempts to treat that part of the large 3-address\
    \ house as a single small house might also fail horribly.\nvar\n    h1, h2: THouse;\n\
    begin\n    h1 := THouse.Create('My house');\n    h2 := h1; // copies the address,\
    \ not the house\n    ...\n    h1.Free;\n    h1 := nil;\n    h2.OpenFrontDoor;\
    \ // uh-oh, what happened to our house?\n\nHere the house was torn down, through\
    \ the reference in h1, and while h1 was cleared as well, h2 still has the old,\
    \ out-of-date, address. Access to the house that is no longer standing might or\
    \ might not work.\nThis is a variation of the dangling pointer above. See its\
    \ memory layout.\n\nBuffer overrun \nYou move more stuff into the house than you\
    \ can possibly fit, spilling into the neighbours house or yard. When the owner\
    \ of that neighbouring house later on comes home, he'll find all sorts of things\
    \ he'll consider his own.\nThis is the reason I chose a fixed-size array. To set\
    \ the stage, assume that\nthe second house we allocate will, for some reason,\
    \ be placed before the\nfirst one in memory. In other words, the second house\
    \ will have a lower\naddress than the first one. Also, they're allocated right\
    \ next to each other.\nThus, this code:\nvar\n    h1, h2: THouse;\nbegin\n   \
    \ h1 := THouse.Create('My house');\n    h2 := THouse.Create('My other house somewhere');\n\
    \                         ^-----------------------^\n                        \
    \  longer than 10 characters\n                         0123456789 <-- 10 characters\n\
    \nMemory layout after first allocation:\n\n                        h1\n      \
    \                  v\n-----------------------[ttttNNNNNNNNNN]\n              \
    \          5678My house\n\nMemory layout after second allocation:\n\n    h2  \
    \                h1\n    v                   v\n---[ttttNNNNNNNNNN]----[ttttNNNNNNNNNN]\n\
    \    1234My other house somewhereouse\n                        ^---+--^\n    \
    \                        |\n                            +- overwritten\n\nThe\
    \ part that will most often cause crash is when you overwrite important parts\n\
    of the data you stored that really should not be randomly changed. For instance\n\
    it might not be a problem that parts of the name of the h1-house was changed,\n\
    in terms of crashing the program, but overwriting the overhead of the\nobject\
    \ will most likely crash when you try to use the broken object,\nas will overwriting\
    \ links that is stored to\nother objects in the object.\n\nLinked lists \nWhen\
    \ you follow an address on a piece of paper, you get to a house, and at that house\
    \ there is another piece of paper with a new address on it, for the next house\
    \ in the chain, and so on.\nvar\n    h1, h2: THouse;\nbegin\n    h1 := THouse.Create('Home');\n\
    \    h2 := THouse.Create('Cabin');\n    h1.NextHouse := h2;\n\nHere we create\
    \ a link from our home house to our cabin. We can follow the chain until a house\
    \ has no NextHouse reference, which means it's the last one. To visit all our\
    \ houses, we could use the following code:\nvar\n    h1, h2: THouse;\n    h: THouse;\n\
    begin\n    h1 := THouse.Create('Home');\n    h2 := THouse.Create('Cabin');\n \
    \   h1.NextHouse := h2;\n    ...\n    h := h1;\n    while h <> nil do\n    begin\n\
    \        h.LockAllDoors;\n        h.CloseAllWindows;\n        h := h.NextHouse;\n\
    \    end;\n\nMemory layout (added NextHouse as a link in the object, noted with\n\
    the four LLLL's in the below diagram):\n\n    h1                      h2\n   \
    \ v                       v\n---[ttttNNNNNNNNNNLLLL]----[ttttNNNNNNNNNNLLLL]\n\
    \    1234Home       +        5678Cabin      +\n                   |        ^ \
    \             |\n                   +--------+              * (no link)\n\n\n\
    In basic terms, what is a memory address?\nA memory address is in basic terms\
    \ just a number. If you think of memory\nas a big array of bytes, the very first\
    \ byte has the address 0, the next one\nthe address 1 and so on upwards. This\
    \ is simplified, but good enough.\nSo this memory layout:\n\n    h1          \
    \       h2\n    v                  v\n---[ttttNNNNNNNNNN]---[ttttNNNNNNNNNN]\n\
    \    1234My house       5678My house\n\nMight have these two address (the leftmost\
    \ - is address 0):\n\nh1 = 4\nh2 = 23\n\nWhich means that our linked list above\
    \ might actuall look like this:\n\n    h1 (=4)                 h2 (=28)\n    v\
    \                       v\n---[ttttNNNNNNNNNNLLLL]----[ttttNNNNNNNNNNLLLL]\n \
    \   1234Home      0028      5678Cabin     0000\n                   |        ^\
    \              |\n                   +--------+              * (no link)\n\nIt\
    \ is typical to store an address that \"points nowhere\" as a zero-address.\n\n\
    In basic terms, what is a pointer?\nA pointer is just a variable holding a memory\
    \ address. You can typically ask the programming\nlanguage to give you its number,\
    \ but most programming languages and runtimes tries to\nhide the fact that there\
    \ is a number beneath, just because the number itself does not\nreally hold any\
    \ meaning to you. It is best to think of a pointer as a black box, ie.\nyou don't\
    \ really know or care about how it is actually implemented, just as long as it\n\
    works.\n"
- - What are the barriers to understanding pointers and what can be done to overcome
    them? [closed]
  - "\nIn my first Comp Sci class, we did the following exercise. Granted, this was\
    \ a lecture hall with roughly 200 students in it...\nProfessor writes on the board:\
    \ int john;\nJohn stands up\nProfessor writes: int *sally = &john;\nSally stands\
    \ up, points at john\nProfessor: int *bill = sally;\nBill stands up, points at\
    \ John\nProfessor: int sam;\nSam stands up\nProfessor: bill = &sam;\nBill now\
    \ points to Sam.\nI think you get the idea. I think we spent about an hour doing\
    \ this, until we went over the basics of pointer assignment.\n"
- - What are the barriers to understanding pointers and what can be done to overcome
    them? [closed]
  - "\nAn analogy I've found helpful for explaining pointers is hyperlinks. Most people\
    \ can understand that a link on a web page 'points' to another page on the internet,\
    \ and if you can copy & paste that hyperlink then they will both point to the\
    \ same original web page. If you go and edit that original page, then follow either\
    \ of those links (pointers) you'll get that new updated page.\n"
- - How do I use arrays in C++?
  - "\nArrays on the type level\nAn array type is denoted as T[n] where T is the element\
    \ type and n is a positive size, the number of elements in the array. The array\
    \ type is a product type of the element type and the size. If one or both of those\
    \ ingredients differ, you get a distinct type:\n#include <type_traits>\n\nstatic_assert(!std::is_same<int[8],\
    \ float[8]>::value, \"distinct element type\");\nstatic_assert(!std::is_same<int[8],\
    \   int[9]>::value, \"distinct size\");\n\nNote that the size is part of the type,\
    \ that is, array types of different size are incompatible types that have absolutely\
    \ nothing to do with each other. sizeof(T[n]) is equivalent to n * sizeof(T).\n\
    Array-to-pointer decay\nThe only \"connection\" between T[n] and T[m] is that\
    \ both types can implicitly be converted to T*, and the result of this conversion\
    \ is a pointer to the first element of the array. That is, anywhere a T* is required,\
    \ you can provide a T[n], and the compiler will silently provide that pointer:\n\
    \                  +---+---+---+---+---+---+---+---+\nthe_actual_array: |   |\
    \   |   |   |   |   |   |   |   int[8]\n                  +---+---+---+---+---+---+---+---+\n\
    \                    ^\n                    |\n                    |\n       \
    \             |\n                    |  pointer_to_the_first_element   int*\n\n\
    This conversion is known as \"array-to-pointer decay\", and it is a major source\
    \ of confusion. The size of the array is lost in this process, since it is no\
    \ longer part of the type (T*). Pro: Forgetting the size of an array on the type\
    \ level allows a pointer to point to the first element of an array of any size.\
    \ Con: Given a pointer to the first (or any other) element of an array, there\
    \ is no way to detect how large that array is or where exactly the pointer points\
    \ to relative to the bounds of the array. Pointers are extremely stupid.\nArrays\
    \ are not pointers\nThe compiler will silently generate a pointer to the first\
    \ element of an array whenever it is deemed useful, that is, whenever an operation\
    \ would fail on an array but succeed on a pointer. This conversion from array\
    \ to pointer is trivial, since the resulting pointer value is simply the address\
    \ of the array. Note that the pointer is not stored as part of the array itself\
    \ (or anywhere else in memory). An array is not a pointer.\nstatic_assert(!std::is_same<int[8],\
    \ int*>::value, \"an array is not a pointer\");\n\nOne important context in which\
    \ an array does not decay into a pointer to its first element is when the & operator\
    \ is applied to it. In that case, the & operator yields a pointer to the entire\
    \ array, not just a pointer to its first element. Although in that case the values\
    \ (the addresses) are the same, a pointer to the first element of an array and\
    \ a pointer to the entire array are completely distinct types:\nstatic_assert(!std::is_same<int*,\
    \ int(*)[8]>::value, \"distinct element type\");\n\nThe following ASCII art explains\
    \ this distinction:\n      +-----------------------------------+\n      | +---+---+---+---+---+---+---+---+\
    \ |\n+---> | |   |   |   |   |   |   |   |   | | int[8]\n|     | +---+---+---+---+---+---+---+---+\
    \ |\n|     +---^-------------------------------+\n|         |\n|         |\n|\
    \         |\n|         |  pointer_to_the_first_element   int*\n|\n|  pointer_to_the_entire_array\
    \              int(*)[8]\n\nNote how the pointer to the first element only points\
    \ to a single integer (depicted as a small box), whereas the pointer to the entire\
    \ array points to an array of 8 integers (depicted as a large box).\nThe same\
    \ situation arises in classes and is maybe more obvious. A pointer to an object\
    \ and a pointer to its first data member have the same value (the same address),\
    \ yet they are completely distinct types.\nIf you are unfamiliar with the C declarator\
    \ syntax, the parenthesis in the type int(*)[8] are essential:\n\nint(*)[8] is\
    \ a pointer to an array of 8 integers.\nint*[8] is an array of 8 pointers, each\
    \ element of type int*.\n\nAccessing elements\nC++ provides two syntactic variations\
    \ to access individual elements of an array.\nNeither of them is superior to the\
    \ other, and you should familiarize yourself with both.\nPointer arithmetic\n\
    Given a pointer p to the first element of an array, the expression p+i yields\
    \ a pointer to the i-th element of the array. By dereferencing that pointer afterwards,\
    \ one can access individual elements:\nstd::cout << *(x+3) << \", \" << *(x+7)\
    \ << std::endl;\n\nIf x denotes an array, then array-to-pointer decay will kick\
    \ in, because adding an array and an integer is meaningless (there is no plus\
    \ operation on arrays), but adding a pointer and an integer makes sense:\n   +---+---+---+---+---+---+---+---+\n\
    x: |   |   |   |   |   |   |   |   |   int[8]\n   +---+---+---+---+---+---+---+---+\n\
    \     ^           ^               ^\n     |           |               |\n    \
    \ |           |               |\n     |           |               |\nx+0  |  \
    \    x+3  |          x+7  |     int*\n\n(Note that the implicitly generated pointer\
    \ has no name, so I wrote x+0 in order to identify it.)\nIf, on the other hand,\
    \ x denotes a pointer to the first (or any other) element of an array, then array-to-pointer\
    \ decay is not necessary, because the pointer on which i is going to be added\
    \ already exists:\n   +---+---+---+---+---+---+---+---+\n   |   |   |   |   |\
    \   |   |   |   |   int[8]\n   +---+---+---+---+---+---+---+---+\n     ^     \
    \      ^               ^\n     |           |               |\n     |         \
    \  |               |\n   +-|-+         |               |\nx: | | |    x+3  | \
    \         x+7  |     int*\n   +---+\n\nNote that in the depicted case, x is a\
    \ pointer variable (discernible by the small box next to x), but it could just\
    \ as well be the result of a function returning a pointer (or any other expression\
    \ of type T*).\nIndexing operator\nSince the syntax *(x+i) is a bit clumsy, C++\
    \ provides the alternative syntax x[i]:\nstd::cout << x[3] << \", \" << x[7] <<\
    \ std::endl;\n\nDue to the fact that addition is commutative, the following code\
    \ does exactly the same:\nstd::cout << 3[x] << \", \" << 7[x] << std::endl;\n\n\
    The definition of the indexing operator leads to the following interesting equivalence:\n\
    &x[i]  ==  &*(x+i)  ==  x+i\n\nHowever, &x[0] is generally not equivalent to x.\
    \ The former is a pointer, the latter an array. Only when the context triggers\
    \ array-to-pointer decay can x and &x[0] be used interchangeably. For example:\n\
    T* p = &array[0];  // rewritten as &*(array+0), decay happens due to the addition\n\
    T* q = array;      // decay happens due to the assignment\n\nOn the first line,\
    \ the compiler detects an assignment from a pointer to a pointer, which trivially\
    \ succeeds. On the second line, it detects an assignment from an array to a pointer.\
    \ Since this is meaningless (but pointer to pointer assignment makes sense), array-to-pointer\
    \ decay kicks in as usual.\nRanges\nAn array of type T[n] has n elements, indexed\
    \ from 0 to n-1; there is no element n. And yet, to support half-open ranges (where\
    \ the beginning is inclusive and the end is exclusive), C++ allows the computation\
    \ of a pointer to the (non-existent) n-th element, but it is illegal to dereference\
    \ that pointer:\n   +---+---+---+---+---+---+---+---+....\nx: |   |   |   |  \
    \ |   |   |   |   |   .   int[8]\n   +---+---+---+---+---+---+---+---+....\n \
    \    ^                               ^\n     |                               |\n\
    \     |                               |\n     |                              \
    \ |\nx+0  |                          x+8  |     int*\n\nFor example, if you want\
    \ to sort an array, both of the following would work equally well:\nstd::sort(x\
    \ + 0, x + n);\nstd::sort(&x[0], &x[0] + n);\n\nNote that it is illegal to provide\
    \ &x[n] as the second argument since this is equivalent to &*(x+n), and the sub-expression\
    \ *(x+n) technically invokes undefined behavior in C++ (but not in C99).\nAlso\
    \ note that you could simply provide x as the first argument. That is a little\
    \ too terse for my taste, and it also makes template argument deduction a bit\
    \ harder for the compiler, because in that case the first argument is an array\
    \ but the second argument is a pointer. (Again, array-to-pointer decay kicks in.)\n"
- - How do I use arrays in C++?
  - "\nProgrammers often confuse multidimensional arrays with arrays of pointers.\n\
    Multidimensional arrays\nMost programmers are familiar with named multidimensional\
    \ arrays, but many are unaware of the fact that multidimensional array can also\
    \ be created anonymously. Multidimensional arrays are often referred to as \"\
    arrays of arrays\" or \"true multidimensional arrays\".\nNamed multidimensional\
    \ arrays\nWhen using named multidimensional arrays, all dimensions must be known\
    \ at compile time:\nint H = read_int();\nint W = read_int();\n\nint connect_four[6][7];\
    \   // okay\n\nint connect_four[H][7];   // ISO C++ forbids variable length array\n\
    int connect_four[6][W];   // ISO C++ forbids variable length array\nint connect_four[H][W];\
    \   // ISO C++ forbids variable length array\n\nThis is how a named multidimensional\
    \ array looks like in memory:\n              +---+---+---+---+---+---+---+\nconnect_four:\
    \ |   |   |   |   |   |   |   |\n              +---+---+---+---+---+---+---+\n\
    \              |   |   |   |   |   |   |   |\n              +---+---+---+---+---+---+---+\n\
    \              |   |   |   |   |   |   |   |\n              +---+---+---+---+---+---+---+\n\
    \              |   |   |   |   |   |   |   |\n              +---+---+---+---+---+---+---+\n\
    \              |   |   |   |   |   |   |   |\n              +---+---+---+---+---+---+---+\n\
    \              |   |   |   |   |   |   |   |\n              +---+---+---+---+---+---+---+\n\
    \nNote that 2D grids such as the above are merely helpful visualizations. From\
    \ the point of view of C++, memory is a \"flat\" sequence of bytes. The elements\
    \ of a multidimensional array are stored in row-major order. That is, connect_four[0][6]\
    \ and connect_four[1][0] are neighbors in memory. In fact, connect_four[0][7]\
    \ and connect_four[1][0] denote the same element! This means that you can take\
    \ multi-dimensional arrays and treat them as large, one-dimensional arrays:\n\
    int* p = &connect_four[0][0];\nint* q = p + 42;\nsome_int_sequence_algorithm(p,\
    \ q);\n\nAnonymous multidimensional arrays\nWith anonymous multidimensional arrays,\
    \ all dimensions except the first must be known at compile time:\nint (*p)[7]\
    \ = new int[6][7];   // okay\nint (*p)[7] = new int[H][7];   // okay\n\nint (*p)[W]\
    \ = new int[6][W];   // ISO C++ forbids variable length array\nint (*p)[W] = new\
    \ int[H][W];   // ISO C++ forbids variable length array\n\nThis is how an anonymous\
    \ multidimensional array looks like in memory:\n              +---+---+---+---+---+---+---+\n\
    \        +---> |   |   |   |   |   |   |   |\n        |     +---+---+---+---+---+---+---+\n\
    \        |     |   |   |   |   |   |   |   |\n        |     +---+---+---+---+---+---+---+\n\
    \        |     |   |   |   |   |   |   |   |\n        |     +---+---+---+---+---+---+---+\n\
    \        |     |   |   |   |   |   |   |   |\n        |     +---+---+---+---+---+---+---+\n\
    \        |     |   |   |   |   |   |   |   |\n        |     +---+---+---+---+---+---+---+\n\
    \        |     |   |   |   |   |   |   |   |\n        |     +---+---+---+---+---+---+---+\n\
    \        |\n      +-|-+\n   p: | | |\n      +---+\n\nNote that the array itself\
    \ is still allocated as a single block in memory.\nArrays of pointers\nYou can\
    \ overcome the restriction of fixed width by introducing another level of indirection.\n\
    Named arrays of pointers\nHere is a named array of five pointers which are initialized\
    \ with anonymous arrays of different lengths:\nint* triangle[5];\nfor (int i =\
    \ 0; i < 5; ++i)\n{\n    triangle[i] = new int[5 - i];\n}\n\n// ...\n\nfor (int\
    \ i = 0; i < 5; ++i)\n{\n    delete[] triangle[i];\n}\n\nAnd here is how it looks\
    \ like in memory:\n          +---+---+---+---+---+\n          |   |   |   |  \
    \ |   |\n          +---+---+---+---+---+\n            ^\n            | +---+---+---+---+\n\
    \            | |   |   |   |   |\n            | +---+---+---+---+\n          \
    \  |   ^\n            |   | +---+---+---+\n            |   | |   |   |   |\n \
    \           |   | +---+---+---+\n            |   |   ^\n            |   |   |\
    \ +---+---+\n            |   |   | |   |   |\n            |   |   | +---+---+\n\
    \            |   |   |   ^\n            |   |   |   | +---+\n            |   |\
    \   |   | |   |\n            |   |   |   | +---+\n            |   |   |   |  \
    \ ^\n            |   |   |   |   |\n            |   |   |   |   |\n          +-|-+-|-+-|-+-|-+-|-+\n\
    triangle: | | | | | | | | | | |\n          +---+---+---+---+---+\n\nSince each\
    \ line is allocated individually now, viewing 2D arrays as 1D arrays does not\
    \ work anymore.\nAnonymous arrays of pointers\nHere is an anonymous array of 5\
    \ (or any other number of) pointers which are initialized with anonymous arrays\
    \ of different lengths:\nint n = calculate_five();   // or any other number\n\
    int** p = new int*[n];\nfor (int i = 0; i < n; ++i)\n{\n    p[i] = new int[n -\
    \ i];\n}\n\n// ...\n\nfor (int i = 0; i < n; ++i)\n{\n    delete[] p[i];\n}\n\
    delete[] p;   // note the extra delete[] !\n\nAnd here is how it looks like in\
    \ memory:\n          +---+---+---+---+---+\n          |   |   |   |   |   |\n\
    \          +---+---+---+---+---+\n            ^\n            | +---+---+---+---+\n\
    \            | |   |   |   |   |\n            | +---+---+---+---+\n          \
    \  |   ^\n            |   | +---+---+---+\n            |   | |   |   |   |\n \
    \           |   | +---+---+---+\n            |   |   ^\n            |   |   |\
    \ +---+---+\n            |   |   | |   |   |\n            |   |   | +---+---+\n\
    \            |   |   |   ^\n            |   |   |   | +---+\n            |   |\
    \   |   | |   |\n            |   |   |   | +---+\n            |   |   |   |  \
    \ ^\n            |   |   |   |   |\n            |   |   |   |   |\n          +-|-+-|-+-|-+-|-+-|-+\n\
    \          | | | | | | | | | | |\n          +---+---+---+---+---+\n          \
    \  ^\n            |\n            |\n          +-|-+\n       p: | | |\n       \
    \   +---+\n\nConversions\nArray-to-pointer decay naturally extends to arrays of\
    \ arrays and arrays of pointers:\nint array_of_arrays[6][7];\nint (*pointer_to_array)[7]\
    \ = array_of_arrays;\n\nint* array_of_pointers[6];\nint** pointer_to_pointer =\
    \ array_of_pointers;\n\nHowever, there is no implicit conversion from T[h][w]\
    \ to T**. If such an implicit conversion did exist, the result would be a pointer\
    \ to the first element of an array of h pointers to T (each pointing to the first\
    \ element of a line in the original 2D array), but that pointer array does not\
    \ exist anywhere in memory yet. If you want such a conversion, you must create\
    \ and fill the required pointer array manually:\nint connect_four[6][7];\n\nint**\
    \ p = new int*[6];\nfor (int i = 0; i < 6; ++i)\n{\n    p[i] = connect_four[i];\n\
    }\n\n// ...\n\ndelete[] p;\n\nNote that this generates a view of the original\
    \ multidimensional array. If you need a copy instead, you must create extra arrays\
    \ and copy the data yourself:\nint connect_four[6][7];\n\nint** p = new int*[6];\n\
    for (int i = 0; i < 6; ++i)\n{\n    p[i] = new int[7];\n    std::copy(connect_four[i],\
    \ connect_four[i + 1], p[i]);\n}\n\n// ...\n\nfor (int i = 0; i < 6; ++i)\n{\n\
    \    delete[] p[i];\n}\ndelete[] p;\n\n"
- - How do I use arrays in C++?
  - "\nAssignment\nFor no particular reason, arrays cannot be assigned to one another.\
    \ Use std::copy instead:\n#include <algorithm>\n\n// ...\n\nint a[8] = {2, 3,\
    \ 5, 7, 11, 13, 17, 19};\nint b[8];\nstd::copy(a + 0, a + 8, b);\n\nThis is more\
    \ flexible than what true array assignment could provide because it is possible\
    \ to copy slices of larger arrays into smaller arrays.\nstd::copy is usually specialized\
    \ for primitive types to give maximum performance. It is unlikely that std::memcpy\
    \ performs better. If in doubt, measure.\nAlthough you cannot assign arrays directly,\
    \ you can assign structs and classes which contain array members. That is because\
    \ array members are copied memberwise by the assignment operator which is provided\
    \ as a default by the compiler. If you define the assignment operator manually\
    \ for your own struct or class types, you must fall back to manual copying for\
    \ the array members.\nParameter passing\nArrays cannot be passed by value. You\
    \ can either pass them by pointer or by reference.\nPass by pointer\nSince arrays\
    \ themselves cannot be passed by value, usually a pointer to their first element\
    \ is passed by value instead. This is often called \"pass by pointer\". Since\
    \ the size of the array is not retrievable via that pointer, you have to pass\
    \ a second parameter indicating the size of the array (the classic C solution)\
    \ or a second pointer pointing after the last element of the array (the C++ iterator\
    \ solution):\n#include <numeric>\n#include <cstddef>\n\nint sum(const int* p,\
    \ std::size_t n)\n{\n    return std::accumulate(p, p + n, 0);\n}\n\nint sum(const\
    \ int* p, const int* q)\n{\n    return std::accumulate(p, q, 0);\n}\n\nAs a syntactic\
    \ alternative, you can also declare parameters as T p[], and it means the exact\
    \ same thing as T* p in the context of parameter lists only:\nint sum(const int\
    \ p[], std::size_t n)\n{\n    return std::accumulate(p, p + n, 0);\n}\n\nYou can\
    \ think of the compiler as rewriting T p[] to T *p in the context of parameter\
    \ lists only. This special rule is partly responsible for the whole confusion\
    \ about arrays and pointers. In every other context, declaring something as an\
    \ array or as a pointer makes a huge difference.\nUnfortunately, you can also\
    \ provide a size in an array parameter which is silently ignored by the compiler.\
    \ That is, the following three signatures are exactly equivalent, as indicated\
    \ by the compiler errors:\nint sum(const int* p, std::size_t n)\n\n// error: redefinition\
    \ of 'int sum(const int*, size_t)'\nint sum(const int p[], std::size_t n)\n\n\
    // error: redefinition of 'int sum(const int*, size_t)'\nint sum(const int p[8],\
    \ std::size_t n)   // the 8 has no meaning here\n\nPass by reference\nArrays can\
    \ also be passed by reference:\nint sum(const int (&a)[8])\n{\n    return std::accumulate(a\
    \ + 0, a + 8, 0);\n}\n\nIn this case, the array size is significant. Since writing\
    \ a function that only accepts arrays of exactly 8 elements is of little use,\
    \ programmers usually write such functions as templates:\ntemplate <std::size_t\
    \ n>\nint sum(const int (&a)[n])\n{\n    return std::accumulate(a + 0, a + n,\
    \ 0);\n}\n\nNote that you can only call such a function template with an actual\
    \ array of integers, not with a pointer to an integer. The size of the array is\
    \ automatically inferred, and for every size n, a different function is instantiated\
    \ from the template. You can also write quite useful function templates that abstract\
    \ from both the element type and from the size.\n"
- - Difference between `constexpr` and `const`
  - "\nBasic meaning and syntax\nBoth keywords can be used in the declaration of objects\
    \ as well as functions. The basic difference when applied to objects is this:\n\
    \nconst declares an object as constant. This implies a guarantee that, once initialized,\
    \ the value of that object won't change, and the compiler can make use of this\
    \ fact for optimizations. It also helps prevent the programmer from writing code\
    \ that modifies objects that were not meant to be modified after initialization.\n\
    constexpr declares an object as fit for use in what the Standard calls constant\
    \ expressions. But note that constexpr is not the only way to do this.\n\nWhen\
    \ applied to functions the basic difference is this:\n\nconst can only be used\
    \ for non-static member functions, not functions in general. It gives a guarantee\
    \ that the member function does not modify any of the non-static data members.\n\
    constexpr can be used with both member and non-member functions, as well as constructors.\
    \ It declares the function fit for use in constant expressions. The compiler will\
    \ only accept it if the function meets certain criteria (7.1.5/3,4), most importantly\
    \ (â ):\n\nThe function body must be non-virtual and extremely simple: Apart from\
    \ typedefs and static asserts, only a single return statement is allowed. In the\
    \ case of a constructor, only an initialization list, typedefs and static assert\
    \ are allowed. (= default and = delete are allowed, too, though.)\nAs of C++14\
    \ the rules are more relaxed, what is allowed since then inside a constexpr function:\
    \ asm declaration, a goto statement, a statement with a label other than case\
    \ and default, try-block, definition of a variable of non-literal type, definition\
    \ of a variable of static or thread storage duration, definition of a variable\
    \ for which no initialization is performed.\nThe arguments and the return type\
    \ must be literal types (i.e., generally speaking, very simple types, typically\
    \ scalars or aggregates)\n\n\nConstant expressions\nAs said above, constexpr declares\
    \ both objects as well as functions as fit for use in constant expressions. A\
    \ constant expression is more than merely constant:\n\nIt can be used in places\
    \ that require compile-time evaluation, for example, template parameters and array-size\
    \ specifiers:\ntemplate<int N>\nclass fixed_size_list\n{ /*...*/ };\n\nfixed_size_list<X>\
    \ mylist;  // X must be an integer constant expression\n\nint numbers[X];  //\
    \ X must be an integer constant expression\n\nBut note:\n\nDeclaring something\
    \ as constexpr does not necessarily guarantee that it will be evaluated at compile\
    \ time. It can be used for such, but it can be used in other places that are evaluated\
    \ at run-time, as well.\nAn object may be fit for use in constant expressions\
    \ without being declared constexpr. Example:\nint main()\n{\n  const int N = 3;\n\
    \  int numbers[N] = {1, 2, 3};  // N is constant expression\n}\n\n\nThis is possible\
    \ because N, being constant and initialized at declaration time with a literal,\
    \ satisfies the criteria for a constant expression, even if it isn't declared\
    \ constexpr.\n\nSo when do I actually have to use constexpr?\n\nAn object like\
    \ N above can be used as constant expression without being declared constexpr.\
    \ This is true for all objects that are:\n\nconst\nof integral or enumeration\
    \ type and\ninitialized at declaration time with an expression that is itself\
    \ a constant expression\n\n\n[This is due to Â§5.19/2: A constant expression must\
    \ not include a subexpressions that involves \"an lvalue-to-rvalue modification\
    \ unless [â¦] a glvalue of integral or enumeration type [â¦]\" Thanks to Richard\
    \ Smith for correcting my earlier claim that this was true for all literal types.]\n\
    For a function to be fit for use in constant expressions, it must be explicitly\
    \ declared constexpr; it is not sufficient for it merely to satisfy the criteria\
    \ for constant-expression functions. Example:\ntemplate<int N>\nclass list\n{\
    \ };\n\nconstexpr int sqr1(int arg)\n{ return arg * arg; }\n\nint sqr2(int arg)\n\
    { return arg * arg; }\n\nint main()\n{\n  const int X = 2;\n  list<sqr1(X)> mylist1;\
    \  // OK: sqr1 is constexpr\n  list<sqr2(X)> mylist2;  // wrong: sqr2 is not constexpr\n\
    }\n\n\nWhen can I / should I use both, const and constexpr together?\nA. In object\
    \ declarations. This is never necessary when both keywords refer to the same object\
    \ to be declared. constexpr implies const.\nconstexpr const int N = 5;\n\nis the\
    \ same as\nconstexpr int N = 5;\n\nHowever, note that there may be situations\
    \ when the keywords each refer to different parts of the declaration:\nstatic\
    \ constexpr int N = 3;\n\nint main()\n{\n  constexpr const int *NP = &N;\n}\n\n\
    Here, NP is declared as an address constant-expression, i.e. an pointer that is\
    \ itself a constant expression. (This is possible when the address is generated\
    \ by applying the address operator to a static/global constant expression.) Here,\
    \ both constexpr and const are required: constexpr always refers to the expression\
    \ being declared (here NP), while const refers to int (it declares a pointer-to-const).\
    \ Removing the const would render the expression illegal (because (a) a pointer\
    \ to a non-const object cannot be a constant expression, and (b) &N is in-fact\
    \ a pointer-to-constant).\nB. In member function declarations. In C++11, constexpr\
    \ implies const, while in C++14 and C++17 that is not the case. A member function\
    \ declared under C++11 as\nconstexpr void f();\n\nneeds to be declared as\nconstexpr\
    \ void f() const;\n\nunder C++14 in order to still be usable as a const function.\n"
- - Difference between `constexpr` and `const`
  - "\nconst applies for variables, and prevents them from being modified in your\
    \ code. \nconstexpr tells the compiler that this expression results in a compile\
    \ time constant value, so it can be used in places like array lengths, assigning\
    \ to const variables, etc. The link given by Oli has a lot of excellent examples.\
    \ \nBasically they are 2 different concepts altogether, and can (and should) be\
    \ used together.\n"
- - Difference between `constexpr` and `const`
  - "\nOverview\n\nconst guarantees that a program does not change an objectâs value.\
    \ However, const does not guarantee which type of initialization the object undergoes.\
    \ \nConsider:\nconst int mx = numeric_limits<int>::max();  // OK: runtime initialization\n\
    \nThe function max() merely returns a literal value. However, because the initializer\
    \ is a function call, mx undergoes runtime initialization. Therefore, you cannot\
    \ use it as a constant expression:\nint arr[mx];  // error: âconstant expression\
    \ requiredâ\n\nconstexpr is a new C++11 keyword that rids you of the need to create\
    \ macros and hardcoded literals. It also guarantees, under certain conditions,\
    \ that objects undergo static initialization. It controls the evaluation time\
    \ of an expression. By enforcing compile-time evaluation of its expression, constexpr\
    \ lets you define true constant expressions that are crucial for time-critical\
    \ applications, system programming, templates, and generally speaking, in any\
    \ code that relies on compile-time constants.\n\nConstant-expression functions\n\
    A constant-expression function is a function declared constexpr. Its body must\
    \ be non-virtual and consist of a single return statement only, apart from typedefs\
    \ and static asserts. Its arguments and return value must have literal types.\
    \ It can be used with non-constant-expression arguments, but when that is done\
    \ the result is not a constant expression.\nA constant-expression function is\
    \ meant to replace macros and hardcoded literals without sacrificing performance\
    \ or type safety.\nconstexpr int max() { return INT_MAX; }           // OK\nconstexpr\
    \ long long_max() { return 2147483647; }  // OK\nconstexpr bool get_val()\n{\n\
    \    bool res = false;\n    return res;\n}  // error: body is not just a return\
    \ statement\n\nconstexpr int square(int x)\n{ return x * x; }  // OK: compile-time\
    \ evaluation only if x is a constant expression\nconst int res = square(5);  //\
    \ OK: compile-time evaluation of square(5)\nint y = getval();\nint n = square(y);\
    \          // OK: runtime evaluation of square(y)\n\nConstant-expression objects\n\
    A constant-expression object is an object declared constexpr. It must be initialized\
    \ with a constant expression or an rvalue constructed by a constant-expression\
    \ constructor with constant-expression arguments.\nA constant-expression object\
    \ behaves as if it was declared const, except that it requires initialization\
    \ before use and its initializer must be a constant expression. Consequently,\
    \ a constant-expression object can always be used as part of another constant\
    \ expression.\nstruct S\n{\n    constexpr int two();      // constant-expression\
    \ function\nprivate:\n    static constexpr int sz;  // constant-expression object\n\
    };\nconstexpr int S::sz = 256;\nenum DataPacket\n{\n    Small = S::two(),  //\
    \ error: S::two() called before it was defined\n    Big = 1024\n};\nconstexpr\
    \ int S::two() { return sz*2; }\nconstexpr S s;\nint arr[s.two()];  // OK: s.two()\
    \ called after its definition\n\nConstant-expression constructors\nA constant-expression\
    \ constructor is a constructor declared constexpr. It can have a member initialization\
    \ list but its body must be empty, apart from typedefs and static asserts. Its\
    \ arguments must have literal types.\nA constant-expression constructor allows\
    \ the compiler to initialize the object at compile-time, provided that the constructorâs\
    \ arguments are all constant expressions.\nstruct complex\n{\n    // constant-expression\
    \ constructor\n    constexpr complex(double r, double i) : re(r), im(i) { }  //\
    \ OK: empty body\n    // constant-expression functions\n    constexpr double real()\
    \ { return re; }\n    constexpr double imag() { return im; }\nprivate:\n    double\
    \ re;\n    double im;\n};\nconstexpr complex COMP(0.0, 1.0);         // creates\
    \ a literal complex\ndouble x = 1.0;\nconstexpr complex cx1(x, 0);           \
    \   // error: x is not a constant expression\nconst complex cx2(x, 1);       \
    \           // OK: runtime initialization\nconstexpr double xx = COMP.real();\
    \        // OK: compile-time initialization\nconstexpr double imaglval = COMP.imag();\
    \  // OK: compile-time initialization\ncomplex cx3(2, 4.6);                  \
    \    // OK: runtime initialization\n\nTips from the book Effective Modern C++\
    \ by Scott Meyers about constexpr:\n\nconstexpr objects are const and are initialized\
    \ with values known during compilation;\nconstexpr functions produce compile-time\
    \ results when called with arguments whose values are known during compilation;\n\
    constexpr objects and functions may be used in a wider range of contexts than\
    \ non-constexpr objects and functions;\nconstexpr is part of an objectâs or functionâs\
    \ interface.\n\nSource:\nUsing constexpr to Improve Security, Performance and\
    \ Encapsulation in C++.\n"
- - Why have header files and .cpp files? [closed]
  - "\nWell, the main reason would be for separating the interface from the implementation.\
    \  The header declares \"what\" a class (or whatever is being implemented) will\
    \ do, while the cpp file defines \"how\" it will perform those features.\nThis\
    \ reduces dependencies so that code that uses the header doesn't necessarily need\
    \ to know all the details of the implementation and any other classes/headers\
    \ needed only for that. This will reduce compilation times and also the amount\
    \ of recompilation needed when something in the implementation changes.\nIt's\
    \ not perfect, and you would usually resort to techniques like the Pimpl Idiom\
    \ to properly separate interface and implementation, but it's a good start.\n"
- - Why have header files and .cpp files? [closed]
  - "\nC++ compilation\nA compilation in C++ is done in 2 major phases:\n\nThe first\
    \ is the compilation of \"source\" text files into binary \"object\" files: The\
    \ CPP file is the compiled file and is compiled without any knowledge about the\
    \ other CPP files (or even libraries), unless fed to it through raw declaration\
    \ or header inclusion. The CPP file is usually compiled into a .OBJ or a .O \"\
    object\" file.\nThe second is the linking together of all the \"object\" files,\
    \ and thus, the creation of the final binary file (either a library or an executable).\n\
    \nWhere does the HPP fit in all this process?\nA poor lonesome CPP file...\nThe\
    \ compilation of each CPP file is independent from all other CPP files, which\
    \ means that if A.CPP needs a symbol defined in B.CPP, like:\n// A.CPP\nvoid doSomething()\n\
    {\n   doSomethingElse(); // Defined in B.CPP\n}\n\n// B.CPP\nvoid doSomethingElse()\n\
    {\n   // Etc.\n}\n\nIt won't compile because A.CPP has no way to know \"doSomethingElse\"\
    \ exists... Unless there is a declaration in A.CPP, like:\n// A.CPP\nvoid doSomethingElse()\
    \ ; // From B.CPP\n\nvoid doSomething()\n{\n   doSomethingElse() ; // Defined\
    \ in B.CPP\n}\n\nThen, if you have C.CPP which uses the same symbol, you then\
    \ copy/paste the declaration...\nCOPY/PASTE ALERT!\nYes, there is a problem. Copy/pastes\
    \ are dangerous, and difficult to maintain. Which means that it would be cool\
    \ if we had some way to NOT copy/paste, and still declare the symbol... How can\
    \ we do it? By the include of some text file, which is commonly suffixed by .h,\
    \ .hxx, .h++ or, my preferred for C++ files, .hpp:\n// B.HPP (here, we decided\
    \ to declare every symbol defined in B.CPP)\nvoid doSomethingElse() ;\n\n// A.CPP\n\
    #include \"B.HPP\"\n\nvoid doSomething()\n{\n   doSomethingElse() ; // Defined\
    \ in B.CPP\n}\n\n// B.CPP\n#include \"B.HPP\"\n\nvoid doSomethingElse()\n{\n \
    \  // Etc.\n}\n\n// C.CPP\n#include \"B.HPP\"\n\nvoid doSomethingAgain()\n{\n\
    \   doSomethingElse() ; // Defined in B.CPP\n}\n\nHow does include work?\nIncluding\
    \ a file will, in essence, parse and then copy-paste its content in the CPP file.\n\
    For example, in the following code, with the A.HPP header:\n// A.HPP\nvoid someFunction();\n\
    void someOtherFunction();\n\n... the source B.CPP:\n// B.CPP\n#include \"A.HPP\"\
    \n\nvoid doSomething()\n{\n   // Etc.\n}\n\n... will become after inclusion:\n\
    // B.CPP\nvoid someFunction();\nvoid someOtherFunction();\n\nvoid doSomething()\n\
    {\n   // Etc.\n}\n\nOne small thing - why include B.HPP in B.CPP?\nIn the current\
    \ case, this is not needed, and B.HPP has the doSomethingElse function declaration,\
    \ and B.CPP has the doSomethingElse function definition (which is, by itself a\
    \ declaration). But in a more general case, where B.HPP is used for declarations\
    \ (and inline code), there could be no corresponding definition (for example,\
    \ enums, plain structs, etc.), so the include could be needed if B.CPP uses those\
    \ declaration from B.HPP. All in all, it is \"good taste\" for a source to include\
    \ by default its header.\nConclusion\nThe header file is thus necessary, because\
    \ the C++ compiler is unable to search for symbol declarations alone, and thus,\
    \ you must help it by including those declarations.\nOne last word: You should\
    \ put header guards around the content of your HPP files, to be sure multiple\
    \ inclusions won't break anything, but all in all, I believe the main reason for\
    \ existence of HPP files is explained above.\n#ifndef B_HPP_\n#define B_HPP_\n\
    \n// The declarations in the B.hpp file\n\n#endif // B_HPP_\n\n"
- - Why have header files and .cpp files? [closed]
  - "\nBecause C, where the concept originated, is 30 years old, and back then, it\
    \ was the only viable way to link together code from multiple files.\nToday, it's\
    \ an awful hack which totally destroys compilation time in C++, causes countless\
    \ needless dependencies (because class definitions in a header file expose too\
    \ much information about the implementation), and so on.\n"
- - How do I find the length of an array?
  - "\nIf you mean a C-style array, then you can do something like:\nint a[7];\nstd::cout\
    \ << \"Length of array = \" << (sizeof(a)/sizeof(*a)) << std::endl;\n\nThis doesn't\
    \ work on pointers, though, i.e. it won't work for either of the following:\n\
    int *p = new int[7];\nstd::cout << \"Length of array = \" << (sizeof(p)/sizeof(*p))\
    \ << std::endl;\n\nor:\nvoid func(int *p)\n{\n    std::cout << \"Length of array\
    \ = \" << (sizeof(p)/sizeof(*p)) << std::endl;\n}\n\nint a[7];\nfunc(a);\n\nIn\
    \ C++, if you want this kind of behaviour, then you should be using a container\
    \ class; probably std::vector.\n"
- - How do I find the length of an array?
  - "\nAs other's said you can use the sizeof(arr)/sizeof(*arr) but this will give\
    \ you the wrong answer for pointer types that aren't arrays.\ntemplate<class T,\
    \ size_t N>\nconstexpr size_t size(T (&)[N]) { return N; }\n\nThis has the nice\
    \ property of failing to compile for non array types (visual studio has _countof\
    \ which does this). The constexpr makes this a compile time expression so it doesn't\
    \ have any drawbacks over the macro (at least none I know of).\nYou can also consider\
    \ using std::array from C++11 which exposes its length with no overhead over a\
    \ native C array.\nC++17 has std::size() in the <iterator> header which does the\
    \ same and works for STL containers too (thanks to @Jon C).\n"
- - How do I find the length of an array?
  - "\nDoing sizeof( myArray ) will get you the total number of bytes allocated for\
    \ that array. You can then find out the number of elements in the array by dividing\
    \ by the size of one element in the array: sizeof( myArray[0] )\n"
- - What does âdereferencingâ a pointer mean?
  - "\nReviewing the basic terminology\nIt's usually good enough - unless you're programming\
    \ assembly - to envisage a pointer containing a numeric memory address, with 1\
    \ referring to the second byte in the process's memory, 2 the third, 3 the fourth\
    \ and so on....\n\nWhat happened to 0 and the first byte?  Well, we'll get to\
    \ that later - see null pointers below.\nFor a more accurate definition of what\
    \ pointers store, and how memory and addresses relate, see \"More about memory\
    \ addresses, and why you probably don't need to know\".\n\nWhen you want to access\
    \ the data/value in the memory that the pointer points to - the contents of the\
    \ address with that numerical index - then you dereference the pointer.\nDifferent\
    \ computer languages have different notations to tell the compiler or interpreter\
    \ that you're now interested in the pointed-to value - I focus below on C and\
    \ C++.\nA pointer scenario\nConsider in C, given a pointer such as p below...\n\
    const char* p = \"abc\";\n\n...four bytes with the numerical values used to encode\
    \ the letters 'a', 'b', 'c', and a 0 byte to denote the end of the textual data,\
    \ are stored somewhere in memory and the numerical address of that data is stored\
    \ in p.\nFor example, if the string literal happened to be at address 0x1000 and\
    \ p a 32-bit pointer at 0x2000, the memory content would be:\nMemory Address (hex)\
    \    Variable name    Contents\n1000                                     'a' ==\
    \ 97 (ASCII)\n1001                                     'b' == 98\n1002       \
    \                              'c' == 99\n1003                               \
    \      0\n...\n2000-2003               p                1000 hex\n\nNote that\
    \ there is no variable name/identifier for address 0x1000, but we can indirectly\
    \ refer to the string literal using a pointer storing its address: p.\nDereferencing\
    \ the pointer\nTo refer to the characters p points to, we dereference p using\
    \ one of these notations (again, for C):\nassert(*p == 'a');  // The first character\
    \ at address p will be 'a'\nassert(p[1] == 'b'); // p[1] actually dereferences\
    \ a pointer created by adding\n                     // p and 1 times the size\
    \ of the things to which p points:\n                     // In this case they're\
    \ char which are 1 byte in C...\nassert(*(p + 1) == 'b');  // Another notation\
    \ for p[1]\n\nYou can also move pointers through the pointed-to data, dereferencing\
    \ them as you go:\n++p;  // Increment p so it's now 0x1001\nassert(*p == 'b');\
    \  // p == 0x1001 which is where the 'b' is...\n\nIf you have some data that can\
    \ be written to, then you can do things like this:\nint x = 2;\nint* p_x = &x;\
    \  // Put the address of the x variable into the pointer p_x\n*p_x = 4;      \
    \ // Change the memory at the address in p_x to be 4\nassert(x == 4); // Check\
    \ x is now 4\n\nAbove, you must have known at compile time that you would need\
    \ a variable called x, and the code asks the compiler to arrange where it should\
    \ be stored, ensuring the address will be available via &x.\nDereferencing and\
    \ accessing a structure data member\nIn C, if you have a variable that is a pointer\
    \ to a structure with data members, you can access those members using the ->\
    \ dereferencing operator:\ntypedef struct X { int i_; double d_; } X;\nX x;\n\
    X* p = &x;\np->d_ = 3.14159;  // Dereference and access data member x.d_\n(*p).d_\
    \ *= -1;    // Another equivalent notation for accessing x.d_\n\nMulti-byte data\
    \ types\nTo use a pointer, a computer program also needs some insight into the\
    \ type of data that is being pointed at - if that data type needs more than one\
    \ byte to represent, then the pointer normally points to the lowest-numbered byte\
    \ in the data.\nSo, looking at a slightly more complex example:\ndouble sizes[]\
    \ = { 10.3, 13.4, 11.2, 19.4 };\ndouble* p = sizes;\nassert(p[0] == 10.3);  //\
    \ Knows to look at all the bytes in the first double value\nassert(p[1] == 13.4);\
    \  // Actually looks at bytes from address p + 1 * sizeof(double)\n          \
    \             // (sizeof(double) is almost always eight bytes)\nassert(++p); \
    \          // Advance p by sizeof(double)\nassert(*p == 13.4);    // The double\
    \ at memory beginning at address p has value 13.4\n*(p + 2) = 29.8;       // Change\
    \ sizes[3] from 19.4 to 29.8\n                       // Note: earlier ++p and\
    \ + 2 here => sizes[3]\n\nPointers to dynamically allocated memory\nSometimes\
    \ you don't know how much memory you'll need until your program is running and\
    \ sees what data is thrown at it... then you can dynamically allocate memory using\
    \ malloc. It is common practice to store the address in a pointer...\nint* p =\
    \ malloc(sizeof(int)); // Get some memory somewhere...\n*p = 10;            //\
    \ Dereference the pointer to the memory, then write a value in\nfn(*p);      \
    \       // Call a function, passing it the value at address p\n(*p) += 3;    \
    \      // Change the value, adding 3 to it\nfree(p);            // Release the\
    \ memory back to the heap allocation library\n\nIn C++, memory allocation is normally\
    \ done with the new operator, and deallocation with delete:\nint* p = new int(10);\
    \ // Memory for one int with initial value 10\ndelete p;\n\np = new int[10]; \
    \     // Memory for ten ints with unspecified initial value\ndelete[] p;\n\np\
    \ = new int[10]();    // Memory for ten ints that are value initialised (to 0)\n\
    delete[] p;\n\nSee also C++ smart pointers below.\nLosing and leaking addresses\n\
    Often a pointer may be the only indication of where some data or buffer exists\
    \ in memory. If ongoing use of that data/buffer is needed, or the ability to call\
    \ free() or delete to avoid leaking the memory, then the programmer must operate\
    \ on a copy of the pointer...\nconst char* p = asprintf(\"name: %s\", name); \
    \ // Common but non-Standard printf-on-heap\n\n// Replace non-printable characters\
    \ with underscores....\nfor (const char* q = p; *q; ++q)\n    if (!isprint(*q))\n\
    \        *q = '_';\n\nprintf(\"%s\\n\", p); // Only q was modified\nfree(p);\n\
    \n...or carefully orchestrate reversal of any changes...\nconst size_t n = ...;\n\
    p += n;\n...\np -= n;  // Restore earlier value...\n\nC++ smart pointers\nIn C++,\
    \ it's best practice to use smart pointer objects to store and manage the pointers,\
    \ automatically deallocating them when the smart pointers' destructors run. Since\
    \ C++11 the Standard Library provides two, unique_ptr for when there's a single\
    \ owner for an allocated object...\n{\n    std::unique_ptr<T> p{new T(42, \"meaning\"\
    )};\n    call_a_function(p);\n    // The function above might throw, so delete\
    \ here is unreliable, but...\n} // p's destructor's guaranteed to run \"here\"\
    , calling delete\n\n...and shared_ptr for share ownership (using reference counting)...\n\
    {\n    std::shared_ptr<T> p(new T(3.14, \"pi\"));\n    number_storage.may_add(p);\
    \ // Might copy p into its container\n} // p's destructor will only delete the\
    \ T if number_storage didn't copy\n\nNull pointers\nIn C, NULL and 0 - and additionally\
    \ in C++ nullptr - can be used to indicate that a pointer doesn't currently hold\
    \ the memory address of a variable, and shouldn't be dereferenced or used in pointer\
    \ arithmetic. For example:\nconst char* p_filename = NULL; // Or \"= 0\", or \"\
    = nullptr\" in C++\nchar c;\nwhile ((c = getopt(argc, argv, \"f:\")) != EOF)\n\
    \    switch (c) {\n      case f: p_filename = optarg; break;\n    }\nif (p_filename)\
    \  // Only NULL converts to false\n    ...   // Only get here if -f flag specified\n\
    \nIn C and C++, just as inbuilt numeric types don't necessarily default to 0,\
    \ nor bools to false, pointers are not always set to NULL. All these are set to\
    \ 0/false/NULL when they're static variables or (C++ only) direct or indirect\
    \ member variables of static objects or their bases, or undergo zero initialisation\
    \ (e.g. new T(); and new T(x, y, z); perform zero-initialisation on T's members\
    \ including pointers, whereas new T; does not).\nFurther, when you assign 0, NULL\
    \ and nullptr to a pointer the bits in the pointer are not necessarily all reset:\
    \ the pointer may not contain \"0\" at the hardware level, or refer to address\
    \ 0 in your virtual address space. The compiler is allowed to store something\
    \ else there if it has reason to, but whatever it does - if you come along and\
    \ compare the pointer to 0, NULL, nullptr or another pointer that was assigned\
    \ any of those, the comparison must work as expected. So, below the source code\
    \ at the compiler level, \"NULL\" is potentially a bit \"magical\" in the C and\
    \ C++ languages...\nMore about memory addresses, and why you probably don't need\
    \ to know\nMore strictly, initialised pointers store a bit-pattern identifying\
    \ either NULL or a (often virtual) memory address.\nThe simple case is where this\
    \ is a numeric offset into the process's entire virtual address space; in more\
    \ complex cases the pointer may be relative to some specific memory area, which\
    \ the CPU may select based on CPU \"segment\" registers or some manner of segment\
    \ id encoded in the bit-pattern, and/or looking in different places depending\
    \ on the machine code instructions using the address.\nFor example, an int* properly\
    \ initialised to point to an int variable might - after casting to a float* -\
    \ access a value in \"GPU\" memory quite distinct from the int variable, then\
    \ once cast to a function pointer might refer to distinct memory holding the machine\
    \ opcodes for the function.\n3GL programming languages like C and C++ tend to\
    \ hide this complexity, such that:\n\nIf the compiler gives you a pointer to a\
    \ variable or function, you can dereference it freely (as long as the variable's\
    \ not destructed/deallocated meanwhile) and it's the compiler's problem whether\
    \ e.g. a particular CPU register needs to be restored beforehand, or a distinct\
    \ machine code instruction used\nIf you get a pointer to an element in an array,\
    \ you can use pointer arithmetic to move anywhere else in the array, or even to\
    \ form an address one-past-the-end of the array that's legal to compare with other\
    \ pointers to elements in the array (or that have similarly been moved by pointer\
    \ arithmetic to the same one-past-the-end value); again in C and C++, it's up\
    \ to the compiler to ensure this \"just works\"\nSpecific OS functions, e.g. shared\
    \ memory mapping, may give you pointers, and they'll \"just work\" within the\
    \ range of addresses that makes sense for them\nAttempts to move legal pointers\
    \ beyond these boundaries, or to cast arbitrary numbers to pointers, or use pointers\
    \ cast to unrelated types, typically have undefined behaviour, so should be avoided\
    \ in higher level libraries and applications, but code for OSes, device drivers,\
    \ etc. may need to rely on behaviour left undefined by C or C++, that is nevertheless\
    \ well defined by their specific hardware.\n\n"
- - What does âdereferencingâ a pointer mean?
  - "\nDereferencing a pointer means getting the value that is stored in the memory\
    \ location pointed by the pointer. The operator * is used to do this, and is called\
    \ the dereferencing operator.\nint a = 10;\nint* ptr = &a;\n\nprintf(\"%d\", *ptr);\
    \ // With *ptr I'm dereferencing the pointer. \n                    // Which means,\
    \ I am asking the value pointed at by the pointer.\n                    // ptr\
    \ is pointing to the location in memory of the variable a.\n                 \
    \   // In a's location, we have 10. So, dereferencing gives this value.\n\n//\
    \ Since we have indirect control over a's location, we can modify its content\
    \ using the pointer. This is an indirect way to access a.\n\n *ptr = 20;     \
    \    // Now a's content is no longer 10, and has been modified to 20.\n\n"
- - What does âdereferencingâ a pointer mean?
  - "\nA pointer is a \"reference\" to a value.. much like a library call number is\
    \ a reference to a book. \"Dereferencing\" the call number is physically going\
    \ through and retrieving that book.\nint a=4 ;\nint *pA = &a ;\nprintf( \"The\
    \ REFERENCE/call number for the variable `a` is %p\\n\", pA ) ;\n\n// The * causes\
    \ pA to DEREFERENCE...  `a` via \"callnumber\" `pA`.\nprintf( \"%d\\n\", *pA )\
    \ ; // prints 4.. \n\nIf the book isn't there, the librarian starts shouting,\
    \ shuts the library down, and a couple of people are set to investigate the cause\
    \ of a person going to find a book that isn't there.\n"
- - How do you clear a stringstream variable?
  - "\nFor all the standard library types the member function empty() is a query,\
    \ not a command, i.e. it means \"are you empty?\" not \"please throw away your\
    \ contents\".\nThe clear() member function is inherited from ios and is used to\
    \ clear the error state of the stream, e.g. if a file stream has the error state\
    \ set to eofbit (end-of-file), then calling clear() will set the error state back\
    \ to goodbit (no error).\nFor clearing the contents of a stringstream, using:\n\
    m.str(\"\");\n\nis correct, although using:\nm.str(std::string());\n\nis technically\
    \ more efficient, because you avoid invoking the std::string constructor that\
    \ takes const char*. But any compiler these days should be able to generate the\
    \ same code in both cases - so I would just go with whatever is more readable.\n"
- - How do you clear a stringstream variable?
  - "\nYou can clear the error state and empty the strignstream all in one line\n\
    std::stringstream().swap(m); // swap m with a default constructed stringstream\n\
    \nThis effectively resets m to a default constructed state\n"
- - How do you clear a stringstream variable?
  - "\nm.str(\"\");\n\nseems to work.\n"
- - What does int argc, char *argv[] mean?
  - "\nargv and argc are how command line arguments are passed to main() in C and\
    \ C++.\nargc will be the number of strings pointed to by argv.  This will (in\
    \ practice) be 1 plus the number of arguments, as virtually all implementations\
    \ will prepend the name of the program to the array.\nThe variables are named\
    \ argc (argument count) and argv (argument vector) by convention, but they can\
    \ be given any valid identifier: int main(int num_args, char** arg_strings) is\
    \ equally valid.\nThey can also be omitted entirely, yielding int main(), if you\
    \ do not intend to process command line arguments.\nTry the following program:\n\
    #include <iostream>\n\nint main(int argc, char** argv) {\n    std::cout << \"\
    Have \" << argc << \" arguments:\" << std::endl;\n    for (int i = 0; i < argc;\
    \ ++i) {\n        std::cout << argv[i] << std::endl;\n    }\n}\n\nRunning it with\
    \ ./test a1 b2 c3 will output\nHave 4 arguments:\n./test\na1\nb2\nc3\n\n"
- - What does int argc, char *argv[] mean?
  - "\nargc is the number of arguments being passed into your program from the command\
    \ line and argv is the array of arguments.\nYou can loop through the arguments\
    \ knowing the number of them like:\nfor(int i = 0; i < argc; i++)\n{\n    // argv[i]\
    \ is the argument at index i\n}\n\n"
- - What does int argc, char *argv[] mean?
  - "\nSuppose you run your program thus (using sh syntax):\n\nmyprog arg1 arg2 'arg\
    \ 3'\n\nIf you declared your main as int main(int argc, char *argv[]), then (in\
    \ most environments), your main() will be called as if like:\np = { \"myprog\"\
    , \"arg1\", \"arg2\", \"arg 3\", NULL };\nexit(main(4, p));\n\nHowever, if you\
    \ declared your main as int main(), it will be called something like\nexit(main());\n\
    \nand you don't get the arguments passed.\nTwo additional things to note:\n\n\
    These are the only two standard-mandated signatures for main.  If a particular\
    \ platform accepts extra arguments or a different return type, then that's an\
    \ extension and should not be relied upon in a portable program.\n*argv[] and\
    \ **argv are exactly equivalent, so you can write int main(int argc, char *argv[])\
    \ as int main(int argc, char **argv).\n\n"
- - Calling C/C++ from Python?
  - "\nYou should have a look at Boost.Python. Here is the short introduction taken\
    \ from their website:\n\nThe Boost Python Library is a framework for interfacing\
    \ Python and\n  C++. It allows you to quickly and seamlessly expose C++ classes\n\
    \  functions and objects to Python, and vice-versa, using no special\n  tools\
    \ -- just your C++ compiler. It is designed to wrap C++ interfaces\n  non-intrusively,\
    \ so that you should not have to change the C++ code at\n  all in order to wrap\
    \ it, making Boost.Python ideal for exposing\n  3rd-party libraries to Python.\
    \ The library's use of advanced\n  metaprogramming techniques simplifies its syntax\
    \ for users, so that\n  wrapping code takes on the look of a kind of declarative\
    \ interface\n  definition language (IDL).\n\n"
- - Calling C/C++ from Python?
  - "\nctypes is part of the standard library, and therefore is more stable and widely\
    \ available than swig, which always tended to give me problems.\nWith ctypes,\
    \ you need to satisfy any compile time dependency on python, and your binding\
    \ will work on any python that has ctypes, not just the one it was compiled against.\n\
    Suppose you have a simple C++ example class you want to talk to in a file called\
    \ foo.cpp:\n#include <iostream>\n\nclass Foo{\n    public:\n        void bar(){\n\
    \            std::cout << \"Hello\" << std::endl;\n        }\n};\n\nSince ctypes\
    \ can only talk to C functions, you need to provide those declaring them as extern\
    \ \"C\"\nextern \"C\" {\n    Foo* Foo_new(){ return new Foo(); }\n    void Foo_bar(Foo*\
    \ foo){ foo->bar(); }\n}\n\nNext you have to compile this to a shared library\n\
    g++ -c -fPIC foo.cpp -o foo.o\ng++ -shared -Wl,-soname,libfoo.so -o libfoo.so\
    \  foo.o\n\nAnd finally you have to write your python wrapper (e.g. in fooWrapper.py)\n\
    from ctypes import cdll\nlib = cdll.LoadLibrary('./libfoo.so')\n\nclass Foo(object):\n\
    \    def __init__(self):\n        self.obj = lib.Foo_new()\n\n    def bar(self):\n\
    \        lib.Foo_bar(self.obj)\n\nOnce you have that you can call it like\nf =\
    \ Foo()\nf.bar() #and you will see \"Hello\" on the screen\n\n"
- - Calling C/C++ from Python?
  - "\nThe quickest way to do this is using SWIG.\nExample from SWIG tutorial:\n/*\
    \ File : example.c */\nint fact(int n) {\n    if (n <= 1) return 1;\n    else\
    \ return n*fact(n-1);\n}\n\nInterface file:\n/* example.i */\n%module example\n\
    %{\n/* Put header files here or function declarations like below */\nextern int\
    \ fact(int n);\n%}\n\nextern int fact(int n);\n\nBuilding a Python module on Unix:\n\
    swig -python example.i\ngcc -fPIC -c example.c example_wrap.c -I/usr/local/include/python2.7\n\
    gcc -shared example.o example_wrap.o -o _example.so\n\nUsage:\n>>> import example\n\
    >>> example.fact(5)\n120\n\nNote that you have to have python-dev. Also in some\
    \ systems python header files will be in /usr/include/python2.7 based on the way\
    \ you have installed it.\nFrom the tutorial:\n\nSWIG is a fairly complete C++\
    \ compiler with support for nearly every language feature. This includes preprocessing,\
    \ pointers, classes, inheritance, and even C++ templates. SWIG can also be used\
    \ to package structures and classes into proxy classes in the target language\
    \ â exposing the underlying functionality in a very natural manner.\n\n"
- - How many levels of pointers can we have?
  - "\nThe C standard specifies the lower limit:\n\n5.2.4.1 Translation limits\n276\
    \ The implementation shall be able to translate and execute at least one program\
    \ that contains at least one instance of every one of the following limits: [...]\n\
    279 â 12 pointer, array, and function declarators (in any combinations) modifying\
    \ an\n      arithmetic, structure, union, or void type in a declaration\n\nThe\
    \ upper limit is implementation specific.\n"
- - How many levels of pointers can we have?
  - "\nActually, C programs commonly make use of infinite pointer indirection. One\
    \ or two static levels are common. Triple indirection is rare. But infinite is\
    \ very common.\nInfinite pointer indirection is achieved with the help of a struct,\
    \ of course, not with a direct declarator, which would be impossible. And a struct\
    \ is needed so that you can include other data in this structure at the different\
    \ levels where this can terminate.\nstruct list { struct list *next; ... };\n\n\
    now you can have list->next->next->next->...->next. This is really just multiple\
    \ pointer indirections: *(*(..(*(*(*list).next).next).next...).next).next.  And\
    \ the .next is basically a noop when it's the first member of the structure, so\
    \ we can imagine this as ***..***ptr.\nThere is really no limit on this because\
    \ the links can be traversed with a loop rather than a giant expression like this,\
    \ and moreover, the structure can easily be made circular.\nThus, in other words,\
    \ linked lists may be the ultimate example of adding another level of indirection\
    \ to solve a problem, since you're doing it dynamically with every push operation.\
    \ :)\n"
- - How many levels of pointers can we have?
  - "\nTheoretically:\nYou can have as many levels of indirections as you want.\n\
    Practically:\nOf course, nothing that consumes memory can be indefinite, there\
    \ will be limitations due to resources available on the host environment. So practically\
    \ there is a maximum limit to what an implementation can support and the implementation\
    \ shall document it appropriately. So in all such artifacts, the standard does\
    \ not specify the maximum limit, but it does specify the lower limits.\nHere's\
    \ the reference:\nC99 Standard 5.2.4.1 Translation limits:\n\nâ 12 pointer, array,\
    \ and function declarators (in any combinations) modifying an\n  arithmetic, structure,\
    \ union, or void type in a declaration.\n\nThis specifies the lower limit that\
    \ every implementation must support. Note that in a footenote the standard further\
    \ says:\n\n18)  Implementations should avoid imposing fixed translation limits\
    \ whenever possible.\n\n"
- - What is an unsigned char?
  - "\nIn C++, there are three distinct character types:\n\nchar\nsigned char\nunsigned\
    \ char\n\nIf you are using character types for text, use the unqualified char:\n\
    \nit is the type of character literals like 'a' or '0'.\nit is the type that makes\
    \ up C strings like \"abcde\"\n\nIt also works out as a number value, but it is\
    \ unspecified whether that value is treated as signed or unsigned. Beware character\
    \ comparisons through inequalities - although if you limit yourself to ASCII (0-127)\
    \ you're just about safe.\nIf you are using character types as numbers, use:\n\
    \nsigned char, which gives you at least the -127 to 127 range. (-128 to 127 is\
    \ common)\nunsigned char, which gives you at least the 0 to 255 range.\n\n\"At\
    \ least\", because the C++ standard only gives the minimum range of values that\
    \ each numeric type is required to cover. sizeof (char) is required to be 1 (i.e.\
    \ one byte), but a byte could in theory be for example 32 bits. sizeof would still\
    \ be report its size as 1 - meaning that you could have sizeof (char) == sizeof\
    \ (long) == 1.\n"
- - What is an unsigned char?
  - "\nThis is implementation dependent, as the C standard does NOT define the signed-ness\
    \ of char.  Depending on the platform, char may be signed or unsigned, so you\
    \ need to explicitly ask for signed char or unsigned char if your implementation\
    \ depends on it.  Just use char if you intend to represent characters from strings,\
    \ as this will match what your platform puts in the string.\nThe difference between\
    \ signed char and unsigned char is as you'd expect.  On most platforms, signed\
    \ char will be an 8-bit two's complement number ranging from -128 to 127, and\
    \ unsigned char will be an 8-bit unsigned integer (0 to 255).  Note the standard\
    \ does NOT require that char types have 8 bits, only that sizeof(char) return\
    \ 1.  You can get at the number of bits in a char with CHAR_BIT in limits.h. \
    \ There are few if any platforms today where this will be something other than\
    \ 8, though.\nThere is a nice summary of this issue here.\nAs others have mentioned\
    \ since I posted this, you're better off using int8_t and uint8_t if you really\
    \ want to represent small integers.\n"
- - What is an unsigned char?
  - "\nBecause i feel it's really called for, i just want to state some rules of C\
    \ and C++ (they are the same in this regard). First, all bits of unsigned char\
    \ participate in determining the value if any unsigned char object. Second, unsigned\
    \ char is explicitly stated unsigned.\nNow, i had a discussion with someone about\
    \ what happens when you convert the value -1 of type int to unsigned char. He\
    \ refused the idea that the resulting unsigned char has all its bits set to 1,\
    \ because he was worried about sign representation. But he don't have to. It's\
    \ immediately following out of this rule that the conversion does what is intended:\n\
    \nIf the new type is unsigned, the value is converted by repeatedly adding or\n\
    \  subtracting one more than the maximum value that can be represented in the\
    \ new type\n  until the value is in the range of the new type. (6.3.1.3p2 in a\
    \ C99 draft)\n\nThat's a mathematical description. C++ describes it in terms of\
    \ modulo calculus, which yields to the same rule. Anyway, what is not guaranteed\
    \ is that all bits in the integer -1 are one before the conversion. So, what do\
    \ we have so we can claim that the resulting unsigned char has all its CHAR_BIT\
    \ bits turned to 1?\n\nAll bits participate in determining its value - that is,\
    \ no padding bits occur in the object. \nAdding only one time UCHAR_MAX+1 to -1\
    \ will yield a value in range, namely UCHAR_MAX\n\nThat's enough, actually! So\
    \ whenever you want to have an unsigned char having all its bits one, you do\n\
    unsigned char c = (unsigned char)-1;\n\nIt also follows that a conversion is not\
    \ just truncating higher order bits. The fortunate event for two's complement\
    \ is that it is just a truncation there, but the same isn't necessarily true for\
    \ other sign representations. \n"
- - 'Iteration over std::vector: unsigned vs signed index variable'
  - "\nIterating Backwards\nSee this answer. \nIterating Forwards\nThis is almost\
    \ identical. Just change the iterators / swap decrement by increment. You should\
    \ prefer iterators. Some people tell you to use std::size_t as the index variable\
    \ type. However, that is not portable. Always use the size_type typedef of the\
    \ container (While you could get away with only a conversion in the forward iterating\
    \ case, it could actually go wrong all the way in the backward iterating case\
    \ when using std::size_t, in case std::size_t is wider than what is the typedef\
    \ of size_type):\nUsing std::vector\nUsing iterators\nfor(std::vector<T>::iterator\
    \ it = v.begin(); it != v.end(); ++it) {\n    /* std::cout << *it; ... */\n}\n\
    \nImportant is, always use the prefix increment form for iterators whose definitions\
    \ you don't know. That will ensure your code runs as generic as possible. \nUsing\
    \ Range C++11\nfor(auto const& value: a) {\n     /* std::cout << value; ... */\n\
    \nUsing indices\nfor(std::vector<int>::size_type i = 0; i != v.size(); i++) {\n\
    \    /* std::cout << v[i]; ... */\n}\n\nUsing arrays\nUsing iterators\nfor(element_type*\
    \ it = a; it != (a + (sizeof a / sizeof *a)); it++) {\n    /* std::cout << *it;\
    \ ... */\n}\n\nUsing Range C++11\nfor(auto const& value: a) {\n     /* std::cout\
    \ << value; ... */\n\nUsing indices\nfor(std::size_t i = 0; i != (sizeof a / sizeof\
    \ *a); i++) {\n    /* std::cout << a[i]; ... */\n}\n\nRead in the backward iterating\
    \ answer what problem the sizeof approach can yield to, though.\n"
- - 'Iteration over std::vector: unsigned vs signed index variable'
  - "\nFour years passed, Google gave me this answer. With the standard C++11 (aka\
    \ C++0x) there is actually a new pleasant way of doing this (at the price of breaking\
    \ backward compatibility): the new auto keyword. It saves you the pain of having\
    \ to explicitly specify the type of the iterator to use (repeating the vector\
    \ type again), when it is obvious (to the compiler), which type to use. With v\
    \ being your vector, you can do something like this:\nfor ( auto i = v.begin();\
    \ i != v.end(); i++ ) {\n    std::cout << *i << std::endl;\n}\n\nC++11 goes even\
    \ further and gives you a special syntax for iterating over collections like vectors.\
    \ It removes the necessity of writing things that are always the same:\nfor (\
    \ auto &i : v ) {\n    std::cout << i << std::endl;\n}\n\nTo see it in a working\
    \ program, build a file auto.cpp:\n#include <vector>\n#include <iostream>\n\n\
    int main(void) {\n    std::vector<int> v = std::vector<int>();\n    v.push_back(17);\n\
    \    v.push_back(12);\n    v.push_back(23);\n    v.push_back(42);\n    for ( auto\
    \ &i : v ) {\n        std::cout << i << std::endl;\n    }\n    return 0;\n}\n\n\
    As of writing this, when you compile this with g++, you normally need to set it\
    \ to work with the new standard by giving an extra flag:\ng++ -std=c++0x -o auto\
    \ auto.cpp\n\nNow you can run the example:\n$ ./auto\n17\n12\n23\n42\n\nPlease\
    \ note that the instructions on compiling and running are specific to gnu c++\
    \ compiler on Linux, the program should be platform (and compiler) independent.\n"
- - 'Iteration over std::vector: unsigned vs signed index variable'
  - "\nIn the specific case in your example, I'd use the STL algorithms to accomplish\
    \ this. \n#include <numeric> \n\nsum = std::accumulate( polygon.begin(), polygon.end(),\
    \ 0 );\n\nFor a more general, but still fairly simple case, I'd go with:\n#include\
    \ <boost/lambda/lambda.hpp>\n#include <boost/lambda/bind.hpp>\n\nusing namespace\
    \ boost::lambda;\nstd::for_each( polygon.begin(), polygon.end(), sum += _1 );\n\
    \n"
- - Is it possible to write a template to check for a function's existence?
  - "\nYes, with SFINAE you can check if a given class does provide a certain method.\
    \ Here's the working code:\n#include <iostream>\n\nstruct Hello\n{\n    int helloworld()\
    \ { return 0; }\n};\n\nstruct Generic {};    \n\n// SFINAE test\ntemplate <typename\
    \ T>\nclass has_helloworld\n{\n    typedef char one;\n    typedef long two;\n\n\
    \    template <typename C> static one test( typeof(&C::helloworld) ) ;\n    template\
    \ <typename C> static two test(...);    \n\npublic:\n    enum { value = sizeof(test<T>(0))\
    \ == sizeof(char) };\n};\n\nint main(int argc, char *argv[])\n{\n    std::cout\
    \ << has_helloworld<Hello>::value << std::endl;\n    std::cout << has_helloworld<Generic>::value\
    \ << std::endl;\n    return 0;\n}\n\nI've just tested it with Linux and gcc 4.1/4.3.\
    \ I don't know if it's portable to other platforms running different compilers.\
    \ \n"
- - Is it possible to write a template to check for a function's existence?
  - "\nThis question is old, but with C++11 we got a new way to check for a functions\
    \ existence (or existence of any non-type member, really), relying on SFINAE again:\n\
    template<class T>\nauto serialize_imp(std::ostream& os, T const& obj, int)\n \
    \   -> decltype(os << obj, void())\n{\n  os << obj;\n}\n\ntemplate<class T>\n\
    auto serialize_imp(std::ostream& os, T const& obj, long)\n    -> decltype(obj.stream(os),\
    \ void())\n{\n  obj.stream(os);\n}\n\ntemplate<class T>\nauto serialize(std::ostream&\
    \ os, T const& obj)\n    -> decltype(serialize_imp(os, obj, 0), void())\n{\n \
    \ serialize_imp(os, obj, 0);\n}\n\nNow onto some explanations. First thing, I\
    \ use expression SFINAE to exclude the serialize(_imp) functions from overload\
    \ resolution, if the first expression inside decltype isn't valid (aka, the function\
    \ doesn't exist).\nThe void() is used to make the return type of all those functions\
    \ void.\nThe 0 argument is used to prefer the os << obj overload if both are available\
    \ (literal 0 is of type int and as such the first overload is a better match).\n\
    \nNow, you probably want a trait to check if a function exists. Luckily, it's\
    \ easy to write that. Note, though, that you need to write a trait yourself for\
    \ every different function name you might want.\n#include <type_traits>\n\ntemplate<class>\n\
    struct sfinae_true : std::true_type{};\n\nnamespace detail{\n  template<class\
    \ T, class A0>\n  static auto test_stream(int)\n      -> sfinae_true<decltype(std::declval<T>().stream(std::declval<A0>()))>;\n\
    \  template<class, class A0>\n  static auto test_stream(long) -> std::false_type;\n\
    } // detail::\n\ntemplate<class T, class Arg>\nstruct has_stream : decltype(detail::test_stream<T,\
    \ Arg>(0)){};\n\nLive example.\nAnd on to explanations. First, sfinae_true is\
    \ a helper type, and it basically amounts to the same as writing decltype(void(std::declval<T>().stream(a0)),\
    \ std::true_type{}). The advantage is simply that it's shorter.\nNext, the struct\
    \ has_stream : decltype(...) inherits from either std::true_type or std::false_type\
    \ in the end, depending on whether the decltype check in test_stream fails or\
    \ not.\nLast, std::declval gives you a \"value\" of whatever type you pass, without\
    \ you needing to know how you can construct it. Note that this is only possible\
    \ inside an unevaluated context, such as decltype, sizeof and others.\n\nNote\
    \ that decltype is not necessarily needed, as sizeof (and all unevaluated contexts)\
    \ got that enhancement. It's just that decltype already delivers a type and as\
    \ such is just cleaner. Here's a sizeof version of one of the overloads:\ntemplate<class\
    \ T>\nvoid serialize_imp(std::ostream& os, T const& obj, int,\n    int(*)[sizeof((os\
    \ << obj),0)] = 0)\n{\n  os << obj;\n}\n\nThe int and long parameters are still\
    \ there for the same reason. The array pointer is used to provide a context where\
    \ sizeof can be used.\n"
- - Is it possible to write a template to check for a function's existence?
  - "\nC++ allows SFINAE to be used for this (notice that with C++11 features this\
    \ is simplier because it supports extended SFINAE on nearly arbitrary expressions\
    \ - the below was crafted to work with common C++03 compilers):\n#define HAS_MEM_FUNC(func,\
    \ name)                                        \\\n    template<typename T, typename\
    \ Sign>                                 \\\n    struct name {                \
    \                                       \\\n        typedef char yes[1];     \
    \                                       \\\n        typedef char no [2];     \
    \                                       \\\n        template <typename U, U> struct\
    \ type_check;                     \\\n        template <typename _1> static yes\
    \ &chk(type_check<Sign, &_1::func > *); \\\n        template <typename   > static\
    \ no  &chk(...);                    \\\n        static bool const value = sizeof(chk<T>(0))\
    \ == sizeof(yes);     \\\n    }\n\nthe above template and macro tries to instantiate\
    \ a template, giving it a member function pointer type, and the actual member\
    \ function pointer. If the types to not fit, SFINAE causes the template to be\
    \ ignored. Usage like this:\nHAS_MEM_FUNC(toString, has_to_string);\n\ntemplate<typename\
    \ T> void\ndoSomething() {\n   if(has_to_string<T, std::string(T::*)()>::value)\
    \ {\n      ...\n   } else {\n      ...\n   }\n}\n\nBut note that you cannot just\
    \ call that toString function in that if branch. since the compiler will check\
    \ for validity in both branches, that would fail for cases the function doesnt\
    \ exist. One way is to use SFINAE once again (enable_if can be gotten from boost\
    \ too):\ntemplate<bool C, typename T = void>\nstruct enable_if {\n  typedef T\
    \ type;\n};\n\ntemplate<typename T>\nstruct enable_if<false, T> { };\n\nHAS_MEM_FUNC(toString,\
    \ has_to_string);\n\ntemplate<typename T> \ntypename enable_if<has_to_string<T,\
    \ \n                   std::string(T::*)()>::value, std::string>::type\ndoSomething(T\
    \ * t) {\n   /* something when T has toString ... */\n   return t->toString();\n\
    }\n\ntemplate<typename T> \ntypename enable_if<!has_to_string<T, \n          \
    \         std::string(T::*)()>::value, std::string>::type\ndoSomething(T * t)\
    \ {\n   /* something when T doesnt have toString ... */\n   return \"T::toString()\
    \ does not exist.\";\n}\n\nHave fun using it. The advantage of it is that it also\
    \ works for overloaded member functions, and also for const member functions (remember\
    \ using  std::string(T::*)() const as the member function pointer type then!).\
    \ \n"
- - How to replace all occurrences of a character in string?
  - "\nstd::string doesn't contain such function but you could use stand-alone replace\
    \ function from algorithm header.\n#include <algorithm>\n#include <string>\n\n\
    void some_func() {\n  std::string s = \"example string\";\n  std::replace( s.begin(),\
    \ s.end(), 'x', 'y'); // replace all 'x' to 'y'\n}\n\n"
- - How to replace all occurrences of a character in string?
  - "\nI thought I'd toss in the boost solution as well:\n#include <boost/algorithm/string/replace.hpp>\n\
    \n// in place\nstd::string in_place = \"blah#blah\";\nboost::replace_all(in_place,\
    \ \"#\", \"@\");\n\n// copy\nconst std::string input = \"blah#blah\";\nstd::string\
    \ output = boost::replace_all_copy(input, \"#\", \"@\");\n\n"
- - How to replace all occurrences of a character in string?
  - "\nThe question is centered on character replacement, but, as I found this page\
    \ very useful (especially Konrad's remark), I'd like to share this more generalized\
    \ implementation, which allows to deal with substrings as well:\nstd::string ReplaceAll(std::string\
    \ str, const std::string& from, const std::string& to) {\n    size_t start_pos\
    \ = 0;\n    while((start_pos = str.find(from, start_pos)) != std::string::npos)\
    \ {\n        str.replace(start_pos, from.length(), to);\n        start_pos +=\
    \ to.length(); // Handles case where 'to' is a substring of 'from'\n    }\n  \
    \  return str;\n}\n\nUsage:\nstd::cout << ReplaceAll(string(\"Number Of Beans\"\
    ), std::string(\" \"), std::string(\"_\")) << std::endl;\nstd::cout << ReplaceAll(string(\"\
    ghghjghugtghty\"), std::string(\"gh\"), std::string(\"X\")) << std::endl;\nstd::cout\
    \ << ReplaceAll(string(\"ghghjghugtghty\"), std::string(\"gh\"), std::string(\"\
    h\")) << std::endl;\n\nOutputs:\n\nNumber_Of_Beans\nXXjXugtXty\nhhjhugthty\n\n\
    \nEDIT:\nThe above can be implemented in a more suitable way, in case performances\
    \ are of your concern, by returning nothing (void) and performing the changes\
    \ directly on the string str given as argument, passed by address instead of by\
    \ value. This would avoid useless and costly copy of the original string, while\
    \ returning the result. Your call, then...\nCode :\nstatic inline void ReplaceAll2(std::string\
    \ &str, const std::string& from, const std::string& to)\n{\n    // Same inner\
    \ code...\n    // No return statement\n}\n\n\nHope this will be helpful for some\
    \ others...\n"
- - Storing C++ template function definitions in a .CPP file
  - "\nThe problem you describe can be solved by defining the template in the header,\
    \ or via the approach you describe above.\nI recommend reading the following points\
    \ from the C++ FAQ Lite: \n\nWhy canât I separate the definition of my templates\
    \ class from its declaration and put it inside a .cpp file?\nHow can I avoid linker\
    \ errors with my template functions?\nHow does the C++ keyword export help with\
    \ template linker errors?\n\nThey go into a lot of detail about these (and other)\
    \ template issues.\n"
- - Storing C++ template function definitions in a .CPP file
  - "\nFor others on this page wondering what the correct syntax is (as did I) for\
    \ explicit template specialisation (or at least in VS2008), its the following...\n\
    In your .h file...\ntemplate<typename T>\nclass foo\n{\npublic:\n    void bar(const\
    \ T &t);\n};\n\nAnd in your .cpp file\ntemplate <class T>\nvoid foo<T>::bar(const\
    \ T &t)\n{ }\n\n// Explicit template instantiation\ntemplate class foo<int>;\n\
    \n"
- - Storing C++ template function definitions in a .CPP file
  - "\nThis code is well-formed. You only have to pay attention that the definition\
    \ of the template is visible at the point of instantiation. To quote the standard,\
    \ Â§ 14.7.2.4:\n\nThe definition of a non-exported function template, a non-exported\
    \ member function template, or a non-exported member function or static data member\
    \ of a class template shall be present in every translation unit in which it is\
    \ explicitly instantiated.\n\n"
- - Programmatically find the number of cores on a machine
  - "\nC++11\n//may return 0 when not able to detect\nunsigned concurentThreadsSupported\
    \ = std::thread::hardware_concurrency();\n\nReference: std::thread::hardware_concurrency\n\
    \nIn C++ prior to C++11, there's no portable way.  Instead, you'll need to use\
    \ one or more of the following methods (guarded by appropriate #ifdef lines):\n\
    \nWin32\nSYSTEM_INFO sysinfo;\nGetSystemInfo(&sysinfo);\nint numCPU = sysinfo.dwNumberOfProcessors;\n\
    \nLinux, Solaris, AIX and Mac OS X >=10.4 (i.e. Tiger onwards)\nint numCPU = sysconf(_SC_NPROCESSORS_ONLN);\n\
    \nFreeBSD, MacOS X, NetBSD, OpenBSD, etc.\nint mib[4];\nint numCPU;\nstd::size_t\
    \ len = sizeof(numCPU); \n\n/* set the mib for hw.ncpu */\nmib[0] = CTL_HW;\n\
    mib[1] = HW_AVAILCPU;  // alternatively, try HW_NCPU;\n\n/* get the number of\
    \ CPUs from the system */\nsysctl(mib, 2, &numCPU, &len, NULL, 0);\n\nif (numCPU\
    \ < 1) \n{\n    mib[1] = HW_NCPU;\n    sysctl(mib, 2, &numCPU, &len, NULL, 0);\n\
    \    if (numCPU < 1)\n        numCPU = 1;\n}\n\nHPUX\nint numCPU = mpctl(MPC_GETNUMSPUS,\
    \ NULL, NULL);\n\nIRIX\nint numCPU = sysconf(_SC_NPROC_ONLN);\n\nObjective-C (Mac\
    \ OS X >=10.5 or iOS)\nNSUInteger a = [[NSProcessInfo processInfo] processorCount];\n\
    NSUInteger b = [[NSProcessInfo processInfo] activeProcessorCount];\n\n\n"
- - Programmatically find the number of cores on a machine
  - "\nThis functionality is part of the C++11 standard.\n#include <thread>\n\nunsigned\
    \ int nthreads = std::thread::hardware_concurrency();\n\nFor older compilers,\
    \ you can use the Boost.Thread library.\n#include <boost/thread.hpp>\n\nunsigned\
    \ int nthreads = boost::thread::hardware_concurrency();\n\nIn either case, hardware_concurrency()\
    \ returns the number of threads that the hardware is capable of executing concurrently\
    \ based on the number of CPU cores and hyper-threading units.\n"
- - Programmatically find the number of cores on a machine
  - "\nOpenMP is supported on many platforms (including Visual Studio 2005) and it\
    \ offers a \nint omp_get_num_procs();\n\nfunction that returns the number of processors/cores\
    \ available at the time of call.\n"
- - In what cases do I use malloc vs new?
  - "\nUnless you are forced to use C, you should never use malloc.  Always use new.\n\
    If you need a big chunk of data just do something like:\nchar *pBuffer = new char[1024];\n\
    \nBe careful though this is not correct:\n//This is incorrect - may delete only\
    \ one element, may corrupt the heap, or worse...\ndelete pBuffer;\n\nInstead you\
    \ should do this when deleting an array of data:\n//This deletes all items in\
    \ the array\ndelete[] pBuffer;\n\nThe new keyword is the C++ way of doing it,\
    \ and it will ensure that your type will have its constructor called.  The new\
    \ keyword is also more type-safe whereas malloc is not type-safe at all. \nThe\
    \ only way I could think that would be beneficial to use malloc would be if you\
    \ needed to change the size of your buffer of data.  The new keyword does not\
    \ have an analogous way like realloc.  The realloc function might be able to extend\
    \ the size of a chunk of memory for you more efficiently. \nIt is worth mentioning\
    \ that you cannot mix new/free and malloc/delete.\nNote: Some answers in this\
    \ question are invalid. \nint* p_scalar = new int(5);  // Does not create 5 elements,\
    \ but initializes to 5\nint* p_array  = new int[5];  // Creates 5 elements\n\n"
- - In what cases do I use malloc vs new?
  - "\nThe short answer is: don't use malloc for C++ without a really good reason\
    \ for doing so. malloc has a number of deficiencies when used with C++, which\
    \ new was defined to overcome.\nDeficiencies fixed by new for C++ code\n\nmalloc\
    \ is not typesafe in any meaningful way. In C++ you are required to cast the return\
    \ from void*. This potentially introduces a lot of problems:\n#include <stdlib.h>\n\
    \nstruct foo {\n  double d[5];\n}; \n\nint main() {\n  foo *f1 = malloc(1); //\
    \ error, no cast\n  foo *f2 = static_cast<foo*>(malloc(sizeof(foo)));\n  foo *f3\
    \ = static_cast<foo*>(malloc(1)); // No error, bad\n}\n\nIt's worse than that\
    \ though. If the type in question is POD (plain old data) then you can semi-sensibly\
    \ use malloc to allocate memory for it, as f2 does in the first example.\nIt's\
    \ not so obvious though if a type is POD. The fact that it's possible for a given\
    \ type to change from POD to non-POD with no resulting compiler error and potentially\
    \ very hard to debug problems is a significant factor. For example if someone\
    \ (possibly another programmer, during maintenance, much later on were to make\
    \ a change that caused foo to no longer be POD then no obvious error would appear\
    \ at compile time as you'd hope, e.g.:\nstruct foo {\n  double d[5];\n  virtual\
    \ ~foo() { }\n};\n\nwould make the malloc of f2 also become bad, without any obvious\
    \ diagnostics. The   example here is trivial, but it's possible to accidentally\
    \ introduce non-PODness much further away (e.g. in a base class, by adding a non-POD\
    \ member). If you have C++11/boost you can use is_pod to check that this assumption\
    \ is correct and produce an error if it's not:\n#include <type_traits>\n#include\
    \ <stdlib.h>\n\nfoo *safe_foo_malloc() {\n  static_assert(std::is_pod<foo>::value,\
    \ \"foo must be POD\");\n  return static_cast<foo*>(malloc(sizeof(foo)));\n}\n\
    \nAlthough boost is unable to determine if a type is POD without C++11 or some\
    \ other compiler extensions.\nmalloc returns NULL if allocation fails. new will\
    \ throw std::bad_alloc. The behaviour of later using a NULL pointer is undefined.\
    \ An exception has clean semantics when it is thrown and it is thrown from the\
    \ source of the error. Wrapping malloc with an appropriate test at every call\
    \ seems tedious and error prone. (You only have to forget once to undo all that\
    \ good work). An exception can be allowed to propagate to a level where a caller\
    \ is able to sensibly process it, where as NULL is much harder to pass back meaningfully.\
    \ We could extend our safe_foo_malloc function to throw an exception or exit the\
    \ program or call some handler:\n#include <type_traits>\n#include <stdlib.h>\n\
    \nvoid my_malloc_failed_handler();\n\nfoo *safe_foo_malloc() {\n  static_assert(std::is_pod<foo>::value,\
    \ \"foo must be POD\");\n  foo *mem = static_cast<foo*>(malloc(sizeof(foo)));\n\
    \  if (!mem) {\n     my_malloc_failed_handler();\n     // or throw ...\n  }\n\
    \  return mem;\n}\n\nFundamentally malloc is a C feature and new is a C++ feature.\
    \ As a result malloc does not play nicely with constructors, it only looks at\
    \ allocating a chunk of bytes. We could extend our safe_foo_malloc further to\
    \ use placement new:\n#include <stdlib.h>\n#include <new>\n\nvoid my_malloc_failed_handler();\n\
    \nfoo *safe_foo_malloc() {\n  void *mem = malloc(sizeof(foo));\n  if (!mem) {\n\
    \     my_malloc_failed_handler();\n     // or throw ...\n  }\n  return new (mem)foo();\n\
    }\n\nOur safe_foo_malloc function isn't very generic - ideally we'd want something\
    \ that can handle any type, not just foo. We can achieve this with templates and\
    \ variadic templates for non-default constructors:\n#include <functional>\n#include\
    \ <new>\n#include <stdlib.h>\n\nvoid my_malloc_failed_handler();\n\ntemplate <typename\
    \ T>\nstruct alloc {\n  template <typename ...Args>\n  static T *safe_malloc(Args&&...\
    \ args) {\n    void *mem = malloc(sizeof(T));\n    if (!mem) {\n       my_malloc_failed_handler();\n\
    \       // or throw ...\n    }\n    return new (mem)T(std::forward(args)...);\n\
    \  }\n};\n\nNow though in fixing all the issues we identified so far we've practically\
    \ reinvented the default new operator. If you're going to use malloc and placement\
    \ new then you might as well just use new to begin with!\n\n"
- - In what cases do I use malloc vs new?
  - "\nFrom the C++ FQA Lite:\n\n[16.4] Why should I use new instead of\n  trustworthy\
    \ old malloc()?\nFAQ: new/delete call the\n  constructor/destructor; new is type\n\
    \  safe, malloc is not; new can be\n  overridden by a class.\nFQA: The virtues\
    \ of new mentioned by\n  the FAQ are not virtues, because\n  constructors, destructors,\
    \ and\n  operator overloading are garbage (see\n  what happens when you have no\
    \ garbage\n  collection?), and the type safety\n  issue is really tiny here (normally\n\
    \  you have to cast the void* returned by\n  malloc to the right pointer type\
    \ to\n  assign it to a typed pointer variable,\n  which may be annoying, but far\
    \ from\n  \"unsafe\").\nOh, and using trustworthy old malloc\n  makes it possible\
    \ to use the equally\n  trustworthy & old realloc. Too bad we\n  don't have a\
    \ shiny new operator renew or something.\nStill, new is not bad enough to\n  justify\
    \ a deviation from the common\n  style used throughout a language, even\n  when\
    \ the language is C++. In\n  particular, classes with non-trivial\n  constructors\
    \ will misbehave in fatal\n  ways if you simply malloc the objects.\n  So why\
    \ not use new throughout the\n  code? People rarely overload operator\n  new,\
    \ so it probably won't get in your\n  way too much. And if they do overload\n\
    \  new, you can always ask them to stop.\n\nSorry, I just couldn't resist. :)\n"
- - What is Linuxâs native GUI API?
  - "\nIn Linux the graphical user interface is not a part of the operating system.\
    \ The graphical user interface found on most Linux desktops is provided by software\
    \ called the X Window System, which defines a device independent way of dealing\
    \ with screens, keyboards and pointer devices.\nX Window defines a network protocol\
    \ for communication, and any program that knows how to \"speak\" this protocol\
    \ can use it. There is a C library called Xlib that makes it easier to use this\
    \ protocol, so Xlib is kind of the native GUI API. Xlib is not the only way to\
    \ access an X Window server; there is also XCB.\nToolkit libraries such as GTK+\
    \ (used by GNOME) and Qt (used by KDE), built on top of Xlib, are used because\
    \ they are easier to program with. For example they give you a consistent look\
    \ and feel across applications, make it easier to use drag-and-drop, provide components\
    \ standard to a modern desktop environment, and so on.\nHow X draws on the screen\
    \ internally depends on the implementation. X.org has a device independent part\
    \ and a device dependent part. The former manages screen resources such as windows,\
    \ while the latter communicates with the graphics card driver, usually a kernel\
    \ module. The communication may happen over direct memory access or through system\
    \ calls to the kernel. The driver translates the commands into a form that the\
    \ hardware on the card understands.\nAs of 2013, a new window system called Wayland\
    \ is starting to become usable, and many distributions have said they will at\
    \ some point migrate to it, though there is still no clear schedule. This system\
    \ is based on OpenGL/ES API, which means that in the future OpenGL will be the\
    \ \"native GUI API\" in Linux. Work is being done to port GTK+ and QT to Wayland,\
    \ so that current popular applications and desktop systems would need minimal\
    \ changes. The applications that cannot be ported will be supported through an\
    \ X11 server, much like OS X supports X11 apps through Xquartz. The GTK+ port\
    \ is expected to be finished within a year, while Qt 5 already has complete Wayland\
    \ support.\nTo further complicate matters, Ubuntu has announced they are developing\
    \ a new system called Mir because of problems they perceive with Wayland. This\
    \ window system is also based on the OpenGL/ES API.\n"
- - What is Linuxâs native GUI API?
  - "\nLinux is a kernel, not a full operating system.  There are different windowing\
    \ systems and gui's that run on top of Linux to provide windowing.  Typically\
    \ X11 is the windowing system used by Linux distros.\n"
- - What is Linuxâs native GUI API?
  - "\nWayland is also worth mentioning as it is mostly referred as a \"future X11\
    \ killer\".\nAlso note that Android and some other mobile operating systems don't\
    \ include X11 although they have a Linux kernel, so in that sense X11 is not native\
    \ to all Linux systems.\nBeing cross-platform has nothing to do with being native.\
    \ Cocoa has also been ported to other platforms via GNUStep but it is still native\
    \ to OS X / macOS.\n"
- - Why should I not wrap every block in âtryâ-âcatchâ?
  - "\nA method should only catch an exception when it can handle it in some sensible\
    \ way.\nOtherwise, pass it on up, in the hope that a method higher up the call\
    \ stack can make sense of it.\nAs others have noted, it is good practice to have\
    \ an unhandled exception handler (with logging) at the highest level of the call\
    \ stack to ensure that any fatal errors are logged.\n"
- - Why should I not wrap every block in âtryâ-âcatchâ?
  - "\nAs Mitch and others stated, you shouldn't catch an exception that you do not\
    \ plan on handling in some way.  You should consider how the application is going\
    \ to systematically handle exceptions when you are designing it.  This usually\
    \ leads to having layers of error handling based on the abstractions - for example,\
    \ you handle all SQL-related errors in your data access code so that the part\
    \ of the application that is interacting with domain objects is not exposed to\
    \ the fact that there is a DB under the hood somewhere.\nThere are a few related\
    \ code smells that you definitely want to avoid in addition to the \"catch everything\
    \ everywhere\" smell.\n\n\"catch, log, rethrow\": if you want scoped based logging,\
    \ then write a class that emits a log statement in its destructor when the stack\
    \ is unrolling due to an exception (ala std::uncaught_exception()).  All that\
    \ you need to do is declare a logging instance in the scope that you are interested\
    \ in and, voila, you have logging and no unnecessary try/catch logic.\n\"catch,\
    \ throw translated\": this usually points to an abstraction problem.  Unless you\
    \ are implementing a federated solution where you are translating several specific\
    \ exceptions into one more generic one, you probably have an unnecessary layer\
    \ of abstraction... and don't say that \"I might need it tomorrow\".\n\"catch,\
    \ cleanup, rethrow\": this is one of my pet-peeves.  If you see a lot of this,\
    \ then you should apply Resource Acquisition is Initialization techniques and\
    \ place the cleanup portion in the destructor of a janitor object instance.\n\n\
    I consider code that is littered with try/catch blocks to be a good target for\
    \ code review and refactoring.  It indicates that either exception handling is\
    \ not well understood or the code has become an amÅba and is in serious need of\
    \ refactoring.\n"
- - Why should I not wrap every block in âtryâ-âcatchâ?
  - "\nBecause the next question is \"I've caught an exception, what do I do next?\"\
    \ What will you do? If you do nothing - that's error hiding and the program could\
    \ \"just not work\" without any chance to find what happened. You need to understand\
    \ what exactly you will do once you've caught the exception and only catch if\
    \ you know.\n"
- - Does the C++ standard allow for an uninitialized bool to crash a program?
  - "\nYes, ISO C++ allows (but doesn't require) implementations to make this choice.\n\
    But also note that ISO C++ allows a compiler to emit code that crashes on purpose\
    \ (e.g. with an illegal instruction) if the program encounters UB, e.g. as a way\
    \ to help you find errors.  (Or because it's a DeathStation 9000.  Being strictly\
    \ conforming is not sufficient for a C++ implementation to be useful for any real\
    \ purpose).  So ISO C++ would allow a compiler to make asm that crashed (for totally\
    \ different reasons) even on similar code that read an uninitialized uint32_t.\
    \  Even though that's required to be a fixed-layout type with no trap representations.\n\
    It's an interesting question about how real implementations work, but remember\
    \ that even if the answer was different, your code would still be unsafe because\
    \ modern C++ is not a portable version of assembly language.\n\nYou're compiling\
    \ for the x86-64 System V ABI, which specifies that a bool as a function arg in\
    \ a register is represented by the bit-patterns false=0 and true=1 in the low\
    \ 8 bits of the register1.  In memory, bool is a 1-byte type that again must have\
    \ an integer value of 0 or 1.\n(An ABI is a set of implementation choices that\
    \ compilers for the same platform agree on so they can make code that calls each\
    \ other's functions, including type sizes, struct layout rules, and calling conventions.)\n\
    ISO C++ doesn't specify it, but this ABI decision is widespread because it makes\
    \ bool->int conversion cheap (just zero-extension).  I'm not aware of any ABIs\
    \ that don't let the compiler assume 0 or 1 for bool, for any architecture (not\
    \ just x86).  It allows optimizations like !mybool with xor eax,1 to flip the\
    \ low bit: Any possible code that can flip a bit/integer/bool between 0 and 1\
    \ in single CPU instruction.  Or compiling a&&b to a bitwise AND for bool types.\
    \  Some compilers do actually take advantage Boolean values as 8 bit in compilers.\
    \ Are operations on them inefficient?.\nIn general, the as-if rule allows allows\
    \ the compiler to take advantage of things that are true on the target platform\
    \ being compiled for, because the end result will be executable code that implements\
    \ the same externally-visible behaviour as the C++ source.  (With all the restrictions\
    \ that Undefined Behaviour places on what is actually \"externally visible\":\
    \ not with a debugger, but from another thread in a well-formed / legal C++ program.)\n\
    The compiler is definitely allowed to take full advantage of an ABI guarantee\
    \ in its code-gen, and make code like you found which optimizes strlen(whichString)\
    \ to\n5U - boolValue.  (BTW, this optimization is kind of clever, but maybe shortsighted\
    \ vs. branching and inlining memcpyas stores of immediate data2.)\nOr the compiler\
    \ could have created a table of pointers and indexed it with the integer value\
    \ of the bool, again assuming it was a 0 or 1.  (This possibility is what @Barmar's\
    \ answer suggested.)\n\nYour __attribute((noinline)) constructor with optimization\
    \ enabled led to clang just loading a byte from the stack to use as uninitializedBool.\
    \  It made space for the object in main with push rax (which is smaller and for\
    \ various reason about as efficient as sub rsp, 8), so whatever garbage was in\
    \ AL on entry to main is the value it used for uninitializedBool.  This is why\
    \ you actually got values that weren't just 0.\n5U - random garbage can easily\
    \ wrap to a large unsigned value, leading memcpy to go into unmapped memory. \
    \ The destination is in static storage, not the stack, so you're not overwriting\
    \ a return address or something.\n\nOther implementations could make different\
    \ choices, e.g. false=0 and true=any non-zero value.  Then clang probably wouldn't\
    \ make code that crashes for this specific instance of UB. (But it would still\
    \ be allowed to if it wanted to.)  I don't know of any implementations that choose\
    \ anything other what x86-64 does for bool, but the C++ standard allows many things\
    \ that nobody does or even would want to do on hardware that's anything like current\
    \ CPUs.\nISO C++ leaves it unspecified what you'll find when you examine or modify\
    \ the object representation of a bool.  (e.g. by memcpying the bool into unsigned\
    \ char, which you're allowed to do because char* can alias anything.  And unsigned\
    \ char is guaranteed to have no padding bits, so the C++ standard does formally\
    \ let you hexdump object representations without any UB.  Pointer-casting to copy\
    \ the object representation is different from assigning char foo = my_bool, of\
    \ course, so booleanization to 0 or 1 wouldn't happen and you'd get the raw object\
    \ representation.)\nYou've partially \"hidden\" the UB on this execution path\
    \ from the compiler with noinline.  Even if it doesn't inline, though, interprocedural\
    \ optimizations could still make a version of the function that depends on the\
    \ definition of another function.  (First, clang is making an executable, not\
    \ a Unix shared library where symbol-interposition can happen.  Second, the definition\
    \ in inside the class{} definition so all translation units must have the same\
    \ definition.  Like with the inline keyword.)\nSo a compiler could emit just a\
    \ ret or ud2 (illegal instruction) as the definition for main, because the path\
    \ of execution starting at the top of main unavoidably encounters Undefined Behaviour.\
    \ (Which the compiler can see at compile time if it decided to follow the path\
    \ through the non-inline constructor.)\nAny program that encounters UB is totally\
    \ undefined for its entire existence.  But UB inside a function or if() branch\
    \ that never actually runs doesn't corrupt the rest of the program.  In practice\
    \ that means that compilers can decide to emit an illegal instruction, or a ret,\
    \ or not emit anything and fall into the next block / function, for the whole\
    \ basic block that can be proven at compile time to contain or lead to UB.\nGCC\
    \ and Clang in practice do actually sometimes emit ud2 on UB, instead of even\
    \ trying to generate code for paths of execution that make no sense.  Or for cases\
    \ like falling off the end of a non-void function, gcc will sometimes omit a ret\
    \ instruction.  If you were thinking that \"my function will just return with\
    \ whatever garbage is in RAX\", you are sorely mistaken.  Modern C++ compilers\
    \ don't treat the language like a portable assembly language any more.  Your program\
    \ really has to be valid C++, without making assumptions about how a stand-alone\
    \ non inlined version of your function might look in asm.\nAnother fun example\
    \ is Why does unaligned access to mmap'ed memory sometimes segfault on AMD64?.\
    \  x86 doesn't fault on unaligned integers, right?  So why would a misaligned\
    \ uint16_t* be a problem?  Because alignof(uint16_t) == 2, and violating that\
    \ assumption led to a segfault when auto-vectorizing with SSE2.\nSee also What\
    \ Every C Programmer Should Know About Undefined Behavior #1/3, an article by\
    \ a clang developer.\nKey point: if the compiler noticed the UB at compile time,\
    \ it could \"break\" (emit surprising asm) the path through your code that causes\
    \ UB even if targeting an ABI where any bit-pattern is a valid object representation\
    \ for bool.\nExpect total hostility toward many mistakes by the programmer, especially\
    \ things modern compilers warn about.  This is why you should use -Wall and fix\
    \ warnings.  C++ is not a user-friendly language, and something in C++ can be\
    \ unsafe even if it would be safe in asm on the target you're compiling for. \
    \ (e.g. signed overflow is UB in C++ and compilers will assume it doesn't happen,\
    \ even when compiling for 2's complement x86, unless you use clang/gcc -fwrapv.)\n\
    Compile-time-visible UB is always dangerous, and it's really hard to be sure (with\
    \ link-time optimization) that you've really hidden UB from the compiler and can\
    \ thus reason about what kind of asm it will generate.\nNot to be over-dramatic;\
    \ often compilers do let you get away with some things and emit code like you're\
    \ expecting even when something is UB.  But maybe it will be a problem in the\
    \ future if compiler devs implement some optimization that gains more info about\
    \ value-ranges (e.g. that a variable is non-negative, maybe allowing it to optimize\
    \ sign-extension to free zero-extension on x86-64).  For example, in current gcc\
    \ and clang, doing tmp = a+INT_MIN doesn't let them optimize a<0 as always-true,\
    \ only that tmp is always negative.  (So they don't backtrack from the inputs\
    \ of a calculation to derive range info, only on the results based on the assumption\
    \ of no signed overflow: example on Godbolt.  I don't know if this is intentional\
    \ user-friendliness or simply a missed optimization.)\nAlso note that implementations\
    \ (aka compilers) are allowed to define behaviour that ISO C++ leaves undefined.\
    \  For example, all compilers that support Intel's intrinsics (like _mm_add_ps(__m128,\
    \ __m128) for manual SIMD vectorization) must allow forming mis-aligned pointers,\
    \ which is UB in C++ even if you don't dereference them.  __m128i _mm_loadu_si128(const\
    \ __m128i *) does unaligned loads by taking a misaligned __m128i* arg, not a void*\
    \ or char*.  Is `reinterpret_cast`ing between hardware vector pointer and the\
    \ corresponding type an undefined behavior?\nGNU C/C++ also defines the behaviour\
    \ of left-shifting a negative signed number (even without -fwrapv), separately\
    \ from the normal signed-overflow UB rules.  (This is UB in ISO C++, while right\
    \ shifts of signed numbers are implementation-defined (logical vs. arithmetic);\
    \ good quality implementations choose arithmetic on HW that has arithmetic right\
    \ shifts, but ISO C++ doesn't specify).  This is documented in the GCC manual's\
    \ Integer section, along with defining implementation-defined behaviour that C\
    \ standards require implementations to define one way or another.\nThere are definitely\
    \ quality-of-implementation issues that compiler developers care about; they generally\
    \ aren't trying to make compilers that are intentionally hostile, but taking advantage\
    \ of all the UB potholes in C++ (except ones they choose to define) to optimize\
    \ better can be nearly indistinguishable at times.\n\nFootnote 1: The upper 56\
    \ bits can be garbage which the callee must ignore, as usual for types narrower\
    \ than a register.\n(Other ABIs do make different choices here.  Some do require\
    \ narrow integer types to be zero- or sign-extended to fill a register when passed\
    \ to or returned from functions, like MIPS64 and PowerPC64.  See the last section\
    \ of this x86-64 answer which compares vs. those earlier ISAs.)\nFor example,\
    \ a caller might have calculated a & 0x01010101 in RDI and used it for something\
    \ else, before calling bool_func(a&1).  The caller could optimize away the &1\
    \ because it already did that to the low byte as part of and edi, 0x01010101,\
    \ and it knows the callee is required to ignore the high bytes.\nOr if a bool\
    \ is passed as the 3rd arg, maybe a caller optimizing for code-size loads it with\
    \ mov dl, [mem] instead of movzx edx, [mem], saving 1 byte at the cost of a false\
    \ dependency on the old value of RDX (or other partial-register effect, depending\
    \ on CPU model).  Or for the first arg, mov dil, byte [r10] instead of movzx edi,\
    \ byte [r10], because both require a REX prefix anyway.\nThis is why clang emits\
    \ movzx   eax, dil in Serialize, instead of sub eax, edi.  (For integer args,\
    \ clang violates this ABI rule, instead depending on the undocumented behaviour\
    \ of gcc and clang to zero- or sign-extend narrow integers to 32 bits.  Is a sign\
    \ or zero extension required when adding a 32bit offset to a pointer for the x86-64\
    \ ABI?\n So I was interested to see that it doesn't do the same thing for bool.)\n\
    \nFootnote 2:  After branching, you'd just have a 4-byte mov-immediate, or a 4-byte\
    \ + 1-byte store.  The length is implicit in the store widths + offsets.\nOTOH,\
    \ glibc memcpy will do two 4-byte loads/stores with an overlap that depends on\
    \ length, so this really does end up making the whole thing free of conditional\
    \ branches on the boolean.  See the L(between_4_7): block  in glibc's memcpy/memmove.\
    \  Or at least, go the same way for either boolean in memcpy's branching to select\
    \ a chunk size.\nIf inlining, you could use 2x mov-immediate + cmov and a conditional\
    \ offset, or you could leave the string data in memory.\nOr if tuning for Intel\
    \ Ice Lake (with the Fast Short REP MOV feature), an actual rep movsb might be\
    \ optimal.  glibc memcpy might start using rep movsb  for small sizes on CPUs\
    \ with that feature, saving a lot of branching.\n\nTools for detecting UB and\
    \ usage of uninitialized values\nIn gcc and clang, you can compile with -fsanitize=undefined\
    \ to add run-time instrumentation that will warn or error out on UB that happens\
    \ at runtime.  That won't catch unitialized variables, though.  (Because it doesn't\
    \ increase type sizes to make room for an \"uninitialized\" bit).\nSee https://developers.redhat.com/blog/2014/10/16/gcc-undefined-behavior-sanitizer-ubsan/\n\
    To find usage of uninitialized data, there's Address Sanitizer and Memory Sanitizer\
    \ in clang/LLVM. https://github.com/google/sanitizers/wiki/MemorySanitizer shows\
    \ examples of clang -fsanitize=memory -fPIE -pie detecting uninitialized memory\
    \ reads.  It might work best if you compile without optimization, so all reads\
    \ of variables end up actually loading from memory in the asm.  They show it being\
    \ used at -O2 in a case where the load wouldn't optimize away.  I haven't tried\
    \ it myself.  (In some cases, e.g. not initializing an accumulator before summing\
    \ an array, clang -O3 will emit code that sums into a vector register that it\
    \ never initialized.  So with optimization, you can have a case where there's\
    \ no memory read associated with the UB.  But -fsanitize=memory changes the generated\
    \ asm, and might result in a check for this.)\n\nIt will tolerate copying of uninitialized\
    \ memory, and also simple logic and arithmetic operations with it. In general,\
    \ MemorySanitizer silently tracks the spread of uninitialized data in memory,\
    \ and reports a warning when a code branch is taken (or not taken) depending on\
    \ an uninitialized value.\nMemorySanitizer implements a subset of functionality\
    \ found in Valgrind (Memcheck tool).\n\nIt should work for this case because the\
    \ call to glibc memcpy with a length calculated from uninitialized memory will\
    \ (inside the library) result in a branch based on length.  If it had inlined\
    \ a fully branchless version that just used cmov, indexing, and two stores, it\
    \ might not have worked.\nValgrind's memcheck will also look for this kind of\
    \ problem, again not complaining if the program simply copies around uninitialized\
    \ data.  But it says it will detect when a \"Conditional jump or move depends\
    \ on uninitialised value(s)\", to try to catch any externally-visible behaviour\
    \ that depends on uninitialized data.\nPerhaps the idea behind not flagging just\
    \ a load is that structs can have padding, and copying the whole struct (including\
    \ padding) with a wide vector load/store is not an error even if the individual\
    \ members were only written one at a time.  At the asm level, the information\
    \ about what was padding and what is actually part of the value has been lost.\n"
- - Does the C++ standard allow for an uninitialized bool to crash a program?
  - "\nThe compiler is allowed to assume that a boolean value passed as an argument\
    \ is a valid boolean value (i.e. one which has been initialised or converted to\
    \ true or false). The true value doesn't have to be the same as the integer 1\
    \ -- indeed, there can be various representations of true and false -- but the\
    \ parameter must be some valid representation of one of those two values, where\
    \ \"valid representation\" is implementation-defined.\nSo if you fail to initialise\
    \ a bool, or if you succeed in overwriting it through some pointer of a different\
    \ type, then the compiler's assumptions will be wrong and Undefined Behaviour\
    \ will ensue. You had been warned:\n\n50) Using a bool value in ways described\
    \ by this International Standard as âundefinedâ, such as by examining the value\
    \ of an uninitialized automatic object, might cause it to behave as if it is neither\
    \ true nor false. (Footnote to para 6 of Â§6.9.1, Fundamental Types)\n\n"
- - Does the C++ standard allow for an uninitialized bool to crash a program?
  - "\nThe function itself is correct, but in your test program, the statement that\
    \ calls the function causes undefined behaviour by using the value of an uninitialized\
    \ variable.  \nThe bug is in the calling function, and it could be detected by\
    \ code review or static analysis of the calling function. Using your compiler\
    \ explorer link, the gcc 8.2 compiler does detect the bug. (Maybe you could file\
    \ a bug report against clang that it doesn't find the problem).\nUndefined behaviour\
    \ means anything can happen, which includes the program crashing a few lines after\
    \ the event that triggered the undefined behaviour.\nNB. The answer to \"Can undefined\
    \ behaviour cause _____ ?\" is always \"Yes\". That's literally the definition\
    \ of undefined behaviour.\n"
- - How do I erase an element from std::vector<> by index?
  - "\nTo delete a single element, you could do:\nstd::vector<int> vec;\n\nvec.push_back(6);\n\
    vec.push_back(-17);\nvec.push_back(12);\n\n// Deletes the second element (vec[1])\n\
    vec.erase(vec.begin() + 1);\n\nOr, to delete more than one element at once:\n\
    // Deletes the second through third elements (vec[1], vec[2])\nvec.erase(vec.begin()\
    \ + 1, vec.begin() + 3);\n\n"
- - How do I erase an element from std::vector<> by index?
  - "\nThe erase method on std::vector is overloaded, so it's probably clearer to\
    \ call\nvec.erase(vec.begin() + index);\n\nwhen you only want to erase a single\
    \ element.\n"
- - How do I erase an element from std::vector<> by index?
  - "\ntemplate <typename T>\nvoid remove(std::vector<T>& vec, size_t pos)\n{\n  \
    \  std::vector<T>::iterator it = vec.begin();\n    std::advance(it, pos);\n  \
    \  vec.erase(it);\n}\n\n"
- - What are the differences between struct and class in C++?
  - "\nYou forget the tricky 2nd difference between classes and structs.\nQuoth the\
    \ standard (Â§11.2.2 in C++98 through C++11):\n\nIn absence of an access-specifier\n\
    \  for a base class, public is assumed\n  when the derived class is declared\n\
    \  struct and private is assumed when the class is declared class.\n\nAnd just\
    \ for completeness' sake, the more widely known difference between class and struct\
    \ is defined in (11.2):\n\nMember of a class defined with the\n  keyword class\
    \ are private by\n  default. Members of a class defined\n  with the keywords struct\
    \ or union\n  are public by default.\n\nAdditional difference: the keyword class\
    \ can be used to declare template parameters, while the struct keyword cannot\
    \ be so used.\n"
- - What are the differences between struct and class in C++?
  - "\nQuoting The C++ FAQ,\n\n[7.8] What's the difference between\n  the keywords\
    \ struct and class?\nThe members and base classes of a\n  struct are public by\
    \ default, while in\n  class, they default to private.  Note:\n  you should make\
    \ your base classes\n  explicitly public, private, or\n  protected, rather than\
    \ relying on the\n  defaults.\nStruct and class are otherwise\n  functionally\
    \ equivalent.\nOK, enough of that squeaky clean\n  techno talk.  Emotionally,\
    \ most\n  developers make a strong distinction\n  between a class and a struct.\
    \  A\n  struct simply feels like an open pile\n  of bits with very little in the\
    \ way of\n  encapsulation or functionality.  A\n  class feels like a living and\n\
    \  responsible member of society with\n  intelligent services, a strong\n  encapsulation\
    \ barrier, and a well\n  defined interface.  Since that's the\n  connotation most\
    \ people already have,\n  you should probably use the struct\n  keyword if you\
    \ have a class that has\n  very few methods and has public data\n  (such things\
    \ do exist in well designed\n  systems!), but otherwise you should\n  probably\
    \ use the class keyword.\n\n"
- - What are the differences between struct and class in C++?
  - "\nIt's worth remembering C++'s origins in, and compatibility with, C.\nC has\
    \ structs, it has no concept of encapsulation, so everything is public.\nBeing\
    \ public by default is generally considered a bad idea when taking an object-oriented\
    \ approach, so in making a form of C that is natively conducive to OOP (you can\
    \ do OO in C, but it won't help you) which was the idea in C++ (originally \"\
    C With Classes\"), it makes sense to make members private by default.\nOn the\
    \ other hand, if Stroustrup had changed the semantics of struct so that its members\
    \ were private by default, it would have broken compatibility (it is no longer\
    \ as often true as the standards diverged, but all valid C programs were also\
    \ valid C++ programs, which had a big effect on giving C++ a foothold).\nSo a\
    \ new keyword, class was introduced to be exactly like a struct, but private by\
    \ default.\nIf C++ had come from scratch, with no history, then it would probably\
    \ have only one such keyword. It also probably wouldn't have made the impact it\
    \ made.\nIn general, people will tend to use struct when they are doing something\
    \ like how structs are used in C; public members, no constructor (as long as it\
    \ isn't in a union, you can have constructors in structs, just like with classes,\
    \ but people tend not to), no virtual methods, etc. Since languages are as much\
    \ to communicate with people reading the code as to instruct machines (or else\
    \ we'd stick with assembly and raw VM opcodes) it's a good idea to stick with\
    \ that.\n"
- - How do I list the symbols in a .so file
  - "\nThe standard tool for listing symbols is nm, you can use it simply like this:\n\
    nm -g yourLib.so\n\nIf you want to see symbols of a C++ library, add the \"-C\"\
    \ option which demangle the symbols (it's far more readable demangled).\nnm -gC\
    \ yourLib.so\n\nIf your .so file is in elf format, you have two options:\nEither\
    \ objdump (-C is also useful for demangling C++):\n$ objdump -TC libz.so\n\nlibz.so:\
    \     file format elf64-x86-64\n\nDYNAMIC SYMBOL TABLE:\n0000000000002010 l  \
    \  d  .init  0000000000000000              .init\n0000000000000000      DF *UND*\
    \  0000000000000000  GLIBC_2.2.5 free\n0000000000000000      DF *UND*  0000000000000000\
    \  GLIBC_2.2.5 __errno_location\n0000000000000000  w   D  *UND*  0000000000000000\
    \              _ITM_deregisterTMCloneTable\n\nOr use readelf:\n$ readelf -Ws libz.so\n\
    Symbol table '.dynsym' contains 112 entries:\n   Num:    Value          Size Type\
    \    Bind   Vis      Ndx Name\n     0: 0000000000000000     0 NOTYPE  LOCAL  DEFAULT\
    \  UND\n     1: 0000000000002010     0 SECTION LOCAL  DEFAULT   10\n     2: 0000000000000000\
    \     0 FUNC    GLOBAL DEFAULT  UND free@GLIBC_2.2.5 (14)\n     3: 0000000000000000\
    \     0 FUNC    GLOBAL DEFAULT  UND __errno_location@GLIBC_2.2.5 (14)\n     4:\
    \ 0000000000000000     0 NOTYPE  WEAK   DEFAULT  UND _ITM_deregisterTMCloneTable\n\
    \n"
- - How do I list the symbols in a .so file
  - "\nIf your .so file is in elf format, you can use readelf program to extract symbol\
    \ information from the binary.  This command will give you the symbol table:\n\
    readelf -Ws /usr/lib/libexample.so\n\nYou only should extract those that are defined\
    \ in this .so file, not in the libraries referenced by it.  Seventh column should\
    \ contain a number in this case.  You can extract it by using a simple regex:\n\
    readelf -Ws /usr/lib/libstdc++.so.6 | grep '^\\([[:space:]]\\+[^[:space:]]\\+\\\
    )\\{6\\}[[:space:]]\\+[[:digit:]]\\+'\n\nor, as proposed by Caspin,:\nreadelf\
    \ -Ws /usr/lib/libstdc++.so.6 | awk '{print $8}';\n\n"
- - How do I list the symbols in a .so file
  - "\nobjdump -TC /usr/lib/libexample.so\n\n"
- - When to use dynamic vs. static libraries
  - "\nStatic libraries increase the size of the code in your binary. They're always\
    \ loaded and whatever version of the code you compiled with is the version of\
    \ the code that will run.\nDynamic libraries are stored and versioned separately.\
    \ It's possible for a version of the dynamic library to be loaded that wasn't\
    \ the original one that shipped with your code if the update is considered binary\
    \ compatible with the original version.\nAdditionally dynamic libraries aren't\
    \ necessarily loaded -- they're usually loaded when first called -- and can be\
    \ shared among components that use the same library (multiple data loads, one\
    \ code load).\nDynamic libraries were considered to be the better approach most\
    \ of the time, but originally they had a major flaw (google DLL hell), which has\
    \ all but been eliminated by more recent Windows OSes (Windows XP in particular).\n"
- - When to use dynamic vs. static libraries
  - "\nOthers have adequately explained what a static library is, but I'd like to\
    \ point out some of the caveats of using static libraries, at least on Windows:\
    \ \n\nSingletons: If something needs to be global/static and unique, be very careful\
    \ about putting it in a static library. If multiple DLLs are linked against that\
    \ static library they will each get their own copy of the singleton. However,\
    \ if your application is a single EXE with no custom DLLs, this may not be a problem.\n\
    Unreferenced code removal: When you link against a static library, only the parts\
    \ of the static library that are referenced by your DLL/EXE will get linked into\
    \ your DLL/EXE. \nFor example, if mylib.lib contains a.obj and b.obj and your\
    \ DLL/EXE only references functions or variables from a.obj, the entirety of b.obj\
    \ will get discarded by the linker. If b.obj contains global/static objects, their\
    \ constructors and destructors will not get executed. If those constructors/destructors\
    \ have side effects, you may be disappointed by their absence.\nLikewise, if the\
    \ static library contains special entrypoints you may need to take care that they\
    \ are actually included. An example of this in embedded programming (okay, not\
    \ Windows) would be an interrupt handler that is marked as being at a specific\
    \ address. You also need to mark the interrupt handler as an entrypoint to make\
    \ sure it doesn't get discarded.\nAnother consequence of this is that a static\
    \ library may contain object files that are completely unusable due to unresolved\
    \ references, but it won't cause a linker error until you reference a function\
    \ or variable from those object files. This may happen long after the library\
    \ is written.\nDebug symbols: You may want a separate PDB for each static library,\
    \ or you may want the debug symbols to be placed in the object files so that they\
    \ get rolled into the PDB for the DLL/EXE. The Visual C++ documentation explains\
    \ the necessary options.\nRTTI: You may end up with multiple type_info objects\
    \ for the same class if you link a single static library into multiple DLLs. If\
    \ your program assumes that type_info is \"singleton\" data and uses &typeid()\
    \ or type_info::before(), you may get undesirable and surprising results. \n\n"
- - When to use dynamic vs. static libraries
  - "\nA lib is a unit of code that is bundled within your application executable.\n\
    A dll is a standalone unit of executable code. It is loaded in the process only\
    \ when a call is made into that code. A dll can be used by multiple applications\
    \ and loaded in multiple processes, while still having only one copy of the code\
    \ on the hard drive.\nDll pros: can be used to reuse/share code between several\
    \ products; load in the process memory on demand and can be unloaded when not\
    \ needed; can be upgraded independently of the rest of the program.\nDll cons:\
    \ performance impact of the dll loading and code rebasing; versioning problems\
    \ (\"dll hell\")\nLib pros: no performance impact as code is always loaded in\
    \ the process and is not rebased; no versioning problems.\nLib cons: executable/process\
    \ \"bloat\" - all the code is in your executable and is loaded upon process start;\
    \ no reuse/sharing - each product has its own copy of the code.\n"
- - Why does GCC generate 15-20% faster code if I optimize for size instead of speed?
  - "\nBy default compilers optimize for \"average\" processor. Since different processors\
    \ favor different instruction sequences, compiler optimizations enabled by -O2\
    \ might benefit average processor, but decrease performance on your particular\
    \ processor (and the same applies to -Os). If you try the same example on different\
    \ processors, you will find that on some of them benefit from -O2 while other\
    \ are more favorable to -Os optimizations.\nHere are the results for time ./test\
    \ 0 0 on several processors (user time reported):\nProcessor (System-on-Chip)\
    \             Compiler   Time (-O2)  Time (-Os)  Fastest\nAMD Opteron 8350   \
    \                    gcc-4.8.1    0.704s      0.896s      -O2\nAMD FX-6300   \
    \                         gcc-4.8.1    0.392s      0.340s      -Os\nAMD E2-1800\
    \                            gcc-4.7.2    0.740s      0.832s      -O2\nIntel Xeon\
    \ E5405                       gcc-4.8.1    0.603s      0.804s      -O2\nIntel\
    \ Xeon E5-2603                     gcc-4.4.7    1.121s      1.122s       -\nIntel\
    \ Core i3-3217U                    gcc-4.6.4    0.709s      0.709s       -\nIntel\
    \ Core i3-3217U                    gcc-4.7.3    0.708s      0.822s      -O2\n\
    Intel Core i3-3217U                    gcc-4.8.1    0.708s      0.944s      -O2\n\
    Intel Core i7-4770K                    gcc-4.8.1    0.296s      0.288s      -Os\n\
    Intel Atom 330                         gcc-4.8.1    2.003s      2.007s      -O2\n\
    ARM 1176JZF-S (Broadcom BCM2835)       gcc-4.6.3    3.470s      3.480s      -O2\n\
    ARM Cortex-A8 (TI OMAP DM3730)         gcc-4.6.3    2.727s      2.727s       -\n\
    ARM Cortex-A9 (TI OMAP 4460)           gcc-4.6.3    1.648s      1.648s       -\n\
    ARM Cortex-A9 (Samsung Exynos 4412)    gcc-4.6.3    1.250s      1.250s       -\n\
    ARM Cortex-A15 (Samsung Exynos 5250)   gcc-4.7.2    0.700s      0.700s       -\n\
    Qualcomm Snapdragon APQ8060A           gcc-4.8       1.53s       1.52s      -Os\n\
    \nIn some cases you can alleviate the effect of disadvantageous optimizations\
    \ by asking gcc to optimize for your particular processor (using options -mtune=native\
    \ or -march=native):\nProcessor            Compiler   Time (-O2 -mtune=native)\
    \ Time (-Os -mtune=native)\nAMD FX-6300          gcc-4.8.1         0.340s    \
    \               0.340s\nAMD E2-1800          gcc-4.7.2         0.740s        \
    \           0.832s\nIntel Xeon E5405     gcc-4.8.1         0.603s            \
    \       0.803s\nIntel Core i7-4770K  gcc-4.8.1         0.296s                \
    \   0.288s\n\nUpdate: on Ivy Bridge-based Core i3 three versions of gcc (4.6.4,\
    \ 4.7.3, and 4.8.1) produce binaries with significantly different performance,\
    \ but the assembly code has only subtle variations. So far, I have no explanation\
    \ of this fact.\nAssembly from gcc-4.6.4 -Os (executes in 0.709 secs):\n00000000004004d2\
    \ <_ZL3addRKiS0_.isra.0>:\n  4004d2:       8d 04 37                lea    eax,[rdi+rsi*1]\n\
    \  4004d5:       c3                      ret\n\n00000000004004d6 <_ZL4workii>:\n\
    \  4004d6:       41 55                   push   r13\n  4004d8:       41 89 fd\
    \                mov    r13d,edi\n  4004db:       41 54                   push\
    \   r12\n  4004dd:       41 89 f4                mov    r12d,esi\n  4004e0:  \
    \     55                      push   rbp\n  4004e1:       bd 00 c2 eb 0b     \
    \     mov    ebp,0xbebc200\n  4004e6:       53                      push   rbx\n\
    \  4004e7:       31 db                   xor    ebx,ebx\n  4004e9:       41 8d\
    \ 34 1c             lea    esi,[r12+rbx*1]\n  4004ed:       41 8d 7c 1d 00   \
    \       lea    edi,[r13+rbx*1+0x0]\n  4004f2:       e8 db ff ff ff          call\
    \   4004d2 <_ZL3addRKiS0_.isra.0>\n  4004f7:       01 c3                   add\
    \    ebx,eax\n  4004f9:       ff cd                   dec    ebp\n  4004fb:  \
    \     75 ec                   jne    4004e9 <_ZL4workii+0x13>\n  4004fd:     \
    \  89 d8                   mov    eax,ebx\n  4004ff:       5b                \
    \      pop    rbx\n  400500:       5d                      pop    rbp\n  400501:\
    \       41 5c                   pop    r12\n  400503:       41 5d            \
    \       pop    r13\n  400505:       c3                      ret\n\nAssembly from\
    \ gcc-4.7.3 -Os (executes in 0.822 secs):\n00000000004004fa <_ZL3addRKiS0_.isra.0>:\n\
    \  4004fa:       8d 04 37                lea    eax,[rdi+rsi*1]\n  4004fd:   \
    \    c3                      ret\n\n00000000004004fe <_ZL4workii>:\n  4004fe:\
    \       41 55                   push   r13\n  400500:       41 89 f5         \
    \       mov    r13d,esi\n  400503:       41 54                   push   r12\n\
    \  400505:       41 89 fc                mov    r12d,edi\n  400508:       55 \
    \                     push   rbp\n  400509:       bd 00 c2 eb 0b          mov\
    \    ebp,0xbebc200\n  40050e:       53                      push   rbx\n  40050f:\
    \       31 db                   xor    ebx,ebx\n  400511:       41 8d 74 1d 00\
    \          lea    esi,[r13+rbx*1+0x0]\n  400516:       41 8d 3c 1c           \
    \  lea    edi,[r12+rbx*1]\n  40051a:       e8 db ff ff ff          call   4004fa\
    \ <_ZL3addRKiS0_.isra.0>\n  40051f:       01 c3                   add    ebx,eax\n\
    \  400521:       ff cd                   dec    ebp\n  400523:       75 ec   \
    \                jne    400511 <_ZL4workii+0x13>\n  400525:       89 d8      \
    \             mov    eax,ebx\n  400527:       5b                      pop    rbx\n\
    \  400528:       5d                      pop    rbp\n  400529:       41 5c   \
    \                pop    r12\n  40052b:       41 5d                   pop    r13\n\
    \  40052d:       c3                      ret\n\nAssembly from gcc-4.8.1 -Os (executes\
    \ in 0.994 secs):\n00000000004004fd <_ZL3addRKiS0_.isra.0>:\n  4004fd:       8d\
    \ 04 37                lea    eax,[rdi+rsi*1]\n  400500:       c3            \
    \          ret\n\n0000000000400501 <_ZL4workii>:\n  400501:       41 55      \
    \             push   r13\n  400503:       41 89 f5                mov    r13d,esi\n\
    \  400506:       41 54                   push   r12\n  400508:       41 89 fc\
    \                mov    r12d,edi\n  40050b:       55                      push\
    \   rbp\n  40050c:       bd 00 c2 eb 0b          mov    ebp,0xbebc200\n  400511:\
    \       53                      push   rbx\n  400512:       31 db            \
    \       xor    ebx,ebx\n  400514:       41 8d 74 1d 00          lea    esi,[r13+rbx*1+0x0]\n\
    \  400519:       41 8d 3c 1c             lea    edi,[r12+rbx*1]\n  40051d:   \
    \    e8 db ff ff ff          call   4004fd <_ZL3addRKiS0_.isra.0>\n  400522: \
    \      01 c3                   add    ebx,eax\n  400524:       ff cd         \
    \          dec    ebp\n  400526:       75 ec                   jne    400514 <_ZL4workii+0x13>\n\
    \  400528:       89 d8                   mov    eax,ebx\n  40052a:       5b  \
    \                    pop    rbx\n  40052b:       5d                      pop \
    \   rbp\n  40052c:       41 5c                   pop    r12\n  40052e:       41\
    \ 5d                   pop    r13\n  400530:       c3                      ret\n\
    \n"
- - Why does GCC generate 15-20% faster code if I optimize for size instead of speed?
  - "\nMy colleague helped me find a plausible answer to my question. He noticed the\
    \ importance of the 256 byte boundary. He is not registered here and encouraged\
    \ me to post the answer myself (and take all the fame).\n\nShort answer:\n\nIs\
    \ it the padding that is the culprit in this case? Why and how?\n\nIt all boils\
    \ down to alignment. Alignments can have a significant impact on the performance,\
    \ that is why we have the -falign-* flags in the first place.\nI have submitted\
    \ a (bogus?) bug report to the gcc developers. It turns out that the default behavior\
    \ is \"we align loops to 8 byte by default but try to align it to 16 byte if we\
    \ don't need to fill in over 10 bytes.\" Apparently, this default is not the best\
    \ choice in this particular case and on my machine. Clang 3.4 (trunk) with -O3\
    \ does the appropriate alignment and the generated code does not show this weird\
    \ behavior.\nOf course, if an inappropriate alignment is done, it makes things\
    \ worse. An unnecessary / bad alignment just eats up bytes for no reason and potentially\
    \ increases cache misses, etc.\n\nThe noise it makes pretty much makes timing\
    \ micro-optimizations\n  impossible.\nHow can I make sure that such accidental\
    \ lucky / unlucky alignments\n  are not interfering when I do micro-optimizations\
    \ (unrelated to stack\n  alignment) on C or C++ source codes?\n\nSimply by telling\
    \ gcc to do the right alignment:\ng++ -O2 -falign-functions=16 -falign-loops=16\n\
    \nLong answer:\nThe code will run slower if:\n\nan XX byte boundary cuts add()\
    \ in the middle (XX being machine dependent).\nif the call to add() has to jump\
    \ over an XX byte boundary and the target is not aligned.\nif  add() is not aligned.\n\
    if the loop is not aligned.\n\nThe first 2 are beautifully visible on the codes\
    \ and results that Marat Dukhan kindly posted. In this case, gcc-4.8.1 -Os (executes\
    \ in 0.994 secs):\n00000000004004fd <_ZL3addRKiS0_.isra.0>:\n  4004fd:       8d\
    \ 04 37                lea    eax,[rdi+rsi*1]\n  400500:       c3   \n\na 256\
    \ byte boundary cuts add() right in the middle and neither add() nor the loop\
    \ is aligned. Surprise, surprise, this is the slowest case!\nIn case gcc-4.7.3\
    \ -Os (executes in 0.822 secs), the 256 byte boundary only cuts into a cold section\
    \ (but neither the loop, nor add() is cut):\n00000000004004fa <_ZL3addRKiS0_.isra.0>:\n\
    \  4004fa:       8d 04 37                lea    eax,[rdi+rsi*1]\n  4004fd:   \
    \    c3                      ret\n\n[...]\n\n  40051a:       e8 db ff ff ff  \
    \        call   4004fa <_ZL3addRKiS0_.isra.0>\n\nNothing is aligned, and the call\
    \ to add() has to jump over the 256 byte boundary. This code is the second slowest.\n\
    In case gcc-4.6.4 -Os (executes in 0.709 secs), although nothing is aligned, the\
    \ call to add() doesn't have to jump over the 256 byte boundary and the target\
    \ is exactly 32 byte away:\n  4004f2:       e8 db ff ff ff          call   4004d2\
    \ <_ZL3addRKiS0_.isra.0>\n  4004f7:       01 c3                   add    ebx,eax\n\
    \  4004f9:       ff cd                   dec    ebp\n  4004fb:       75 ec   \
    \                jne    4004e9 <_ZL4workii+0x13>\n\nThis is the fastest of all\
    \ three. Why the 256 byte boundary is speacial on his machine, I will leave it\
    \ up to him to figure it out. I don't have such a processor.\nNow, on my machine\
    \ I don't get this 256 byte boundary effect. Only the function and the loop alignment\
    \ kicks in on my machine. If I pass g++ -O2 -falign-functions=16 -falign-loops=16\
    \ then everything is back to normal: I always get the fastest case and the time\
    \ isn't sensitive to the -fno-omit-frame-pointer flag anymore. I can pass g++\
    \ -O2 -falign-functions=32 -falign-loops=32 or any multiples of 16, the code is\
    \ not sensitive to that either.\n\nI first noticed in 2009 that gcc (at least\
    \ on my projects and on my\n  machines) have the tendency to generate noticeably\
    \ faster code if I\n  optimize for size (-Os) instead of speed (-O2 or -O3) and\
    \ I have been\n  wondering ever since why.\n\nA likely explanation is that I had\
    \ hotspots which were sensitive to the alignment, just like the one in this example.\
    \ By messing with the flags (passing -Os instead of -O2), those hotspots were\
    \ aligned in a lucky way by accident and the code became faster. It had nothing\
    \ to do with optimizing for size: These were by sheer accident that the hotspots\
    \ got aligned better. From now on, I will check the effects of alignment on my\
    \ projects.\nOh, and one more thing. How can such hotspots arise, like the one\
    \ shown in the example? How can the inlining of such a tiny function like add()\
    \ fail?\nConsider this:\n// add.cpp\nint add(const int& x, const int& y) {\n \
    \   return x + y;\n}\n\nand in a separate file:\n// main.cpp\nint add(const int&\
    \ x, const int& y);\n\nconst int LOOP_BOUND = 200000000;\n\n__attribute__((noinline))\n\
    static int work(int xval, int yval) {\n    int sum(0);\n    for (int i=0; i<LOOP_BOUND;\
    \ ++i) {\n        int x(xval+sum);\n        int y(yval+sum);\n        int z =\
    \ add(x, y);\n        sum += z;\n    }\n    return sum;\n}\n\nint main(int , char*\
    \ argv[]) {\n    int result = work(*argv[1], *argv[2]);\n    return result;\n\
    }\n\nand compiled as: g++ -O2 add.cpp main.cpp. \nÂ Â Â Â Â Â gcc won't inline add()!\n\
    That's all, it's that easy to  unintendedly create hotspots like the one in the\
    \ OP. Of course it is partly my fault: gcc is an excellent compiler. If compile\
    \ the above as: g++ -O2 -flto add.cpp main.cpp, that is, if I perform link time\
    \ optimization, the code runs in 0.19s! \n(Inlining is artificially disabled in\
    \ the OP, hence, the code in the OP was 2x slower).\n"
- - Why does GCC generate 15-20% faster code if I optimize for size instead of speed?
  - "\nI'm adding this post-accept to point out that the effects of alignment on overall\
    \ performance of programs - including big ones - has been studied.  For example,\
    \ this article (and I believe a version of this also appeared in CACM) shows how\
    \ link order and OS environment size changes alone were sufficient to shift performance\
    \ significantly.  They attribute this to alignment of \"hot loops\".\nThis paper,\
    \ titled \"Producing wrong data without doing anything obviously wrong!\" says\
    \ that inadvertent experimental bias due to nearly uncontrollable differences\
    \ in program running environments probably renders many benchmark results meaningless.\
    \ \nI think you're encountering a different angle on the same observation.  \n\
    For performance-critical code, this is a pretty good argument for systems that\
    \ assess the environment at installation or run time and choose the local best\
    \ among differently optimized versions of key routines. \n"
- - Why does the order in which libraries are linked sometimes cause errors in GCC?
  - "\n(See the history on this answer to get the more elaborate text, but I now think\
    \ it's easier for the reader to see real command lines).\n\nCommon files shared\
    \ by all below commands\n$ cat a.cpp\nextern int a;\nint main() {\n  return a;\n\
    }\n\n$ cat b.cpp\nextern int b;\nint a = b;\n\n$ cat d.cpp\nint b;\n\nLinking\
    \ to static libraries\n$ g++ -c b.cpp -o b.o\n$ ar cr libb.a b.o\n$ g++ -c d.cpp\
    \ -o d.o\n$ ar cr libd.a d.o\n\n$ g++ -L. -ld -lb a.cpp # wrong order\n$ g++ -L.\
    \ -lb -ld a.cpp # wrong order\n$ g++ a.cpp -L. -ld -lb # wrong order\n$ g++ a.cpp\
    \ -L. -lb -ld # right order\n\nThe linker searches from left to right, and notes\
    \ unresolved symbols as it go. If a library resolves the symbol, it takes the\
    \ object files of that library to resolve the symbol (b.o out of libb.a in this\
    \ case). \nDependencies of static libraries against each other work the same -\
    \ the library that needs symbols must be first, then the library that resolves\
    \ the symbol.\nIf a static library depends on another library, but the other library\
    \ again depends on the former library, there is a cycle. You can resolve this\
    \ by enclosing the cyclically dependent libraries by -( and -), such as -( -la\
    \ -lb -) (you may need to escape the parens, such as -\\( and -\\)). The linker\
    \ then searches those enclosed lib multiple times to ensure cycling dependencies\
    \ are resolved. Alternatively, you can specify the libraries multiple times, so\
    \ each is before one another: -la -lb -la.\nLinking to dynamic libraries\n$ export\
    \ LD_LIBRARY_PATH=. # not needed if libs go to /usr/lib etc\n$ g++ -fpic -shared\
    \ d.cpp -o libd.so\n$ g++ -fpic -shared b.cpp -L. -ld -o libb.so # specifies its\
    \ dependency!\n\n$ g++ -L. -lb a.cpp # wrong order (works on some distributions)\n\
    $ g++ -Wl,--as-needed -L. -lb a.cpp # wrong order\n$ g++ -Wl,--as-needed a.cpp\
    \ -L. -lb # right order\n\nIt's the same here - the libraries must follow the\
    \ object files of the program. The difference here with the static libraries is\
    \ that you must not care about the dependencies of the libraries against each\
    \ other, because dynamic libraries sort out their dependencies themselves. \n\
    Some recent distributions apparently default to using the --as-needed linker flag,\
    \ which enforces that the program's object files come before the dynamic libraries.\
    \ If that flag is passed, the linker will not link to libraries that are not actually\
    \ needed by the executable (and it detects this from left to right). My recent\
    \ archlinux distribution doesn't use this flag by default, so it didn't give an\
    \ error for not following the correct order.\nIt is not correct to omit the dependency\
    \ of b.so against d.so when creating the former. You will be required to specify\
    \ the library when linking a then, but a doesn't really need the integer b itself,\
    \ so it should not be made to care about b's own dependencies. \nHere is an example\
    \ of the implications if you miss specifying the dependencies for libb.so\n$ export\
    \ LD_LIBRARY_PATH=. # not needed if libs go to /usr/lib etc\n$ g++ -fpic -shared\
    \ d.cpp -o libd.so\n$ g++ -fpic -shared b.cpp -o libb.so # wrong (but links)\n\
    \n$ g++ -L. -lb a.cpp # wrong, as above\n$ g++ -Wl,--as-needed -L. -lb a.cpp #\
    \ wrong, as above\n$ g++ a.cpp -L. -lb # wrong, missing libd.so\n$ g++ a.cpp -L.\
    \ -ld -lb # wrong order (works on some distributions)\n$ g++ -Wl,--as-needed a.cpp\
    \ -L. -ld -lb # wrong order (like static libs)\n$ g++ -Wl,--as-needed a.cpp -L.\
    \ -lb -ld # \"right\"\n\nIf you now look into what dependencies the binary has,\
    \ you note the binary itself depends also on libd, not just libb as it should.\
    \ The binary will need to be relinked if libb later depends on another library,\
    \ if you do it this way. And if someone else loads libb using dlopen at runtime\
    \ (think of loading plugins dynamically), the call will fail as well. So the \"\
    right\" really should be a wrong as well. \n"
- - Why does the order in which libraries are linked sometimes cause errors in GCC?
  - "\nThe GNU ld linker is a so-called smart linker.  It will keep track of the functions\
    \ used by preceding static libraries, permanently tossing out those functions\
    \ that are not used from its lookup tables.  The result is that if you link a\
    \ static library too early, then the functions in that library are no longer available\
    \ to static libraries later on the link line.  \nThe typical UNIX linker works\
    \ from left to right, so put all your dependent libraries on the left, and the\
    \ ones that satisfy those dependencies on the right of the link line.  You may\
    \ find that some libraries depend on others while at the same time other libraries\
    \ depend on them.  This is where it gets complicated.  When it comes to circular\
    \ references, fix your code!\n"
- - Why does the order in which libraries are linked sometimes cause errors in GCC?
  - "\nHere's an example to make it clear how things work with GCC when static libraries\
    \ are involved. So let's assume we have the following scenario:\n\nmyprog.o -\
    \ containing main() function, dependent on libmysqlclient\nlibmysqlclient - static,\
    \ for the sake of the example (you'd prefer the shared library, of course, as\
    \ the libmysqlclient is huge); in /usr/local/lib; and dependent on stuff from\
    \ libz\nlibz (dynamic)\n\nHow do we link this? (Note: examples from compiling\
    \ on Cygwin using gcc 4.3.4)\ngcc -L/usr/local/lib -lmysqlclient myprog.o\n# undefined\
    \ reference to `_mysql_init'\n# myprog depends on libmysqlclient\n# so myprog\
    \ has to come earlier on the command line\n\ngcc myprog.o -L/usr/local/lib -lmysqlclient\n\
    # undefined reference to `_uncompress'\n# we have to link with libz, too\n\ngcc\
    \ myprog.o -lz -L/usr/local/lib -lmysqlclient\n# undefined reference to `_uncompress'\n\
    # libz is needed by libmysqlclient\n# so it has to appear *after* it on the command\
    \ line\n\ngcc myprog.o -L/usr/local/lib -lmysqlclient -lz\n# this works\n\n"
- - What's the difference between âSTLâ and âC++ Standard Libraryâ?
  - "\nThe \"STL\" was written by Alexander Stepanov in the days long before C++ was\
    \ standardised. C++ existed through the 80s, but what we now call \"C++\" is the\
    \ language standardised in ISO/IEC 14882:2014 (and earlier versions, such as ISO/IEC\
    \ 14882:2011).\nThe STL was already widely used as a library for C++, giving programmers\
    \ access to containers, iterators and algorithms. When the standardisation happened,\
    \ the language committee designed parts of the C++ Standard Library (which is\
    \ part of the language standard) to very closely match the STL.\nOver the years,\
    \ many people â including prominent book authors, and various websites â have\
    \ continued to refer to the C++ Standard Library as \"the STL\", despite the fact\
    \ that the two entities are separate and that there are some differences. These\
    \ differences are even more pronounced in the upcoming new C++ standard, which\
    \ includes various features and significantly alters some classes.\nThe original\
    \ STL is now often called \"an implementation of the C++ Standard Template Library\"\
    \ (rather backwards to actual history!), in the same way that your Microsoft Visual\
    \ Studio or GCC ships an implementation of the C++ Standard Library. But the \"\
    Standard Template Library\" and the \"Standard Library\" are not the same thing.\n\
    The battle is about whether the current Standard Library should be called \"the\
    \ STL\" in whole or in part, and/or whether it matters what it's called.\nFor\
    \ \"STL\"\nThere is a school of thought that says that everybody knows now that\
    \ \"STL\" means the standard library, just as everybody now knows that \"C++\"\
    \ is the ISO-standardised language.\nIt also includes those who believe that it\
    \ doesn't really matter as long as all parties understand what is being talked\
    \ about.\nIt's a term made even more prevalent by the nature of the beast, much\
    \ of which makes heavy use of the C++ feature known as \"templates\".\nFor \"\
    C++ Standard Library\" (or stdlib)\nHowever, there is another school of thought\
    \ â to which I subscribe â that says that this is confusing. People learning C++\
    \ for the first time do not know this distinction, and may not notice small language\
    \ differences.\nThe author of that article has numerous times encountered people\
    \ who believe that the entire C++ Standard Library is the STL, including features\
    \ that were never part of the STL itself. Most vocal proponents of \"the STL\"\
    , in contrast, know exactly what they mean by it and refuse to believe that not\
    \ everybody \"gets it\". Clearly, the term's usage is not uniform.\nIn addition,\
    \ there are some STL-like libraries that are in fact implementations of the original\
    \ STL, not the C++ Standard Library. Until recently, STLPort was one of them (and\
    \ even there, the confusion abounds!).\nFurther, the C++ Standard does not contain\
    \ the text \"STL\" anywhere, and some people habitually employ phrases like \"\
    the STL is included in the C++ Standard Library\", which is plain incorrect. \n\
    It's my belief that continuing to propagate the usage of the term in this way\
    \ will just lead to the misunderstanding going on forever. Alas, it may be entirely\
    \ counter-productive to attempt to change things, even if it's supposed to be\
    \ for the better. We may just be stuck with double-meanings forever.\nConclusion\n\
    I appreciate that this post has been a little biased: I wrote the article you\
    \ linked to. :) Anyway, I hope this helps to explain the battle a bit better.\n\
    Update 13/04/2011\nHere are three perfect examples of someone who is using \"\
    the STL\" to refer to the entire C++ Standard Library. It continues to baffle\
    \ me that so many people swear blind that nobody ever does this, when it's plain\
    \ to see almost on a daily basis.\n"
- - What's the difference between âSTLâ and âC++ Standard Libraryâ?
  - "\nThere is no one answer that's really correct. Alexander Stepanov developed\
    \ a library he called STL (working for HP at the time). That library was then\
    \ proposed for inclusion in the C++ standard.\nThat basically \"forked\" development.\
    \ The committee included some parts, rejected others completely, and redesigned\
    \ a few (with Alexander's participation). Development of the original library\
    \ was later moved to Silicon Graphics, but continued separately from the C++ standard\
    \ library.\nAfter those pieces were added to the standard library, some other\
    \ parts of the standard library were modified to fit better with what was added\
    \ (e.g., begin, end, rbegin and rend were added to std::string so it could be\
    \ used like a container). Around the same time, most of the library (even pieces\
    \ that were completely unrelated were made into templates to accommodate different\
    \ types (e.g., standard streams).\nSome people also use STL as just a short form\
    \ of \"STandard Library\".\nThat means when somebody uses the term \"STL\" they\
    \ could be referring to any of about half a dozen different things. For better\
    \ or worse, most people who use it seem to ignore the multiplicity of meanings,\
    \ and assume that everybody else will recognize what they're referring to. This\
    \ leads to many misunderstandings, and at least a few serious flame-wars that\
    \ made most of the participants look foolish because they were simply talking\
    \ about entirely different things.\nUnfortunately, the confusion is likely to\
    \ continue unabated. It's much more convenient to refer to \"STL\" than something\
    \ like \"the containers, iterators, and algorithms in the C++ standard library,\
    \ but not including std::string, even though it can act like a container.\" Even\
    \ though \"C++ standard library\" isn't quite as long and clumsy as that, \"STL\"\
    \ is still a lot shorter and simpler still. Until or unless somebody invents terms\
    \ that are more precise (when necessary), and just as convenient, \"STL\" will\
    \ continue to be used and confusion will continue to result.\n"
- - What's the difference between âSTLâ and âC++ Standard Libraryâ?
  - "\nThe term \"STL\" or \"Standard Template Library\" does not show up anywhere\
    \ in the ISO 14882 C++ standard. So referring to the C++ standard library as STL\
    \ is wrong. The term \"C++ Standard Library\" or \"standard library\" is what's\
    \ officially used by ISO 14882:\n\nISO 14882 C++ Standard:\n17 - Library introduction\
    \ [lib.library]:\n\nThis clauses describes the contents of the C++ Standard Library,\
    \ how\n  a well-formed C++ program makes use of\n  the library, and how a conforming\n\
    \  implementation may provide the\n  entities in the library.\n\n...\n\nSTL is\
    \ a library originally designed by Alexander Stepanov, independent of the C++\
    \ standard. However, some components of the C++ standard library include STL components\
    \ like vector, list and algorithms like copy and swap.\nBut of course the C++\
    \ standard includes much more things outside the STL, so the term \"C++ standard\
    \ library\" is more correct (and is what's actually used by the standards documents).\n"
- - Which is better option to use for dividing an integer number by 2?
  - "\nUse the operation that best describes what you are trying to do.\n\nIf you\
    \ are treating the number as a sequence of bits, use bitshift.\nIf you are treating\
    \ it as a numerical value, use division.\n\nNote that they are not exactly equivalent.\
    \ They can give different results for negative integers. For example: \n-5 / 2\
    \  = -2\n-5 >> 1 = -3\n\n(ideone)\n"
- - Which is better option to use for dividing an integer number by 2?
  - "\nDoes the first one look like dividing? No. If you want to divide, use x / 2.\
    \ Compiler can optimise it to use bit-shift if possible (it's called strength\
    \ reduction), which makes it a useless micro-optimisation if you do it on your\
    \ own.\n"
- - Which is better option to use for dividing an integer number by 2?
  - "\nTo pile on: there are so many reasons to favor using x = x / 2;  Here are some:\n\
    \nit expresses your intent more clearly (assuming you're not dealing with bit\
    \ twiddling register bits or something)\nthe compiler will reduce this to a shift\
    \ operation anyway\neven if the compiler didn't reduce it and chose a slower operation\
    \ than the shift, the likelihood that this ends up affecting your program's performance\
    \ in a measurable way is itself vanishingly small (and if it does affect it measurably,\
    \ then you have an actual reason to use a shift)\nif the division is going to\
    \ be part of a larger expression, you're more likely to get the precedence right\
    \ if you use the division operator:\nx = x / 2 + 5;\nx = x >> 1 + 5;  // not the\
    \ same as above\n\nsigned arithmetic might complicate things even more than the\
    \ precedence problem mentioned above\nto reiterate - the compiler will already\
    \ do this for you anyway. In fact, it'll convert division by a constant to a series\
    \ of shifts, adds, and multiplies for all sorts of numbers, not just powers of\
    \ two. See this question for links to even more information about this.\n\nIn\
    \ short, you buy nothing by coding a shift when you really mean to multiply or\
    \ divide, except maybe an increased possibility of introducing a bug. It's been\
    \ a lifetime since compilers weren't smart enough to optimize this kind of thing\
    \ to a shift when appropriate.\n"
- - Where do I find the current C or C++ standard documents?
  - "\nPDF versions of the standard\nAs of 1st September 2014, the best locations\
    \ by price for C and C++ standards documents in PDF are:\n\nC++17 â ISO/IEC 14882:2017:\
    \ $116 from ansi.org\nC++14 â ISO/IEC 14882:2014: $90 NZD (about $60 US) from\
    \ Standards New Zealand\nC++11 â ISO/IEC 14882:2011: $60 from ansi.org $60 from\
    \ Techstreet\nC++03 â ISO 14882:2003: $30 from ansi.org $48 from SAI Global\n\
    C++98 â ISO/IEC 14882:1998: $90 NZD (about $60 US) from Standards New Zealand\n\
    C17/C18 â ISO/IEC 9899:2018: $232 from ansi.org / N2176 / c17_updated_proposed_fdis.pdf\
    \ draft from November 2017 (Link broken, see Wayback Machine N2176)\nC11 â ISO/IEC\
    \ 9899:2011: $30 $60 from ansi.org / WG14 draft version N1570\nC99 â ISO 9899:1999:\
    \ $30 $60 from ansi.org / WG14 draft version N1256\nC90 â AS 3955-1991: $141 from\
    \ ansi.org $175 from Techstreet (the Australian version of C90, identical to ISO\
    \ 9899:1990)\nC90 â 9899:1990 Hardcopy available from SAI Global ($88 + shipping)\n\
    \nYou cannot usually get old revisions of a standard (any standard) directly from\
    \ the standards bodies shortly after a new edition of the standard is released.\
    \  Thus, standards for C89, C90, C99, C++98, C++03 will be hard to find for purchase\
    \ from a standards body.  If you need an old revision of a standard, check Techstreet\
    \ as one possible source.  For example, it can still provide the Canadian version\
    \ CAN/CSA-ISO/IEC 9899:1990 standard in PDF, for a fee.\nNon-PDF electronic versions\
    \ of the standard\n\nC89 â Draft version in ANSI text format: (https://web.archive.org/web/20161223125339/http://flash-gordon.me.uk/ansi.c.txt)\n\
    C90 TC1; ISO/IEC 9899 TCOR1, single-page HTML document: (http://www.open-std.org/jtc1/sc22/wg14/www/docs/tc1.htm)\n\
    C90 TC2; ISO/IEC 9899 TCOR2, single-page HTML document: (http://www.open-std.org/jtc1/sc22/wg14/www/docs/tc2.htm)\n\
    \nPrint versions of the standard\nPrint copies of the standards are available\
    \ from national standards bodies and ISO but are very expensive.\nIf you want\
    \ a hardcopy of the C90 standard for much less money than above, you may be able\
    \ to find a cheap used copy of Herb Schildt's book The Annotated ANSI Standard\
    \ at Amazon, which contains the actual text of the standard (useful) and commentary\
    \ on the standard (less useful).\nThe C99 and C++03 standards are available in\
    \ book form from Wiley and the BSI (British Standards Institute):\n\nC++03 Standard\
    \ on Amazon\nC99 Standard on Amazon\n\nStandards committee draft versions\nThe\
    \ working draft for future standards is often available from the committee websites:\n\
    \nC++ committee website\nC committee website\n\nNote that these documents are\
    \ not the same as the standard, though the versions just prior to the meetings\
    \ that decide on a standard are usually very close to what is in the final standard.\
    \  The FCD (Final Committee Draft) versions are password protected; you need to\
    \ be on the standards committee to get them.\nHowever, in my opinion, even though\
    \ the draft versions might be very close to the final ratified versions of the\
    \ standards, you should really get a copy of the actual documents â especially\
    \ if you're planning on quoting them as references.  Of course, starving students\
    \ should go ahead and use the drafts if strapped for cash.\n\nIt appears that,\
    \ if you are willing and able to wait a few months after ratification of a standard,\
    \ to search for \"INCITS/ISO/IEC\" instead of \"ISO/IEC\" when looking for a standard\
    \ is the key. This way I was able to find the C++11 standard at reasonable price\
    \ and now the C11 standard. So, as an example you search for \"INCITS/ISO/IEC\
    \ 9899:2011\" instead of \"ISO/IEC 9899:2011\" on webstore.ansi.org and you will\
    \ find the reasonably priced PDF version.\n\nThe site https://wg21.link/ provides\
    \ short-URL links to the C++ current working draft and draft standards, and committee\
    \ papers:\n\nhttps://wg21.link/std11 - C++11\nhttps://wg21.link/std14 - C++14\n\
    https://wg21.link/std17 - C++17\nhttps://wg21.link/std - current working draft\n\
    \n"
- - Where do I find the current C or C++ standard documents?
  - "\nOnline versions of the standard can be found:\nWorking Draft, Standard for\
    \ Programming Language C++\nThe following all draft versions of the standard:\n\
    All the following are freely downloadable\n2018-02-12: N4727 git\n2017-11-27:\
    \ N4713 git\n2017-10-16: N4700 git\n2017-07-30: N4687 git \nThis seems to be the\
    \ new standard:\nThese version requires Authentication\n2017-03-21: N4660 is the\
    \ C++17 Draft Standard  \nThe following all draft versions of the standard:\n\
    All the following are freely downloadable\n2017-03-21: N4659 git\n2017-02-06:\
    \ N4640 git\n2016-11-28: N4618 git\n2016-07-12: N4606 git\n2016-05-30: N4594 git\n\
    2016-03-19: N4582 git\n2015-11-09: N4567 git\n2015-05-22: N4527 git\n2015-04-10:\
    \ N4431 git\n2014-11-19: N4296 git \nThis seems to be the old C++14 standard:\n\
    These version requires Authentication\n2014-10-07: N4140 git Essentially C++14\
    \ with minor errors and typos corrected\n2014-09-02: N4141 git Standard C++14\n\
    2014-03-02: N3937\n2014-03-02: N3936 git\nThe following all draft versions of\
    \ the standard:\nAll the following are freely downloadable\n2013-10-13: N3797\
    \ git\n2013-05-16: N3691\n2013-05-15: N3690\n2012-11-02: N3485\n2012-02-28: N3376\n\
    2012-01-16: N3337 git Essentially C++11 with minor errors and typos corrected\
    \  \nThis seems to be the old C++11 standard:\nThis version requires Authentication\n\
    2011-04-05: N3291  C++11 (Or Very Close)\nThe following all draft versions of\
    \ the standard:\nAll the following are freely downloadable\n2011-02-28: N3242\
    \  (differences from N3291 very minor)\n2010-11-27: N3225\n2010-08-21: N3126\n\
    2010-03-29: N3090\n2010-02-16: N3035\n2009-11-09: N3000\n2009-09-25: N2960\n2009-06-22:\
    \ N2914\n2009-03-23: N2857\n2008-10-04: N2798\n2008-08-25: N2723\n2008-06-27:\
    \ N2691\n2008-05-19: N2606\n2008-03-17: N2588\n2008-02-04: N2521\n2007-10-22:\
    \ N2461\n2007-08-06: N2369\n2007-06-25: N2315\n2007-05-07: N2284\n2006-11-03:\
    \ N2134\n2006-04-21: N2009\n2005-10-19: N1905\n2005-04-27: N1804 \nThis seems\
    \ to be the old C++03 standard:\nAll the below versions require Authentication\n\
    2004-11-05: N1733\n2004-07-16: N1655 Unofficial\n2004-02-07: N1577 C++03 (Or Very\
    \ Close)\n2001-09-13: N1316 Draft Expanded Technical Corrigendum\n1997-00-00:\
    \ N1117   Draft Expanded Technical Corrigendum  \nThe following all draft versions\
    \ of the standard:\nAll the following are freely downloadable\n1996-00-00: N0836\
    \  Draft Expanded Technical Corrigendum\n1995-00-00: N0785 Working Paper for Draft\
    \ Proposed International Standard for Information Systems - Programming Language\
    \ C++  \nOther Interesting Papers:\n2018 /\n2017 /\n2016 /\n2015 /\n2014 /\n2013\
    \ /\n2012 /\n2011 \n"
- - Where do I find the current C or C++ standard documents?
  - "\nC99 is available online. Quoted from www.open-std.org:\n\nThe lastest publically\
    \ available version of the standard is the\n  combined C99 + TC1 + TC2 + TC3,\
    \ WG14 N1256, dated 2007-09-07. This is\n  a WG14 working paper, but it reflects\
    \ the consolidated standard at the\n  time of issue.\n\n"
- - Static constant string (class member)
  - "\nYou have to define your static member outside the class definition and provide\
    \ the initializer there.\nFirst\n// In a header file (if it is in a header file\
    \ in your case)\nclass A {   \nprivate:      \n  static const string RECTANGLE;\n\
    };\n\nand then\n// In one of the implementation files\nconst string A::RECTANGLE\
    \ = \"rectangle\";\n\nThe syntax you were originally trying to use (initializer\
    \ inside class definition) is only allowed with integral and enum types.\n"
- - Static constant string (class member)
  - "\nIn C++11 you can do now:\nclass A {\n private:\n  static constexpr const char*\
    \ STRING = \"some useful string constant\";\n};\n\n"
- - Static constant string (class member)
  - "\nInside class definitions you can only declare static members. They have to\
    \ be defined outside of the class. For compile-time integral constants the standard\
    \ makes the exception that you can \"initialize\" members. It's still not a definition,\
    \ though. Taking the address would not work without definition, for example.\n\
    I'd like to mention that I don't see the benefit of using std::string over const\
    \ char[] for constants. std::string is nice and all but it requires dynamic initialization.\
    \ So, if you write something like\nconst std::string foo = \"hello\";\n\nat namespace\
    \ scope the constructor of foo will be run right before execution of main starts\
    \ and this constructor will create a copy of the constant \"hello\" in the heap\
    \ memory. Unless you really need RECTANGLE to be a std::string you could just\
    \ as well write\n// class definition with incomplete static member could be in\
    \ a header file\nclass A {\n    static const char RECTANGLE[];\n};\n\n// this\
    \ needs to be placed in a single translation unit only\nconst char A::RECTANGLE[]\
    \ = \"rectangle\";\n\nThere! No heap allocation, no copying, no dynamic initialization.\n\
    Cheers, s.\n"
- - How to get rid of `deprecated conversion from string constant to âchar*â` warnings
    in GCC?
  - "\nI believe passing -Wno-write-strings to gcc will suppress this warning.\n"
- - How to get rid of `deprecated conversion from string constant to âchar*â` warnings
    in GCC?
  - "\nAny functions into which you pass string literals \"I am a string literal\"\
    \ should use char const * as the type instead of char*.\nIf you're going to fix\
    \ something, fix it right.\nExplanation:\nYou can not use string literals to initialise\
    \ strings that will be modified, because they are of type const char*. Casting\
    \ away the constness to later modify them is undefined behaviour, so you have\
    \ to copy your const char* strings char by char into dynamically allocated char*\
    \ strings in order to modify them.\nExample:\n#include <iostream>\n\nvoid print(char*\
    \ ch);\n\nvoid print(const char* ch) {\n    std::cout<<ch;\n}\n\nint main() {\n\
    \    print(\"Hello\");\n    return 0;\n}\n\n"
- - How to get rid of `deprecated conversion from string constant to âchar*â` warnings
    in GCC?
  - "\nCheck out gcc's Diagnostic Pragma support, and the list of -W warning options\
    \ (changed: new link to warning options).\nFor gcc, you can use #pragma warning\
    \ directives like explained here.\n"
- - When should I really use noexcept?
  - "\nI think it is too early to give a \"best practices\" answer for this as there\
    \ hasn't been enough time to use it in practice. If this was asked about throw\
    \ specifiers right after they came out then the answers would be very different\
    \ to now.\n\nHaving to think about whether or not I need to append noexcept after\
    \ every function declaration would greatly reduce programmer productivity (and\
    \ frankly, would be a pain). \n\nWell then use it when it's obvious that the function\
    \ will never throw.\n\nWhen can I realistically expect to observe a performance\
    \ improvement after using noexcept? [...] Personally, I care about noexcept because\
    \  of the increased freedom provided to the compiler to safely apply certain kinds\
    \ of optimizations. \n\nIt seems like the biggest optimization gains are from\
    \ user optimizations, not compiler ones due to the possibility of checking noexcept\
    \ and overloading on it. Most compilers follow a no-penalty-if-you-don't-throw\
    \ exception handling method so I doubt it would change much (or anything) on the\
    \ machine code level of your code, although perhaps reduce the binary size by\
    \ removing the handling code.\nUsing noexcept in the big 4 (constructors, assignment,\
    \ not destructors as they're already noexcept) will likely cause the best improvements\
    \ as noexcept checks are 'common' in template code such as in std containers.\
    \ For instance, std::vector won't use your class's move unless it's marked noexcept\
    \ (or the compiler can deduce it otherwise).\n"
- - When should I really use noexcept?
  - "\nAs I keep repeating these days: semantics first.\nAdding noexcept, noexcept(true)\
    \ and noexcept(false) is first and foremost about semantics. It only incidentally\
    \ condition a number of possible optimizations.\nAs a programmer reading code,\
    \ the presence of noexcept is akin to that of const: it helps me better grok what\
    \ may or may not happen. Therefore, it is worthwhile spending some time thinking\
    \ about whether or not you know if the function will throw. For reminder, any\
    \ kind of dynamic memory allocation may throw.\n\nOkay, now on to the possible\
    \ optimizations.\nThe most obvious optimizations are actually performed in the\
    \ libraries. C++11 provides a number of traits that allows knowing whether a function\
    \ is noexcept or not, and the Standard Library implementation themselves will\
    \ use those traits to favor noexcept operations on the user-defined objects they\
    \ manipulate, if possible. Such as move semantics.\nThe compiler may only shave\
    \ a bit of fat (perhaps) from the exception handling data, because it has to take\
    \ into account the fact that you may have lied. If a function marked noexcept\
    \ does throw, then std::terminate is called.\nThese semantics were chosen for\
    \ two reasons:\n\nimmediately benefiting from noexcept even when dependencies\
    \ do not use it already (backward compatibility)\nallowing the specification of\
    \ noexcept when calling functions that may theoretically throw but are not expected\
    \ to for the given arguments\n\n"
- - When should I really use noexcept?
  - "\nThis actually does make a (potentially) huge difference to the optimizer in\
    \ the compiler. Compilers have actually had this feature for years via the empty\
    \ throw() statement after a function definition, as well as propriety extensions.\
    \ I can assure you that modern compilers do take advantage of this knowledge to\
    \ generate better code.\nAlmost every optimization in the compiler uses something\
    \ called a \"flow graph\" of a function to reason about what is legal. A flow\
    \ graph consists of what are generally called \"blocks\" of the function (areas\
    \ of code that have a single entrance and a single exit) and edges between the\
    \ blocks to indicate where flow can jump to. Noexcept alters the flow graph.\n\
    You asked for a specific example. Consider this code:\nvoid foo(int x) {\n   \
    \ try {\n        bar();\n        x = 5;\n        // other stuff which doesn't\
    \ modify x, but might throw\n    } catch(...) {\n        // don't modify x\n \
    \   }\n\n    baz(x); // or other statement using x\n}\n\nThe flow graph for this\
    \ function is different if bar is labeled noexcept (there is no way for execution\
    \ to jump between the end of bar and the catch statement). When labeled as noexcept,\
    \ the compiler is certain the value of x is 5 during the baz function - the x=5\
    \ block is said to \"dominate\" the baz(x) block without the edge from bar() to\
    \ the catch statement. It can then do something called \"constant propagation\"\
    \ to generate more efficient code. Here if baz is inlined, the statements using\
    \ x might also contain constants and then what used to be a runtime evaluation\
    \ can be turned into a compile-time evaluation, etc.\nAnyway, short answer: noexcept\
    \ lets the compiler generate a tighter flow graph, and the flow graph is used\
    \ to reason about all sorts of common compiler optimizations. To a compiler, user\
    \ annotations of this nature are awesome. The compiler will try to figure this\
    \ stuff out, but it usually can't (the function in question might be in another\
    \ object file not visible to the compiler or transitively use some function which\
    \ is not visible), or when it does there is some trivial exception which might\
    \ be thrown that you're not even aware of so it can't implicitly label it as noexcept\
    \ (allocating memory might throw bad_alloc, for example).\n"
- - When to use reinterpret_cast?
  - "\nThe C++ standard guarantees the following:\nstatic_casting a pointer to and\
    \ from void* preserves the address. That is, in the following, a, b and c all\
    \ point to the same address:\nint* a = new int();\nvoid* b = static_cast<void*>(a);\n\
    int* c = static_cast<int*>(b);\n\nreinterpret_cast only guarantees that if you\
    \ cast a pointer to a different type, and then reinterpret_cast it back to the\
    \ original type, you get the original value. So in the following:\nint* a = new\
    \ int();\nvoid* b = reinterpret_cast<void*>(a);\nint* c = reinterpret_cast<int*>(b);\n\
    \na and c contain the same value, but the value of b is unspecified. (in practice\
    \ it will typically contain the same address as a and c, but that's not specified\
    \ in the standard, and it may not be true on machines with more complex memory\
    \ systems.)\nFor casting to and from void*, static_cast should be preferred.\n"
- - When to use reinterpret_cast?
  - "\nOne case when reinterpret_cast is necessary is when interfacing with opaque\
    \ data types.  This occurs frequently in vendor APIs over which the programmer\
    \ has no control.  Here's a contrived example where a vendor provides an API for\
    \ storing and retrieving arbitrary global data:\n// vendor.hpp\ntypedef struct\
    \ _Opaque * VendorGlobalUserData;\nvoid VendorSetUserData(VendorGlobalUserData\
    \ p);\nVendorGlobalUserData VendorGetUserData();\n\nTo use this API, the programmer\
    \ must cast their data to VendorGlobalUserData and back again.  static_cast won't\
    \ work, one must use reinterpret_cast:\n// main.cpp\n#include \"vendor.hpp\"\n\
    #include <iostream>\nusing namespace std;\n\nstruct MyUserData {\n    MyUserData()\
    \ : m(42) {}\n    int m;\n};\n\nint main() {\n    MyUserData u;\n\n        //\
    \ store global data\n    VendorGlobalUserData d1;\n//  d1 = &u;              \
    \                            // compile error\n//  d1 = static_cast<VendorGlobalUserData>(&u);\
    \       // compile error\n    d1 = reinterpret_cast<VendorGlobalUserData>(&u);\
    \  // ok\n    VendorSetUserData(d1);\n\n        // do other stuff...\n\n     \
    \   // retrieve global data\n    VendorGlobalUserData d2 = VendorGetUserData();\n\
    \    MyUserData * p = 0;\n//  p = d2;                                        \
    \   // compile error\n//  p = static_cast<MyUserData *>(d2);                //\
    \ compile error\n    p = reinterpret_cast<MyUserData *>(d2);           // ok\n\
    \n    if (p) { cout << p->m << endl; }\n    return 0;\n}\n\nBelow is a contrived\
    \ implementation of the sample API:\n// vendor.cpp\nstatic VendorGlobalUserData\
    \ g = 0;\nvoid VendorSetUserData(VendorGlobalUserData p) { g = p; }\nVendorGlobalUserData\
    \ VendorGetUserData() { return g; }\n\n"
- - When to use reinterpret_cast?
  - "\nThe short answer:\nIf you don't know what reinterpret_cast stands for, don't\
    \ use it. If you will need it in the future, you will know.\nFull answer:\nLet's\
    \ consider basic number types.\nWhen you convert for example int(12) to unsigned\
    \ float (12.0f) your processor needs to invoke some calculations as both numbers\
    \ has different bit representation. This is what static_cast stands for.\nOn the\
    \ other hand, when you call reinterpret_cast the CPU does not invoke any calculations.\
    \ It just treats a set of bits in the memory like if it had another type. So when\
    \ you convert int* to float* with this keyword, the new value (after pointer dereferecing)\
    \ has nothing to do with the old value in mathematical meaning.\nExample: It is\
    \ true that reinterpret_cast is not portable because of one reason - byte order\
    \ (endianness). But this is often surprisingly the best reason to use it. Let's\
    \ imagine the example: you have to read binary 32bit number from file, and you\
    \ know it is big endian. Your code has to be generic and works properly on big\
    \ endian (e.g. ARM) and little endian (e.g. x86) systems. So you have to check\
    \ the byte order. It is well-known on compile time so you can write constexpr\
    \ function:\nconstexpr bool is_little_endian() {\n  std::uint16_t x=0x0001;\n\
    \  auto p = reinterpret_cast<std::uint8_t*>(&x);\n  return *p != 0;\n}\n\nExplanation:\
    \ the binary representation of x in memory could be 0000'0000'0000'0001 (big)\
    \ or 0000'0001'0000'0000 (little endian). After reinterpret-casting the byte under\
    \ p pointer could be respectively 0000'0000 or 0000'0001. If you use static-casting,\
    \ it will always be 0000'0001, no matter what endianness is being used.\n"
- - How to use the PI constant in C++
  - "\nOn some (especially older) platforms (see the comments below) you might need\
    \ to\n#define _USE_MATH_DEFINES\n\nand then include the necessary header file:\n\
    #include <math.h>\n\nand the value of pi can be accessed via:\nM_PI\n\nIn my math.h\
    \ (2014) it is defined as: \n# define M_PI           3.14159265358979323846  /*\
    \ pi */\n\nbut check your math.h for more. An extract from the \"old\" math.h\
    \ (in 2009):\n/* Define _USE_MATH_DEFINES before including math.h to expose these\
    \ macro\n * definitions for common math constants.  These are placed under an\
    \ #ifdef\n * since these commonly-defined names are not part of the C/C++ standards.\n\
    \ */\n\nHowever:\n\non newer platforms (at least on my 64 bit Ubuntu 14.04) I\
    \ do not need to define the _USE_MATH_DEFINES \nOn (recent) Linux platforms there\
    \ are long double values too provided as a GNU Extension:\n# define M_PIl    \
    \      3.141592653589793238462643383279502884L /* pi */\n\n\n"
- - How to use the PI constant in C++
  - "\nPi can be calculated as atan(1)*4. You could calculate the value this way and\
    \ cache it.\n"
- - How to use the PI constant in C++
  - "\nYou could also use boost, which defines important math constants with maximum\
    \ accuracy for the requested type (i.e. float vs double).  \nconst double pi =\
    \ boost::math::constants::pi<double>();\n\nCheck out the boost documentation for\
    \ more examples.\n"
- - Typedef function pointer?
  - "\ntypedef is a language construct that associates a name to a type.\nYou use\
    \ it the same way you would use the original type, for instance\n  typedef int\
    \ myinteger;\n  typedef char *mystring;\n  typedef void (*myfunc)();\n\nusing\
    \ them like\n  myinteger i;   // is equivalent to    int i;\n  mystring s;   \
    \ // is the same as      char *s;\n  myfunc f;      // compile equally as  void\
    \ (*f)();\n\nAs you can see, you could just replace the typedefed name with its\
    \ definition given above.\nThe difficulty lies in the pointer to functions syntax\
    \ and readability in C and C++, and the typedef can improve the readability of\
    \ such declarations. However, the syntax is appropriate, since functions - unlike\
    \ other simpler types - may have a return value and parameters, thus the sometimes\
    \ lengthy and complex declaration of a pointer to function.\nThe readability may\
    \ start to be really tricky with pointers to functions arrays, and some other\
    \ even more indirect flavors.\nTo answer your three questions\n\nWhy is typedef\
    \ used?\nTo ease the reading of the code - especially for pointers to functions,\
    \ or structure names. \nThe syntax looks odd (in the pointer to function declaration)\n\
    That syntax is not obvious to read, at least when beginning. Using a typedef declaration\
    \ instead eases the reading\nIs a function pointer created to store the memory\
    \ address of a function?\nYes, a function pointer stores the address of a function.\
    \ This has nothing to do with the typedef construct which only ease the writing/reading\
    \ of a program ; the compiler just expands the typedef definition before compiling\
    \ the actual code.\n\nExample:\ntypedef int (*t_somefunc)(int,int);\n\nint product(int\
    \ u, int v) {\n  return u*v;\n}\n\nt_somefunc afunc = &product;\n...\nint x2 =\
    \ (*afunc)(123, 456); // call product() to calculate 123*456\n\n"
- - Typedef function pointer?
  - "\n\ntypedef is used to alias types; in this case you're aliasing FunctionFunc\
    \ to void(*)().\nIndeed the syntax does look odd, have a look at this:\ntypedef\
    \   void      (*FunctionFunc)  ( );\n//         ^                ^         ^\n\
    //     return type      type name  arguments\n\nNo, this simply tells the compiler\
    \ that the FunctionFunc type will be a function pointer, it doesn't define one,\
    \ like this:\nFunctionFunc x;\nvoid doSomething() { printf(\"Hello there\\n\"\
    ); }\nx = &doSomething;\n\nx(); //prints \"Hello there\"\n\n\n"
- - Typedef function pointer?
  - "\nWithout the typedef word, in C++ the declaration would declare a variable FunctionFunc\
    \ of type pointer to function of no arguments, returning void.\nWith the typedef\
    \ it instead defines FunctionFunc as a name for that type.\n"
- - How do I tokenize a string in C++?
  - "\nYour simple case can easily be built using the std::string::find method. However,\
    \ take a look at Boost.Tokenizer. It's great. Boost generally has some very cool\
    \ string tools.\n"
- - How do I tokenize a string in C++?
  - "\nThe Boost tokenizer class can make this sort of thing quite simple:\n#include\
    \ <iostream>\n#include <string>\n#include <boost/foreach.hpp>\n#include <boost/tokenizer.hpp>\n\
    \nusing namespace std;\nusing namespace boost;\n\nint main(int, char**)\n{\n \
    \   string text = \"token, test   string\";\n\n    char_separator<char> sep(\"\
    , \");\n    tokenizer< char_separator<char> > tokens(text, sep);\n    BOOST_FOREACH\
    \ (const string& t, tokens) {\n        cout << t << \".\" << endl;\n    }\n}\n\
    \nUpdated for C++11:\n#include <iostream>\n#include <string>\n#include <boost/tokenizer.hpp>\n\
    \nusing namespace std;\nusing namespace boost;\n\nint main(int, char**)\n{\n \
    \   string text = \"token, test   string\";\n\n    char_separator<char> sep(\"\
    , \");\n    tokenizer<char_separator<char>> tokens(text, sep);\n    for (const\
    \ auto& t : tokens) {\n        cout << t << \".\" << endl;\n    }\n}\n\n"
- - How do I tokenize a string in C++?
  - "\nHere's a real simple one:\n#include <vector>\n#include <string>\nusing namespace\
    \ std;\n\nvector<string> split(const char *str, char c = ' ')\n{\n    vector<string>\
    \ result;\n\n    do\n    {\n        const char *begin = str;\n\n        while(*str\
    \ != c && *str)\n            str++;\n\n        result.push_back(string(begin,\
    \ str));\n    } while (0 != *str++);\n\n    return result;\n}\n\n"
- - Initializing a static std::map<int, int> in C++
  - "\nUsing C++11:\n#include <map>\nusing namespace std;\n\nmap<int, char> m = {{1,\
    \ 'a'}, {3, 'b'}, {5, 'c'}, {7, 'd'}};\n\nUsing Boost.Assign:\n#include <map>\n\
    #include \"boost/assign.hpp\"\nusing namespace std;\nusing namespace boost::assign;\n\
    \nmap<int, char> m = map_list_of (1, 'a') (3, 'b') (5, 'c') (7, 'd');\n\n"
- - Initializing a static std::map<int, int> in C++
  - "\nBest way is to use a function:\n#include <map>\n\nusing namespace std;\n\n\
    map<int,int> create_map()\n{\n  map<int,int> m;\n  m[1] = 2;\n  m[3] = 4;\n  m[5]\
    \ = 6;\n  return m;\n}\n\nmap<int,int> m = create_map();\n\n"
- - Initializing a static std::map<int, int> in C++
  - "\nIt's not a complicated issue to make something similar to boost. Here's a class\
    \ with just three functions, including the constructor, to replicate what boost\
    \ did (almost).\ntemplate <typename T, typename U>\nclass create_map\n{\nprivate:\n\
    \    std::map<T, U> m_map;\npublic:\n    create_map(const T& key, const U& val)\n\
    \    {\n        m_map[key] = val;\n    }\n\n    create_map<T, U>& operator()(const\
    \ T& key, const U& val)\n    {\n        m_map[key] = val;\n        return *this;\n\
    \    }\n\n    operator std::map<T, U>()\n    {\n        return m_map;\n    }\n\
    };\n\nUsage:\n\nstd::map mymap = create_map<int, int >(1,2)(3,4)(5,6);\n\nThe\
    \ above code works best for initialization of global variables or static members\
    \ of a class which needs to be initialized and you have no idea when it gets used\
    \ first but you want to assure that the values are available in it.\nIf say, you've\
    \ got to insert elements into an existing std::map... here's another class for\
    \ you.\ntemplate <typename MapType>\nclass map_add_values {\nprivate:\n    MapType\
    \ mMap;\npublic:\n    typedef typename MapType::key_type KeyType;\n    typedef\
    \ typename MapType::mapped_type MappedType;\n\n    map_add_values(const KeyType&\
    \ key, const MappedType& val)\n    {\n        mMap[key] = val;\n    }\n\n    map_add_values&\
    \ operator()(const KeyType& key, const MappedType& val) {\n        mMap[key] =\
    \ val;\n        return *this;\n    }\n\n    void to (MapType& map) {\n       \
    \ map.insert(mMap.begin(), mMap.end());\n    }\n};\n\nUsage:\ntypedef std::map<int,\
    \ int> Int2IntMap;\nInt2IntMap testMap;\nmap_add_values<Int2IntMap>(1,2)(3,4)(5,6).to(testMap);\n\
    \nSee it in action with GCC 4.7.2 here: http://ideone.com/3uYJiH\n###############\
    \ EVERYTHING BELOW THIS IS OBSOLETE #################\nEDIT: The map_add_values\
    \ class below, which was the original solution I had suggested, would fail when\
    \ it comes to GCC 4.5+. Please look at the code above for how to add values to\
    \ existing map.\n\ntemplate<typename T, typename U>\nclass map_add_values\n{\n\
    private:\n    std::map<T,U>& m_map;\npublic:\n    map_add_values(std::map<T, U>&\
    \ _map):m_map(_map){}\n    map_add_values& operator()(const T& _key, const U&\
    \ _val)\n    {\n        m_map[key] = val;\n        return *this;\n    }\n};\n\
    Usage:\nstd::map<int, int> my_map;\n// Later somewhere along the code\nmap_add_values<int,int>(my_map)(1,2)(3,4)(5,6);\n\
    NOTE: Previously I used a operator [] for adding the actual values. This is not\
    \ possible as commented by dalle.\n##################### END OF OBSOLETE SECTION\
    \ #####################\n"
- - Difference of keywords 'typename' and 'class' in templates?
  - "\ntypename and class are interchangeable in the basic case of specifying a template:\n\
    template<class T>\nclass Foo\n{\n};\n\nand\ntemplate<typename T>\nclass Foo\n\
    {\n};\n\nare equivalent.\nHaving said that, there are specific cases where there\
    \ is a difference between typename and class.\nThe first one is in the case of\
    \ dependent types. typename is used to declare when you are referencing a nested\
    \ type that depends on another template parameter, such as the typedef in this\
    \ example:\ntemplate<typename param_t>\nclass Foo\n{\n    typedef typename param_t::baz\
    \ sub_t;\n};\n\nThe second one you actually show in your question, though you\
    \ might not realize it:\ntemplate < template < typename, typename > class Container,\
    \ typename Type >\n\nWhen specifying a template template, the class keyword MUST\
    \ be used as above -- it is not interchangeable with typename in this case (note:\
    \ since C++17 both keywords are allowed in this case).\nYou also must use class\
    \ when explicitly instantiating a template:\ntemplate class Foo<int>;\n\nI'm sure\
    \ that there are other cases that I've missed, but the bottom line is: these two\
    \ keywords are not equivalent, and these are some common cases where you need\
    \ to use one or the other.\n"
- - Difference of keywords 'typename' and 'class' in templates?
  - "\nFor naming template parameters, typename and class are equivalent. Â§14.1.2:\n\
    \nThere is no semantic difference\n  between class and typename in a\n  template-parameter.\n\
    \ntypename however is possible in another context when using templates - to hint\
    \ at the compiler that you are referring to a dependent type. Â§14.6.2:\n\nA name\
    \ used in a template declaration\n  or definition and that is dependent on\n \
    \ a template-parameter is assumed not to\n  name a type unless the applicable\
    \ name\n  lookup finds a type name or the name\n  is qualified by the keyword\
    \ typename.\n\nExample:\ntypename some_template<T>::some_type\n\nWithout typename\
    \ the compiler can't tell in general whether you are referring to a type or not.\n"
- - Difference of keywords 'typename' and 'class' in templates?
  - "\nWhile there is no technical difference, I have seen the two used to denote\
    \ slightly different things.\nFor a template that should accept any type as T,\
    \ including built-ins (such as an array )\ntemplate<typename T>\nclass Foo { ...\
    \ }\n\nFor a template that will only work where T is a real class.\ntemplate<class\
    \ T>\nclass Foo { ... }\n\nBut keep in mind that this is purely a style thing\
    \ some people use. Not mandated by the standard or enforced by compilers\n"
